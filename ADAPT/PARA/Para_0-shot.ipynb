{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e5e14a-f435-4fea-8c1e-027967121ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed cache: C:\\Users\\stdFurqan\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased\n",
      "Removed cache: C:\\Users\\stdFurqan\\.cache\\huggingface\\hub\\models--xlm-roberta-base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Get user profile path\n",
    "user_profile = os.environ[\"USERPROFILE\"]\n",
    "\n",
    "# Paths to Hugging Face cached models\n",
    "cached_models = [\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--bert-base-multilingual-cased\"),\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--xlm-roberta-base\")\n",
    "]\n",
    "\n",
    "# Remove cached models if they exist\n",
    "for path in cached_models:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed cache: {path}\")\n",
    "    else:\n",
    "        print(f\"No cache found at: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fb3673-e6d3-4c1b-b9e3-d3e62a80313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15579873-a110-4b82-b1b9-5db067858a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4080 SUPER\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Imports\n",
    "# ==============================\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptForClassification, PromptDataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "\n",
    "# ========================================\n",
    "# Check CUDA\n",
    "# ========================================\n",
    "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "\n",
    "# ========================================\n",
    "# Seeds for reproducibility\n",
    "# ========================================\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccd5428-2982-48ce-aa9e-b83708883d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\stdFurqan\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "### Classes ###\n",
    "classes = ['P', 'NP']\n",
    "\n",
    "### Label Map ###\n",
    "label_map = {'P': 1, 'NP': 0}\n",
    "\n",
    "\n",
    "\n",
    "# # Step 1: Use load_plm with 'roberta' to get the correct WrapperClass\n",
    "_, _, _, WrapperClass = load_plm(\"roberta\", \"roberta-base\")  # Just to get the wrapper\n",
    "\n",
    "# # # Step 2: Manually load XLM-RoBERTa model/tokenizer\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "plm = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# ==============================\n",
    "# Load Pretrained Language Model (mBERT)\n",
    "# ==============================\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e18e97-ead9-482a-b46c-8e037627b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ManualTemplate(\n",
    "    text = '{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©ÛŒØ§ Ø§ÛŒÚ© Ø¬ÛŒØ³Û’ ÛÛŒÚºØŸ {\"mask\"}',\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# templates = [\n",
    "#     (\"T1\", ManualTemplate(\n",
    "#         text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©ÛŒØ§ Ø§ÛŒÚ© Ø¬ÛŒØ³Û’ ÛÛŒÚºØŸ {\"mask\"}',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "\n",
    "#     (\"T2\", ManualTemplate(\n",
    "#         text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ù…ÛŒÚº Ù…Ø¹Ù†ÛŒ Ú©ÛŒ Ù…Ø·Ø§Ø¨Ù‚Øª {\"mask\"} ÛÛ’Û”',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "\n",
    "#     (\"T3\", ManualTemplate(\n",
    "#         text='Ú©ÛŒØ§ ÛŒÛ Ø¯ÙˆÙ†ÙˆÚº Ø¬Ù…Ù„Û’ {\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ø§ÛŒÚ© Ø¯ÙˆØ³Ø±Û’ Ú©ÛŒ ØªÚ©Ø±Ø§Ø± ÛÛŒÚºØŸ {\"mask\"}',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "\n",
    "#     (\"T4\", ManualTemplate(\n",
    "#         text='{\"placeholder\":\"text_a\"} Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ {\"placeholder\":\"text_b\"} {\"mask\"} ÛÛ’Û”',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "\n",
    "#     (\"T5\", ManualTemplate(\n",
    "#         text='Ø¯ÙˆÙ†ÙˆÚº Ø¬Ù…Ù„Û’ {\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©Ø§ ØªØ¹Ù„Ù‚ {\"mask\"} ÛÛ’Û”',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "\n",
    "#     (\"T6\", ManualTemplate(\n",
    "#         text='{\"placeholder\":\"text_b\"} Ø§ÙˆØ± {\"placeholder\":\"text_a\"} Ø§ÛŒÚ© Ø¬ÛŒØ³Û’ Ø®ÛŒØ§Ù„ Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚºØŸ {\"mask\"}',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "\n",
    "#     (\"T7\", ManualTemplate(\n",
    "#         text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©Û’ Ø§Ù„ÙØ§Ø¸ Ú©ÛŒ ÛÙ… Ø¢ÛÙ†Ú¯ÛŒ {\"mask\"} ÛÛ’Û”',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "\n",
    "#     (\"T8\", ManualTemplate(\n",
    "#         text='Ø§Ú¯Ø± ÛÙ… {\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ÛŒÚº ØªÙˆ ÛŒÛ {\"mask\"} ÛÛŒÚºÛ”',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "\n",
    "#     (\"T9\", ManualTemplate(\n",
    "#         text='{\"placeholder\":\"text_a\"} Ú©ÛŒ Ø±ÙˆØ´Ù†ÛŒ Ù…ÛŒÚº {\"placeholder\":\"text_b\"} {\"mask\"} Ø³Ù…Ø¬Ú¾Ø§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’Û”',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "\n",
    "#     (\"T10\", ManualTemplate(\n",
    "#         text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©Û’ Ø¨ÛŒÚ† Ù…ÙÛÙˆÙ… Ú©Ø§ ØªØ¹Ù„Ù‚ {\"mask\"} ÛÛ’Û”',\n",
    "#         tokenizer=tokenizer,\n",
    "#     )),\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "verbalizer = ManualVerbalizer(\n",
    "    classes=classes,\n",
    "    label_words={\n",
    "        \"P\": [\"ÛÙ… Ù…Ø¹Ù†ÛŒ\", \"Ø§ÛŒÚ© Ø¬ÛŒØ³Û’\"],    # Paraphrase: same meaning / similar\n",
    "        \"NP\": [\"Ù…Ø®ØªÙ„Ù\", \"ØºÛŒØ± Ù…ØªØ±Ø§Ø¯Ù\"] \n",
    "    },\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbfa212f-f27f-4f80-97e5-96ef3e2ab9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Create Prompt Model\n",
    "# ==============================\n",
    "prompt_model = PromptForClassification(\n",
    "    template=template,\n",
    "    plm=plm,\n",
    "    verbalizer=verbalizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16d2e0f-2bad-48bd-a630-400c7a8dec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "tokenizing: 28it [00:00, 664.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š PARA Urdu Dev Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           P     0.4643    1.0000    0.6341        13\n",
      "          NP     0.0000    0.0000    0.0000        15\n",
      "\n",
      "    accuracy                         0.4643        28\n",
      "   macro avg     0.2321    0.5000    0.3171        28\n",
      "weighted avg     0.2156    0.4643    0.2944        28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Helper Function to Read XML\n",
    "# ==============================\n",
    "def read_xml_text(path):\n",
    "    \"\"\"\n",
    "    Reads an XML file and returns cleaned text inside <UPPC_document>...</UPPC_document>.\n",
    "    Removes all tags and converts inner double quotes to single quotes.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        xml = f.read()\n",
    "    \n",
    "    # Extract text between opening and closing <UPPC_document>\n",
    "    start = xml.find(\">\") + 1\n",
    "    end = xml.rfind(\"</UPPC_document>\")\n",
    "    text = xml[start:end].strip()\n",
    "    \n",
    "    # Remove any remaining tags inside the text\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    \n",
    "    # Replace multiple spaces/newlines with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Replace double quotes inside text with single quotes\n",
    "    text = text.replace('\"', \"'\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ==============================\n",
    "# Load Test CSV\n",
    "# ==============================\n",
    "df = pd.read_csv(r\"C:\\Users\\stdFurqan\\Desktop\\paft\\PARA\\test_pairs.csv\")\n",
    "\n",
    "data_folder = r\"C:\\Users\\stdFurqan\\Desktop\\paft\\PARA\\data\"\n",
    "\n",
    "# ==============================\n",
    "# Make InputExamples\n",
    "# ==============================\n",
    "eval_dataset = []\n",
    "for i, row in df.iterrows():\n",
    "    # Read XML texts\n",
    "    text_a_path = os.path.join(data_folder, row['source_file'])\n",
    "    text_b_path = os.path.join(data_folder, row['target_file'])\n",
    "    \n",
    "    text_a = read_xml_text(text_a_path)\n",
    "    text_b = read_xml_text(text_b_path)\n",
    "    \n",
    "    eval_dataset.append(\n",
    "        InputExample(\n",
    "            guid=i,\n",
    "            text_a=text_a,\n",
    "            text_b=text_b,\n",
    "            label=label_map[row['label']]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# PromptDataLoader\n",
    "# ==============================\n",
    "eval_loader = PromptDataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=template,              # your ManualTemplate for XNLI\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Evaluate Model\n",
    "# ==============================\n",
    "prompt_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        logits = prompt_model(batch)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(batch['label'].cpu().tolist())\n",
    "\n",
    "# ==============================\n",
    "# Print Classification Report\n",
    "# ==============================\n",
    "print(\"\\nðŸ“Š PARA Urdu Dev Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4344437-ef98-41bd-aad4-8f04990b1738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f570f01-7077-4933-b2cd-5a65e3686345",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta\n",
    "ðŸ“Š PARA Urdu Dev Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           P     0.4643    1.0000    0.6341        13\n",
    "          NP     0.0000    0.0000    0.0000        15\n",
    "\n",
    "    accuracy                         0.4643        28\n",
    "   macro avg     0.2321    0.5000    0.3171        28\n",
    "weighted avg     0.2156    0.4643    0.2944        28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce41ee0-1a17-4a6b-91f2-7abd7a25ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert\n",
    "ðŸ“Š PARA Urdu Dev Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           P     0.4643    1.0000    0.6341        13\n",
    "          NP     0.0000    0.0000    0.0000        15\n",
    "\n",
    "    accuracy                         0.4643        28\n",
    "   macro avg     0.2321    0.5000    0.3171        28\n",
    "weighted avg     0.2156    0.4643    0.2944        28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb89e0-6506-458c-963e-664fcb5e5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================\n",
    "# # Load Evaluation Dataset\n",
    "# # ==============================\n",
    "# df = pd.read_csv(r\"C:\\Users\\stdFurqan\\Desktop\\paft\\SST-2\\urdu_sentiment_test_labeled.csv\")\n",
    "# eval_dataset = [\n",
    "#     InputExample(guid=i, text_a=row['text'], label=label_map[row['label']])\n",
    "#     for i, row in df.iterrows()\n",
    "# ]\n",
    "\n",
    "# # ==============================\n",
    "# # 0-Shot Evaluation with Each Template\n",
    "# # ==============================\n",
    "# prompt_model.eval()  # ensure model is in evaluation mode\n",
    "# batch_size = 8    # eval batch size\n",
    "\n",
    "# # Optional: store template order and results\n",
    "# all_pass_patterns = {}\n",
    "\n",
    "# for pass_idx, (prompt_name, current_template) in enumerate(templates, start=1):\n",
    "#     print(f\"\\nðŸŸ¦ 0-Shot Evaluation - Template {prompt_name} ({pass_idx}/{len(templates)})\")\n",
    "\n",
    "#     # Create PromptDataLoader with current template\n",
    "#     eval_loader = PromptDataLoader(\n",
    "#         dataset=eval_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#         template=current_template,\n",
    "#         tokenizer_wrapper_class=WrapperClass,\n",
    "#         max_seq_length=128,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False\n",
    "#     )\n",
    "\n",
    "#     pass_preds = []\n",
    "#     pass_labels = []\n",
    "\n",
    "#     # Run evaluation\n",
    "#     with torch.no_grad():\n",
    "#         for batch in eval_loader:\n",
    "#             logits = prompt_model(batch)\n",
    "#             preds = torch.argmax(logits, dim=-1)\n",
    "#             pass_preds.extend(preds.cpu().tolist())\n",
    "#             pass_labels.extend(batch['label'].cpu().tolist())\n",
    "\n",
    "#     # Print report immediately after this template\n",
    "#     print(f\"\\nðŸ“Š STS_B Urdu Dev Classification Report - Template {prompt_name}\")\n",
    "#     print(classification_report(pass_labels, pass_preds, target_names=classes, digits=4))\n",
    "\n",
    "#     # Store template name (optional)\n",
    "#     all_pass_patterns[f\"pass_{pass_idx}\"] = prompt_name\n",
    "\n",
    "# # Optional: print template order at the end\n",
    "# print(\"\\nâœ… Templates used per pass:\", all_pass_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea18e7cf-96d6-427c-b07e-d519bff165ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d822b-6839-499d-9698-ea98adfb19f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
