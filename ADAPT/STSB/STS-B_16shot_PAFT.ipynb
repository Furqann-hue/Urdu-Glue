{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a47b0e-a12f-46bb-962d-35fd8ba7b237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "# ==============================\n",
    "# Load STS-B labeled dataset\n",
    "# ==============================\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\stdFurqan\\Desktop\\paft\\STS-B\\Final_STSB_train_labeled.csv\"\n",
    ")\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# ==============================\n",
    "# Label Mapping (5-way classification)\n",
    "# ==============================\n",
    "label_map = {\n",
    "    \"unrelated\": 0,\n",
    "    \"distant\": 1,\n",
    "    \"similar\": 2,\n",
    "    \"equivalent\": 3,\n",
    "    \"identical\": 4\n",
    "}\n",
    "\n",
    "classes = list(label_map.keys())\n",
    "\n",
    "guid = 0\n",
    "examples = []\n",
    "class_stats = {}\n",
    "\n",
    "# ==============================\n",
    "# Take FIRST 16 examples per label\n",
    "# ==============================\n",
    "for label_name in classes:\n",
    "    class_df = df[df[\"score_to_labels\"] == label_name].head(16)\n",
    "    class_stats[label_name] = len(class_df)\n",
    "\n",
    "    for _, row in class_df.iterrows():\n",
    "        examples.append(\n",
    "            InputExample(\n",
    "                guid=guid,\n",
    "                text_a=str(row[\"sentence1\"]).strip(),\n",
    "                text_b=str(row[\"sentence2\"]).strip(),\n",
    "                label=label_map[label_name]\n",
    "            )\n",
    "        )\n",
    "        guid += 1\n",
    "\n",
    "# ==============================\n",
    "# Print InputExamples (exact format)\n",
    "# ==============================\n",
    "print(\"\\n### InputExamples ###\\n\")\n",
    "for ex in examples:\n",
    "    print(\n",
    "        \"InputExample(\\n\"\n",
    "        f\"    guid={ex.guid},\\n\"\n",
    "        f\"    text_a=\\\"{ex.text_a}\\\",\\n\"\n",
    "        f\"    text_b=\\\"{ex.text_b}\\\",\\n\"\n",
    "        f\"    label={ex.label}\\n\"\n",
    "        \"),\"\n",
    "    )\n",
    "\n",
    "# ==============================\n",
    "# Metadata\n",
    "# ==============================\n",
    "print(\"\\n### Classes ###\")\n",
    "print(f\"classes = {classes}\")\n",
    "\n",
    "print(\"\\n### Label Map ###\")\n",
    "print(f\"label_map = {label_map}\")\n",
    "\n",
    "print(\"\\n### Samples per Class ###\")\n",
    "for cls, count in class_stats.items():\n",
    "    print(f\"{cls}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal InputExamples generated: {guid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13324888-b09c-4286-b435-1e1151f2a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "\n",
    "    (\"P1\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ø¢Ù¾Ø³ Ù…ÛŒÚº {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P2\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©Û’ Ù…Ø¹Ù†ÛŒ {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P3\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Ø§ {\"placeholder\":\"text_b\"} Ø³Û’ ØªØ¹Ù„Ù‚ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P4\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ù…Ø¹Ù†ÛŒ Ú©Û’ Ù„Ø­Ø§Ø¸ Ø³Û’ {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P5\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ø§ÛŒÚ© Ø¯ÙˆØ³Ø±Û’ Ø³Û’ {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P6\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Ùˆ {\"placeholder\":\"text_b\"} Ø³Û’ Ù…Ù„Ø§ÛŒØ§ Ø¬Ø§Ø¦Û’ ØªÙˆ Ù†ØªÛŒØ¬Û {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P7\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©Ø§ Ù…ÙÛÙˆÙ… {\"mask\"} Ø¨Ù†ØªØ§ ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P8\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº {\"placeholder\":\"text_b\"} {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P9\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ù…ÛŒÚº Ù…Ø¹Ù†ÛŒ Ú©Ø§ Ø±Ø´ØªÛ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P10\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ø§ÛŒÚ© Ø¬ÛŒØ³Û’ ÛÙˆÙ†Û’ Ú©Û’ Ù„Ø­Ø§Ø¸ Ø³Û’ {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89b720-96cc-4d24-8708-ed389f2e4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalizer = {\n",
    "    \"unrelated\": [\"ØºÛŒØ±Ù…ØªØ¹Ù„Ù‚\", \"Ø§Ù„Ú¯\"],\n",
    "    \"distant\": [\"Ú©Ù…\", \"Ø¯ÙˆØ±\"],\n",
    "    \"similar\": [\"Ù…Ù„ØªÛ’\", \"Ù…Ø´Ø§Ø¨Û\"],\n",
    "    \"equivalent\": [\"ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹\", \"Ø¨Ø±Ø§Ø¨Ø±\"],\n",
    "    \"identical\": [\"Ø¨Ø§Ù„Ú©Ù„\", \"Ø§ÛŒÚ©\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514771bb-80b7-4f33-84f8-36e11d143d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2c77b4-b3e0-4855-b51d-7c81dec983cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4080 SUPER\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Imports\n",
    "# ==============================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptForClassification, PromptDataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "\n",
    "# ========================================\n",
    "# Check CUDA\n",
    "# ========================================\n",
    "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "\n",
    "# ========================================\n",
    "# Seeds for reproducibility\n",
    "# ========================================\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d15cfa69-e390-4f92-9345-ecdd31167243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Balanced Batch Sampler\n",
    "# ==============================\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \"\"\"\n",
    "        dataset: list of InputExample\n",
    "        batch_size: total batch size (must be divisible by number of classes)\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.labels = [ex.label for ex in dataset]\n",
    "        self.classes = list(sorted(set(self.labels)))\n",
    "        self.num_classes = len(self.classes)\n",
    "        assert batch_size % self.num_classes == 0, \"Batch size must be divisible by number of classes\"\n",
    "        self.batch_size_per_class = batch_size // self.num_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        class_indices = {c: np.where(np.array(self.labels) == c)[0].tolist() for c in self.classes}\n",
    "        for c in self.classes:\n",
    "            np.random.shuffle(class_indices[c])\n",
    "\n",
    "        num_batches = min(len(class_indices[c]) // self.batch_size_per_class for c in self.classes)\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            batch = []\n",
    "            for c in self.classes:\n",
    "                start = i * self.batch_size_per_class\n",
    "                end = start + self.batch_size_per_class\n",
    "                batch.extend(class_indices[c][start:end])\n",
    "            np.random.shuffle(batch)\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(np.where(np.array(self.labels) == c)[0]) // self.batch_size_per_class for c in self.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1765b4d-7856-454c-94be-17cf6f2e9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Training data (16-shot) ha,m and spam\n",
    "train_dataset = [\n",
    "    \n",
    "        InputExample(\n",
    "            guid=0,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø³Ú¯Ø±ÛŒÙ¹ Ù¾ÛŒ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø³Ú©ÛŒÙ¹Ù†Ú¯ Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=1,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ù„Ú©Ú¾ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª ØªÛŒØ± Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=2,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù†Ø§Ú† Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø§Øª Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=3,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù…Ú†Ú¾Ù„ÛŒ Ù¾Ú©Ú‘ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ ÙˆØ±Ø²Ø´ Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=4,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù„ÛØ±Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø§Øª Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=5,\n",
    "            text_a=\"Ø¯Ùˆ Ù„Ú‘Ú©Û’ Ú¯Ø§Ú‘ÛŒ Ú†Ù„Ø§ Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            text_b=\"Ø¯Ùˆ Ø¨Û’ Ø±Ù‚Øµ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=6,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ù†Ø§Ú† Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú©Ú¾Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=7,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¨Ù„ÛŒ Ù¾ÛŒØ§Ù†Ùˆ Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=8,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ù†Ø§Ú† Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø§Øª Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=9,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú©Ø´ØªÛŒ Ú†Ù„Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¹Ù…Ø§Ù¹Ø± Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=10,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú©ÛŒÙ†Ùˆ Ú©Ùˆ Ù¾ÛŒÚˆÙ„ Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ ÛØ§Ø±Ù¾ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=11,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ ØªÙ†Ø¯ÙˆØ± Ù…ÛŒÚº Ù¾ÛŒÙ† ÚˆØ§Ù„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ù¾ÛŒØ§Ø² Ú©Ø§Ù¹ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=12,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ù…ÛŒØ² Ù¾Ø± Ø¨ÛŒÙ¹Ú¾ÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ú¯Ø¯Ú¾Û’ Ù¾Ø± Ø³ÙˆØ§Ø± ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=13,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú©Ø§ØºØ° Ú©Ùˆ ØªÛÛ Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ú©Ø§Ù„ÛŒ Ù…Ø±Ú† Ú©Ø§Ù¹ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=14,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø´ÛŒØ´Û’ Ú©Û’ Ø¯Ø±ÙˆØ§Ø²Û’ Ú©ÛŒ ØµÙØ§Ø¦ÛŒ Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=15,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª ÙˆØ²Ù† Ú©ÛŒ Ù…Ø´Ù‚ÛŒÚº Ú©Ø± Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø§Ù¾Ù†Û’ Ø¨Ø§Ù„ Ø¨Ù†Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=0\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=16,\n",
    "            text_a=\"Ø¢Ø¯Ù…ÛŒ Ù¾ÛŒØ§Ù†Ùˆ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=17,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø³Ú‘Ú© Ù¾Ø± Ø¯ÙˆÚ‘ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù¾Ø§Ù†ÚˆØ§ Ú©ØªØ§ Ø³Ú‘Ú© Ù¾Ø± Ø¯ÙˆÚ‘ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=18,\n",
    "            text_a=\"Ø´ÛŒØ± Ú©Ø§ Ø¨Ú†Û Ú¯ÛŒÙ†Ø¯ Ø³Û’ Ú©Ú¾ÛŒÙ„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¨Ú†Û Ú¯Ú‘ÛŒØ§ Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ú¾ÛŒÙ„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=19,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø´Ø®Øµ Ù¹Ù…Ø§Ù¹Ø± Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø´Ø®Øµ Ú©Ú†Ú¾ Ú¯ÙˆØ´Øª Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=20,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¾ÛŒØ§Ù†Ùˆ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª ÙˆØ§Ø¦Ù„Ù† Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=21,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø¨Ø§Ù†Ø³Ø±ÛŒ Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=22,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ ÚˆÚ¾ÙˆÙ„ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=23,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¨Ù„ÛŒ Ú©Ù„ÛŒØ¯ÛŒ ØªØ®ØªÛ Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¯Ùˆ Ú©ÛŒ Ø¨ÙˆØ±Úˆ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=24,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù…ÙˆÙ¹Ø± Ø³Ø§Ø¦ÛŒÚ©Ù„ Ú†Ù„Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ú¾ÙˆÚ‘Û’ Ù¾Ø± Ø³ÙˆØ§Ø± ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=25,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù…ÙˆÙ¹Ø± Ø³Ø§Ø¦ÛŒÚ©Ù„ Ú†Ù„Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ú¾ÙˆÚ‘Û’ Ù¾Ø± Ø³ÙˆØ§Ø± ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=26,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¾ÛŒØ§Ù†Ùˆ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ ØµÙˆØ± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=27,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ù¹Ù…Ø§Ù¹Ø± Ú©Ø§Ù¹ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¾ÛŒØ§Ø² Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=28,\n",
    "            text_a=\"Ø§ÛŒÚ© Ù…Ø±Ø¯ Ø§ÙˆØ± Ø¹ÙˆØ±Øª Ø¨Ø§Øª Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù…Ø±Ø¯ Ø§ÙˆØ± Ø¹ÙˆØ±Øª Ú©Ú¾Ø§ Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=29,\n",
    "            text_a=\"Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø§Ù†Ø³Ø±ÛŒ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=30,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¾ÛŒØ§Ù†Ùˆ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø¢Ø¯Ù…ÛŒ ÙˆØ§Ø¦Ù„Ù† Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=31,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø³Ú‘Ú© Ù¾Ø± Ø¯ÙˆÚ‘ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ú©Ø§Ø± Ø³Ú‘Ú© Ù¾Ø± Ú†Ù„ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=1\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=32,\n",
    "            text_a=\"ØªÛŒÙ† Ø¢Ø¯Ù…ÛŒ Ø´Ø·Ø±Ù†Ø¬ Ú©Ú¾ÛŒÙ„ Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            text_b=\"Ø¯Ùˆ Ø¢Ø¯Ù…ÛŒ Ø´Ø·Ø±Ù†Ø¬ Ú©Ú¾ÛŒÙ„ Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=33,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Ø§ÙˆØ± Ú¯Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª ØµÙˆØªÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Ø§ÙˆØ± Ú¯Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=34,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù„Ú‘Ú©ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=35,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø§ÛŒÚ© Ø¨Ù† Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¾ÛŒØ§Ø² Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=36,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ù¾ÛŒØ§Ø² Ú©Ø§Ù¹ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¾ÛŒØ§Ø² Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=37,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¢Ù„Ùˆ Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ø§Ø¬Ø± Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=38,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Ø§ÙˆØ± Ú¯Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=39,\n",
    "            text_a=\"Ø´ÛŒØ± Ú©Ø§ Ø¨Ú†Û Ø¢ÙˆØ§Ø² Ø§Ù¹Ú¾Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø´ÛŒØ± Ø§Ø¯Ú¾Ø± Ø§Ø¯Ú¾Ø± Ú¯Ú¾ÙˆÙ… Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=40,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø´Ø®Øµ Ù¾ÛŒØ§Ø² Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø´Ø®Øµ Ù¾ÛŒØ§Ø² Ú†Ú¾ÛŒÙ„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=41,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø¢Ù„Ùˆ Ú†Ú¾ÛŒÙ„ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø³ÛŒØ¨ Ú†Ú¾ÛŒÙ„ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=42,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø´Ø®Øµ Ù¾ÛŒØ§Ø² Ú†Ú¾ÛŒÙ„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø´Ø®Øµ Ø¨ÛŒÙ†Ú¯Ù† Ú†Ú¾ÛŒÙ„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=43,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ØªØ§ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=44,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¾Ø§Ù†ÛŒ Ú©Û’ Ø§Ù†Ø¯Ø± ØªÛŒØ±ØªØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ù¾Ø§Ù†ÛŒ Ú©Û’ Ø§Ù†Ø¯Ø± ØªÛŒØ± Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=45,\n",
    "            text_a=\"Ù…Ø±Ø¯ Ú©Ø±Ú©Ù¹ Ú©Ú¾ÛŒÙ„ Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            text_b=\"Ù…Ø±Ø¯ Ø¨Ø§Ø³Ú©Ù¹ Ø¨Ø§Ù„ Ú©Ú¾ÛŒÙ„ Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=46,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ú¾ÙˆÚ‘Û’ Ù¾Ø± Ø³ÙˆØ§Ø± ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù„Ú‘Ú©ÛŒ Ú¯Ú¾ÙˆÚ‘Û’ Ù¾Ø± Ø³ÙˆØ§Ø± ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=47,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø³Ø§Ø¦ÛŒÚ©Ù„ Ú†Ù„Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¨Ù†Ø¯Ø± Ø³Ø§Ø¦ÛŒÚ©Ù„ Ú†Ù„Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=2\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=48,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ú‘ÛŒ Ø¨Ø§Ù†Ø³Ø±ÛŒ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø§Ù†Ø³Ø±ÛŒ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=49,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¾ÛŒØ²Ø§ Ù¾Ø± Ú©Ù¹Ø§ ÛÙˆØ§ Ù¾Ù†ÛŒØ± Ù¾Ú¾ÛŒÙ„Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨ØºÛŒØ± Ù¾Ú©Û’ ÛÙˆØ¦Û’ Ù¾ÛŒØ²Ø§ Ù¾Ø± Ú©Ù¹Ø§ ÛÙˆØ§ Ù¾Ù†ÛŒØ± Ù¾Ú¾ÛŒÙ„Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=50,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø§Ù†Ø³Ø±ÛŒ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø§Ù†Ø³ Ú©ÛŒ Ø¨Ø§Ù†Ø³Ø±ÛŒ Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=51,\n",
    "            text_a=\"Ø§ÛŒÚ© Ú©ØªØ§ Ø§Ù¾Ù†ÛŒ Ù¾ÛŒÙ¹Ú¾ Ø³Û’ Ø¨ÛŒÚ©Ù† Ø§ØªØ§Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ú©ØªØ§ Ø§Ù¾Ù†ÛŒ Ù¾ÛŒÙ¹Ú¾ Ù¾Ø± Ù„Ú¯Û’ Ø¨ÛŒÚ©Ù† Ú©Ùˆ Ú©Ú¾Ø§Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=52,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¨Ù„ÛŒ Ø¨Ú†Û’ Ú©Û’ Ú†ÛØ±Û’ Ù¾Ø± Ø±Ú¯Ú‘ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¨Ù„ÛŒ Ø¨Ú†Û’ Ú©Û’ Ø®Ù„Ø§Ù Ø±Ú¯Ú‘ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=53,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø±ØªÙ† Ù…ÛŒÚº ØªÛŒÙ„ ÚˆØ§Ù„ØªØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø±ØªÙ† Ù…ÛŒÚº Ø´Ø±Ø§Ø¨ ÚˆØ§Ù„ØªØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=54,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ú©Ú†Ú¾ Ú©Ú¾Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ú¯ÙˆØ´Øª Ú©Ú¾Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=55,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø¯ÙˆØ³Ø±ÛŒ Ø¹ÙˆØ±ØªÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ Ù†Ø§Ú† Ø±ÛÛŒ ÛÛ’ Ø§ÙˆØ± Ú¯Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø¨Ø§Ø±Ø´ Ù…ÛŒÚº Ù†Ø§Ú† Ø±ÛÛŒ ÛÛ’ Ø§ÙˆØ± Ú¯Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=56,\n",
    "            text_a=\"Ø´ÛŒØ± Ù„ÙˆÚ¯ÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ú¾ÛŒÙ„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø´ÛŒØ± Ø¯Ùˆ Ø¢Ø¯Ù…ÛŒÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ú¾ÛŒÙ„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=57,\n",
    "            text_a=\"Ú©ÙˆØ¦ÛŒ Ù…Ø¬Ø³Ù…Û ØªØ±Ø§Ø´ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù…Ø¬Ø³Ù…Û ØªØ±Ø§Ø´ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=58,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø§Ù„ÛŒÚ©Ù¹Ø±Ú© Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=59,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¨Ú†Û Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù„Ú‘Ú©Ø§ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=60,\n",
    "            text_a=\"Ø§ÛŒÚ© Ù„Ú‘Ú©Ø§ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=61,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù„Ú‘Ú©Ø§ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=62,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø¨Ø±Ù‚ÛŒ Ú¯Ù¹Ø§Ø± Ø¨Ø¬Ø§ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=63,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ù¾ÛŒØ§Ø² Ú©Ø§Ù¹ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ù¾ÛŒØ§Ø² Ú©Ø§Ù¹ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=3\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=64,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø·ÛŒØ§Ø±Û Ø§Ú‘Ø§Ù† Ø¨Ú¾Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© ÛÙˆØ§Ø¦ÛŒ Ø¬ÛØ§Ø² Ø§Ú‘Ø§Ù† Ø¨Ú¾Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=65,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ø³ÛŒÙ„Ùˆ Ú©Ú¾ÛŒÙ„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø¨ÛŒÙ¹Ú¾Ø§ ÛÙˆØ§ Ø¢Ø¯Ù…ÛŒ Ø³ÛŒÙ„Ùˆ Ú©Ú¾ÛŒÙ„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=66,\n",
    "            text_a=\"Ú©Ú†Ú¾ Ø¢Ø¯Ù…ÛŒ Ù„Ú‘ Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            text_b=\"Ø¯Ùˆ Ø¢Ø¯Ù…ÛŒ Ù„Ú‘ Ø±ÛÛ’ ÛÛŒÚº Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=67,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø´Ø®Øµ Ø§ÛŒÚ© Ø¨Ù„ÛŒ Ú©Ùˆ Ú†Ú¾Øª Ù¾Ø± Ù¾Ú¾ÛŒÙ†Ú© Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø´Ø®Øµ Ø§ÛŒÚ© Ø¨Ù„ÛŒ Ú©Ùˆ Ú†Ú¾Øª Ù¾Ø± Ù¾Ú¾ÛŒÙ†Ú© Ø¯ÛŒØªØ§ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=68,\n",
    "            text_a=\"Ø¢Ø¯Ù…ÛŒ Ù†Û’ Ø¯ÙˆØ³Ø±Û’ Ø¢Ø¯Ù…ÛŒ Ú©Ùˆ Ú†Ú¾Ú‘ÛŒ Ø³Û’ Ù…Ø§Ø±Ø§ Û”\",\n",
    "            text_b=\"Ø§Ø³ Ø´Ø®Øµ Ù†Û’ Ø¯ÙˆØ³Ø±Û’ Ø¢Ø¯Ù…ÛŒ Ú©Ùˆ Ú†Ú¾Ú‘ÛŒ Ø³Û’ Ù…Ø§Ø±Ø§ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=69,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø§Ù¹Ú¾Ø§ØªÛŒ ÛÛ’ Ø§ÙˆØ± Ø¨Ú†Û’ Ú©Û’ Ú©ÛŒÙ†Ú¯Ø±Ùˆ Ú©Ùˆ Ù¾Ú©Ú‘ØªÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø§Ù¹Ú¾Ø§ØªÛŒ ÛÛ’ Ø§ÙˆØ± Ø§Ù¾Ù†Û’ Ø¨Ø§Ø²ÙˆØ¤Úº Ù…ÛŒÚº Ú©ÛŒÙ†Ú¯Ø±Ùˆ Ú©Û’ Ø¨Ú†Û’ Ú©Ùˆ Ù¾Ú©Ú‘ØªÛŒ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=70,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø´Ø®Øµ Ú©Ø§ØºØ° Ú©Ø§ Ø§ÛŒÚ© Ù¹Ú©Ú‘Ø§ ØªÛÛ Ú©Ø± Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ú©ÙˆØ¦ÛŒ Ú©Ø§ØºØ° Ú©Ø§ Ù¹Ú©Ú‘Ø§ Ø¬ÙˆÚ‘ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=71,\n",
    "            text_a=\"Ù‚Ø·Ø¨ÛŒ Ø±ÛŒÚ†Ú¾ Ø¨Ø±Ù Ù¾Ø± Ù¾Ú¾Ø³Ù„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù‚Ø·Ø¨ÛŒ Ø±ÛŒÚ†Ú¾ Ø¨Ø±Ù Ú©Û’ Ù¾Ø§Ø± Ù¾Ú¾Ø³Ù„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=72,\n",
    "            text_a=\"Ø¢Ø¯Ù…ÛŒ Ú¯Ú¾ÙˆÚ‘Û’ Ù¾Ø± Ø³ÙˆØ§Ø± ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¢Ø¯Ù…ÛŒ Ú¯Ú¾ÙˆÚ‘Û’ Ù¾Ø± Ø³ÙˆØ§Ø± ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=73,\n",
    "            text_a=\"Ø§ÛŒÚ© Ù¾Ø§Ù†ÚˆØ§ Ø§ÛŒÚ© Ø³Ù„Ø§Ø¦ÛŒÚˆ Ø³Û’ Ù†ÛŒÚ†Û’ Ù¾Ú¾Ø³Ù„ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù¾Ø§Ù†ÚˆØ§ Ø§ÛŒÚ© Ø³Ù„Ø§Ø¦ÛŒÚˆ Ø³Û’ Ù†ÛŒÚ†Û’ Ù¾Ú¾Ø³Ù„ØªØ§ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=74,\n",
    "            text_a=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø¢Ù„Ùˆ Ú†Ú¾ÛŒÙ„ØªÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø¢Ù„Ùˆ Ú†Ú¾ÛŒÙ„ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=75,\n",
    "            text_a=\"Ù„Ú‘Ú©Ø§ Ø§Ù¾Ù†ÛŒ Ù…ÙˆÙ¹Ø± Ø³Ø§Ø¦ÛŒÚ©Ù„ Ø³Û’ Ú¯Ø± Ú¯ÛŒØ§ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù„Ú‘Ú©Ø§ Ø§Ù¾Ù†ÛŒ Ù…ÙˆÙ¹Ø± Ø³Ø§Ø¦ÛŒÚ©Ù„ Ø³Û’ Ú¯Ø± Ø¬Ø§ØªØ§ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=76,\n",
    "            text_a=\"Ø¹ÙˆØ±Øª Ø¨Ø§Ù†Ø³Ø±ÛŒ Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø¨Ø§Ù†Ø³Ø±ÛŒ Ø¨Ø¬Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=77,\n",
    "            text_a=\"Ø®Ø±Ú¯ÙˆØ´ Ø¹Ù‚Ø§Ø¨ Ø³Û’ Ø¯ÙˆÚ‘ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø®Ø±Ú¯ÙˆØ´ Ø¹Ù‚Ø§Ø¨ Ø³Û’ Ø¯ÙˆÚ‘ Ø±ÛØ§ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=78,\n",
    "            text_a=\"Ø¹ÙˆØ±Øª Ø±ÙˆÙ¹ÛŒ ÙˆØ§Ù„Û’ Ø³ÙˆØ± Ú©Û’ Ú¯ÙˆØ´Øª Ú©Û’ Ù¹Ú©Ú‘Û’ Ú©Ùˆ ØªÙ„ÛŒ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ø¹ÙˆØ±Øª Ø±ÙˆÙ¹ÛŒ ÙˆØ§Ù„Ø§ Ø³ÙˆØ± Ú©Ø§ Ú¯ÙˆØ´Øª Ù¾Ú©Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        InputExample(\n",
    "            guid=79,\n",
    "            text_a=\"Ø§ÛŒÚ© Ù„Ú‘Ú©ÛŒ Ù¾ØªÙ†Ú¯ Ø§Ú‘Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            text_b=\"Ø§ÛŒÚ© Ù„Ú‘Ú©ÛŒ Ø¯ÙˆÚ‘ØªÛŒ ÛÙˆØ¦ÛŒ Ù¾ØªÙ†Ú¯ Ø§Ú‘Ø§ Ø±ÛÛŒ ÛÛ’ Û”\",\n",
    "            label=4\n",
    "        ),\n",
    "        \n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28dbab7-839f-43b0-9099-e796a5326a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### Classes ###\n",
    "classes = ['unrelated', 'distant', 'similar', 'equivalent', 'identical']\n",
    "\n",
    "### Label Map ###\n",
    "label_map = {'unrelated': 0, 'distant': 1, 'similar': 2, 'equivalent': 3, 'identical': 4}\n",
    "\n",
    "\n",
    "\n",
    "# # # Step 1: Use load_plm with 'roberta' to get the correct WrapperClass\n",
    "# _, _, _, WrapperClass = load_plm(\"roberta\", \"roberta-base\")  # Just to get the wrapper\n",
    "\n",
    "# # # # Step 2: Manually load XLM-RoBERTa model/tokenizer\n",
    "# model_name = \"xlm-roberta-base\"\n",
    "# tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "# plm = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# ==============================\n",
    "# Load Pretrained Language Model (mBERT)\n",
    "# ==============================\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55b9782-f3ef-494c-8acf-ffa3ad35f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Define Prompt Template (Manual)\n",
    "# ==============================\n",
    "template = ManualTemplate(\n",
    "    text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ø¢Ù¾Ø³ Ù…ÛŒÚº {\"mask\"} ÛÛŒÚºÛ”',\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "templates = [\n",
    "\n",
    "    (\"P1\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ø¢Ù¾Ø³ Ù…ÛŒÚº {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P2\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©Û’ Ù…Ø¹Ù†ÛŒ {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P3\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Ø§ {\"placeholder\":\"text_b\"} Ø³Û’ ØªØ¹Ù„Ù‚ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P4\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ù…Ø¹Ù†ÛŒ Ú©Û’ Ù„Ø­Ø§Ø¸ Ø³Û’ {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P5\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ø§ÛŒÚ© Ø¯ÙˆØ³Ø±Û’ Ø³Û’ {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P6\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Ùˆ {\"placeholder\":\"text_b\"} Ø³Û’ Ù…Ù„Ø§ÛŒØ§ Ø¬Ø§Ø¦Û’ ØªÙˆ Ù†ØªÛŒØ¬Û {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P7\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ú©Ø§ Ù…ÙÛÙˆÙ… {\"mask\"} Ø¨Ù†ØªØ§ ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P8\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº {\"placeholder\":\"text_b\"} {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P9\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ù…ÛŒÚº Ù…Ø¹Ù†ÛŒ Ú©Ø§ Ø±Ø´ØªÛ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "\n",
    "    (\"P10\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± {\"placeholder\":\"text_b\"} Ø§ÛŒÚ© Ø¬ÛŒØ³Û’ ÛÙˆÙ†Û’ Ú©Û’ Ù„Ø­Ø§Ø¸ Ø³Û’ {\"mask\"} ÛÛŒÚºÛ”',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "verbalizer = ManualVerbalizer(\n",
    "    classes=classes,\n",
    "    label_words={\n",
    "        \"unrelated\": [\"ØºÛŒØ±Ù…ØªØ¹Ù„Ù‚\", \"Ø§Ù„Ú¯\"],\n",
    "        \"distant\": [\"Ú©Ù…\", \"Ø¯ÙˆØ±\"],\n",
    "        \"similar\": [\"Ù…Ù„ØªÛ’\", \"Ù…Ø´Ø§Ø¨Û\"],\n",
    "        \"equivalent\": [\"ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹\", \"Ø¨Ø±Ø§Ø¨Ø±\"],\n",
    "        \"identical\": [\"Ø¨Ø§Ù„Ú©Ù„\", \"Ø§ÛŒÚ©\"]\n",
    "    },\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e91e42d-a240-4941-b515-3df7fea891dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¦ Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 2667.37it/s]\n",
      "tokenizing: 80it [00:00, 1552.15it/s]\n",
      "tokenizing: 80it [00:00, 1798.18it/s]\n",
      "tokenizing: 80it [00:00, 1857.22it/s]\n",
      "tokenizing: 80it [00:00, 1848.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 13.0265\n",
      "Prompt pattern: ['P2', 'P1', 'P5', 'P4']\n",
      "\n",
      "ğŸŸ¦ Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1909.32it/s]\n",
      "tokenizing: 80it [00:00, 1792.24it/s]\n",
      "tokenizing: 80it [00:00, 1762.24it/s]\n",
      "tokenizing: 80it [00:00, 1960.86it/s]\n",
      "tokenizing: 80it [00:00, 1447.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 7.5653\n",
      "Prompt pattern: ['P3', 'P2', 'P9', 'P2']\n",
      "\n",
      "ğŸŸ¦ Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1477.65it/s]\n",
      "tokenizing: 80it [00:00, 1196.24it/s]\n",
      "tokenizing: 80it [00:00, 1494.54it/s]\n",
      "tokenizing: 80it [00:00, 1579.17it/s]\n",
      "tokenizing: 80it [00:00, 1432.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.6010\n",
      "Prompt pattern: ['P7', 'P1', 'P1', 'P2']\n",
      "\n",
      "ğŸŸ¦ Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1317.19it/s]\n",
      "tokenizing: 80it [00:00, 1596.67it/s]\n",
      "tokenizing: 80it [00:00, 1554.26it/s]\n",
      "tokenizing: 80it [00:00, 1736.65it/s]\n",
      "tokenizing: 80it [00:00, 1112.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 7.7190\n",
      "Prompt pattern: ['P4', 'P9', 'P10', 'P1']\n",
      "\n",
      "ğŸŸ¦ Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1149.42it/s]\n",
      "tokenizing: 80it [00:00, 1457.02it/s]\n",
      "tokenizing: 80it [00:00, 950.10it/s]\n",
      "tokenizing: 80it [00:00, 1490.98it/s]\n",
      "tokenizing: 80it [00:00, 1549.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 6.6617\n",
      "Prompt pattern: ['P4', 'P9', 'P7', 'P4']\n",
      "\n",
      "ğŸŸ¦ Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 579.89it/s]\n",
      "tokenizing: 80it [00:00, 1591.01it/s]\n",
      "tokenizing: 80it [00:00, 810.18it/s]\n",
      "tokenizing: 80it [00:00, 1368.16it/s]\n",
      "tokenizing: 80it [00:00, 1341.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 7.0889\n",
      "Prompt pattern: ['P10', 'P5', 'P1', 'P3']\n",
      "\n",
      "ğŸŸ¦ Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 943.53it/s]\n",
      "tokenizing: 80it [00:00, 1583.62it/s]\n",
      "tokenizing: 80it [00:00, 1220.01it/s]\n",
      "tokenizing: 80it [00:00, 1304.89it/s]\n",
      "tokenizing: 80it [00:00, 1385.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 6.8285\n",
      "Prompt pattern: ['P6', 'P5', 'P3', 'P4']\n",
      "\n",
      "ğŸŸ¦ Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1061.17it/s]\n",
      "tokenizing: 80it [00:00, 1502.30it/s]\n",
      "tokenizing: 80it [00:00, 1583.59it/s]\n",
      "tokenizing: 80it [00:00, 1380.22it/s]\n",
      "tokenizing: 80it [00:00, 1423.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 6.6060\n",
      "Prompt pattern: ['P2', 'P2', 'P7', 'P2']\n",
      "\n",
      "ğŸŸ¦ Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1398.71it/s]\n",
      "tokenizing: 80it [00:00, 1376.83it/s]\n",
      "tokenizing: 80it [00:00, 1603.63it/s]\n",
      "tokenizing: 80it [00:00, 1354.86it/s]\n",
      "tokenizing: 80it [00:00, 1694.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 6.5582\n",
      "Prompt pattern: ['P6', 'P10', 'P5', 'P1']\n",
      "\n",
      "ğŸŸ¦ Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1255.61it/s]\n",
      "tokenizing: 80it [00:00, 1615.42it/s]\n",
      "tokenizing: 80it [00:00, 1260.80it/s]\n",
      "tokenizing: 80it [00:00, 1561.87it/s]\n",
      "tokenizing: 80it [00:00, 975.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 6.3308\n",
      "Prompt pattern: ['P9', 'P2', 'P7', 'P2']\n",
      "\n",
      "ğŸŸ¦ Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1100.87it/s]\n",
      "tokenizing: 80it [00:00, 1492.63it/s]\n",
      "tokenizing: 80it [00:00, 1126.10it/s]\n",
      "tokenizing: 80it [00:00, 1400.75it/s]\n",
      "tokenizing: 80it [00:00, 1384.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 6.0831\n",
      "Prompt pattern: ['P5', 'P10', 'P6', 'P10']\n",
      "\n",
      "ğŸŸ¦ Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1533.97it/s]\n",
      "tokenizing: 80it [00:00, 1069.86it/s]\n",
      "tokenizing: 80it [00:00, 1334.22it/s]\n",
      "tokenizing: 80it [00:00, 1178.35it/s]\n",
      "tokenizing: 80it [00:00, 1417.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 5.9280\n",
      "Prompt pattern: ['P2', 'P1', 'P4', 'P5']\n",
      "\n",
      "ğŸŸ¦ Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1486.81it/s]\n",
      "tokenizing: 80it [00:00, 1148.73it/s]\n",
      "tokenizing: 80it [00:00, 1497.97it/s]\n",
      "tokenizing: 80it [00:00, 1689.93it/s]\n",
      "tokenizing: 80it [00:00, 1650.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 5.9304\n",
      "Prompt pattern: ['P4', 'P2', 'P7', 'P5']\n",
      "\n",
      "ğŸŸ¦ Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1400.78it/s]\n",
      "tokenizing: 80it [00:00, 1473.69it/s]\n",
      "tokenizing: 80it [00:00, 1303.46it/s]\n",
      "tokenizing: 80it [00:00, 1563.25it/s]\n",
      "tokenizing: 80it [00:00, 636.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 6.1138\n",
      "Prompt pattern: ['P6', 'P3', 'P6', 'P6']\n",
      "\n",
      "ğŸŸ¦ Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1547.48it/s]\n",
      "tokenizing: 80it [00:00, 1438.30it/s]\n",
      "tokenizing: 80it [00:00, 996.19it/s]\n",
      "tokenizing: 80it [00:00, 1599.89it/s]\n",
      "tokenizing: 80it [00:00, 1192.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 5.6005\n",
      "Prompt pattern: ['P5', 'P2', 'P10', 'P3']\n",
      "\n",
      "ğŸŸ¦ Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1425.21it/s]\n",
      "tokenizing: 80it [00:00, 1530.38it/s]\n",
      "tokenizing: 80it [00:00, 1786.43it/s]\n",
      "tokenizing: 80it [00:00, 1490.31it/s]\n",
      "tokenizing: 80it [00:00, 1627.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 5.3829\n",
      "Prompt pattern: ['P4', 'P3', 'P8', 'P7']\n",
      "\n",
      "ğŸŸ¦ Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1475.36it/s]\n",
      "tokenizing: 80it [00:00, 1159.68it/s]\n",
      "tokenizing: 80it [00:00, 1359.10it/s]\n",
      "tokenizing: 80it [00:00, 880.56it/s]\n",
      "tokenizing: 80it [00:00, 1441.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 5.3844\n",
      "Prompt pattern: ['P9', 'P4', 'P6', 'P1']\n",
      "\n",
      "ğŸŸ¦ Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1461.43it/s]\n",
      "tokenizing: 80it [00:00, 1154.15it/s]\n",
      "tokenizing: 80it [00:00, 1121.22it/s]\n",
      "tokenizing: 80it [00:00, 1524.18it/s]\n",
      "tokenizing: 80it [00:00, 1448.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 4.9955\n",
      "Prompt pattern: ['P1', 'P6', 'P7', 'P5']\n",
      "\n",
      "ğŸŸ¦ Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 879.11it/s]\n",
      "tokenizing: 80it [00:00, 1610.92it/s]\n",
      "tokenizing: 80it [00:00, 1403.79it/s]\n",
      "tokenizing: 80it [00:00, 1415.67it/s]\n",
      "tokenizing: 80it [00:00, 1344.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 4.7615\n",
      "Prompt pattern: ['P4', 'P10', 'P6', 'P4']\n",
      "\n",
      "ğŸŸ¦ Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 80it [00:00, 1369.09it/s]\n",
      "tokenizing: 80it [00:00, 1591.01it/s]\n",
      "tokenizing: 80it [00:00, 1430.35it/s]\n",
      "tokenizing: 80it [00:00, 1391.97it/s]\n",
      "tokenizing: 80it [00:00, 1592.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 4.9410\n",
      "Prompt pattern: ['P7', 'P8', 'P3', 'P5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Create Prompt Model\n",
    "# ==============================\n",
    "prompt_model = PromptForClassification(\n",
    "    template=template,\n",
    "    plm=plm,\n",
    "    verbalizer=verbalizer\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Training loop with BalancedBatchSampler + random template switching\n",
    "# ==============================\n",
    "T = 20   # epochs\n",
    "K = 1    # steps per prompt\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt_model.train()\n",
    "optimizer = AdamW(prompt_model.parameters(), lr=1e-5)\n",
    "all_epoch_patterns = {}\n",
    "\n",
    "for epoch in range(T):\n",
    "    print(f\"\\nğŸŸ¦ Epoch {epoch+1}/{T}\")\n",
    "\n",
    "    # Random initial template\n",
    "    prompt_name, current_template = random.choice(templates)\n",
    "    epoch_pattern = []\n",
    "\n",
    "    # Create PromptDataLoader with BalancedBatchSampler\n",
    "    sampler = BalancedBatchSampler(train_dataset, batch_size=batch_size)\n",
    "    train_loader = PromptDataLoader(\n",
    "        dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        template=current_template,\n",
    "        tokenizer_wrapper_class=WrapperClass,\n",
    "        max_seq_length=128,\n",
    "        batch_size=batch_size,\n",
    "        batch_sampler=sampler,\n",
    "        shuffle=False  # shuffle is ignored when batch_sampler is used\n",
    "    )\n",
    "\n",
    "    step_counter = 0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move batch to device\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        logits = prompt_model(batch)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "        # loss = criterion(logits, batch[\"label\"])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_pattern.append(prompt_name)\n",
    "\n",
    "        step_counter += 1\n",
    "\n",
    "        # Switch template every K steps\n",
    "        if step_counter % K == 0:\n",
    "            prompt_name, current_template = random.choice(templates)\n",
    "\n",
    "            # Rebuild PromptDataLoader with new template but same sampler\n",
    "            train_loader = PromptDataLoader(\n",
    "                dataset=train_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                template=current_template,\n",
    "                tokenizer_wrapper_class=WrapperClass,\n",
    "                max_seq_length=128,\n",
    "                batch_size=batch_size,\n",
    "                batch_sampler=sampler,\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "    all_epoch_patterns[f\"epoch_{epoch+1}\"] = epoch_pattern\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Prompt pattern: {epoch_pattern}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8643ddd6-2f7d-40f1-bc7e-9c86f869adec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1500it [00:01, 914.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š STS_B Urdu Dev Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   unrelated     0.3952    0.2849    0.3311       344\n",
      "     distant     0.2207    0.2520    0.2353       254\n",
      "     similar     0.2000    0.0769    0.1111       273\n",
      "  equivalent     0.3139    0.4575    0.3724       365\n",
      "   identical     0.3200    0.3939    0.3531       264\n",
      "\n",
      "    accuracy                         0.3027      1500\n",
      "   macro avg     0.2900    0.2930    0.2806      1500\n",
      "weighted avg     0.2971    0.3027    0.2888      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Load Evaluation Dataset\n",
    "# ==============================\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\stdFurqan\\Desktop\\paft\\STS-B\\Final_dev_labeled.csv\"\n",
    ")\n",
    "\n",
    "# Make InputExamples\n",
    "eval_dataset = [\n",
    "    InputExample(\n",
    "        guid=i,\n",
    "        text_a=row['sentence1'],\n",
    "        text_b=row['sentence2'],\n",
    "        label=label_map[row['score_to_labels']]\n",
    "    )\n",
    "    for i, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# ==============================\n",
    "# PromptDataLoader\n",
    "# ==============================\n",
    "eval_loader = PromptDataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=template,              # your ManualTemplate for XNLI\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Evaluate Model\n",
    "# ==============================\n",
    "prompt_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        logits = prompt_model(batch)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(batch['label'].cpu().tolist())\n",
    "\n",
    "# ==============================\n",
    "# Print Classification Report\n",
    "# ==============================\n",
    "print(\"\\nğŸ“Š STS_B Urdu Dev Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716e78e-8ff8-4fcf-89fd-fddbcaf51360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f90e0-ee47-4f41-bd18-4cbb63415d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta\n",
    "\n",
    "ğŸ“Š WSTS-B Urdu Dev Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "   unrelated     0.4710    0.3547    0.4046       344\n",
    "     distant     0.2857    0.0157    0.0299       254\n",
    "     similar     0.0000    0.0000    0.0000       273\n",
    "  equivalent     0.3121    0.2959    0.3038       365\n",
    "   identical     0.2007    0.6629    0.3081       264\n",
    "\n",
    "    accuracy                         0.2727      1500\n",
    "   macro avg     0.2539    0.2658    0.2093      1500\n",
    "weighted avg     0.2677    0.2727    0.2260      1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec6a47-58f9-4629-9a6c-2ed068a70442",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert\n",
    "\n",
    "ğŸ“Š STS_B Urdu Dev Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "   unrelated     0.3952    0.2849    0.3311       344\n",
    "     distant     0.2207    0.2520    0.2353       254\n",
    "     similar     0.2000    0.0769    0.1111       273\n",
    "  equivalent     0.3139    0.4575    0.3724       365\n",
    "   identical     0.3200    0.3939    0.3531       264\n",
    "\n",
    "    accuracy                         0.3027      1500\n",
    "   macro avg     0.2900    0.2930    0.2806      1500\n",
    "weighted avg     0.2971    0.3027    0.2888      1500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
