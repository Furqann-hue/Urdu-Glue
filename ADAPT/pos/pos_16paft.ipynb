{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc284179-1884-4b4c-89a3-7faca5a41529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\stdFurqan\\Desktop\\paft\\pos\\train.csv\")\n",
    "\n",
    "# Preserve tag order\n",
    "unique_tags = df['tag'].unique().tolist()\n",
    "\n",
    "# Label mapping\n",
    "label_map = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "classes = unique_tags.copy()\n",
    "\n",
    "# Collect UNIQUE words per tag (order preserved)\n",
    "tag_words = defaultdict(list)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    tag = row['tag']\n",
    "    word = str(row['word']).strip()\n",
    "\n",
    "    if word not in tag_words[tag]:\n",
    "        tag_words[tag].append(word)\n",
    "\n",
    "# Generate InputExamples\n",
    "print(\"### InputExamples ###\\n\")\n",
    "\n",
    "guid = 0\n",
    "tag_stats = {}  # to track how many examples per tag\n",
    "\n",
    "for tag in unique_tags:\n",
    "    label = label_map[tag]\n",
    "\n",
    "    # Take min(16, available unique words)\n",
    "    selected_words = tag_words[tag][:16]\n",
    "    tag_stats[tag] = len(selected_words)\n",
    "\n",
    "    for word in selected_words:\n",
    "        print(f'InputExample(guid={guid}, text_a=\"{word}\", label={label}),')\n",
    "        guid += 1\n",
    "\n",
    "# Print metadata\n",
    "print(\"\\n### Classes ###\")\n",
    "print(f\"classes = {classes}\")\n",
    "\n",
    "print(\"\\n### Label Map ###\")\n",
    "print(f\"label_map = {label_map}\")\n",
    "\n",
    "# Optional: show summary\n",
    "print(\"\\n### Samples per Tag ###\")\n",
    "for tag, count in tag_stats.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal InputExamples generated: {guid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39271396-a3c1-4862-be8b-8ce0a7135afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "\n",
    "templates = [\n",
    "    (\"P1\", ManualTemplate(\n",
    "        text='ŸÑŸÅÿ∏: {\"placeholder\":\"text_a\"} ‚Üí ÿ≠ÿµ€Å ⁄©ŸÑÿßŸÖ: {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P2\", ManualTemplate(\n",
    "        text='ÿ¨ŸÖŸÑ€í ŸÖ€å⁄∫ €å€Å ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ⁄©ÿ≥ ŸÇÿ≥ŸÖ ⁄©ÿß €Å€í: {\"mask\"}€î',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P3\", ManualTemplate(\n",
    "        text='ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ⁄©ÿß POS tag ⁄©€åÿß €Å€íÿü {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P4\", ManualTemplate(\n",
    "        text='€å€Å ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ÿ¨ŸÖŸÑ€í ŸÖ€å⁄∫ ⁄©ÿ≥ ÿ≤ŸÖÿ±€í ⁄©ÿß €Å€íÿü {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P5\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} ‚Üí ÿ≠ÿµ€Å ⁄©ŸÑÿßŸÖ: {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P6\", ManualTemplate(\n",
    "        text='ŸÑŸÅÿ∏: {\"placeholder\":\"text_a\"} ÿßÿ≥ ⁄©ÿß ÿ≠ÿµ€Å ⁄©ŸÑÿßŸÖ {\"mask\"} €Å€í€î',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P7\", ManualTemplate(\n",
    "        text='POS tag for {\"placeholder\":\"text_a\"} is {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P8\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} ⁄©ÿ≥ ŸÇÿ≥ŸÖ ⁄©ÿß ŸÑŸÅÿ∏ €Å€í: {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P9\", ManualTemplate(\n",
    "        text='ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ‚Üí POS: {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P10\", ManualTemplate(\n",
    "        text='ÿ¨ŸÖŸÑ€í ŸÖ€å⁄∫ ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ⁄©ÿ≥ POS category ÿ≥€í ÿ™ÿπŸÑŸÇ ÿ±⁄©⁄æÿ™ÿß €Å€íÿü {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26973e50-e1d7-49b9-b869-eeba6247cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\stdFurqan\\Desktop\\paft\\pos\\train.csv\")  # columns: 'word', 'tag'\n",
    "\n",
    "# Find unique tags\n",
    "unique_tags = df['tag'].unique()\n",
    "print(\"Unique POS tags:\", unique_tags)\n",
    "\n",
    "# Create label map\n",
    "label_map = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "print(\"Label map:\", label_map)\n",
    "\n",
    "# Prepare few-shot dataset: first 16 examples per tag\n",
    "train_dataset = []\n",
    "guid = 0\n",
    "\n",
    "for tag in unique_tags:\n",
    "    # Filter rows with this tag\n",
    "    tag_rows = df[df['tag'] == tag].head(16)  # first 16 examples\n",
    "    for _, row in tag_rows.iterrows():\n",
    "        example = InputExample(\n",
    "            guid=str(guid),\n",
    "            text_a=row['word'],\n",
    "            label=label_map[row['tag']]\n",
    "        )\n",
    "        train_dataset.append(example)\n",
    "        guid += 1\n",
    "\n",
    "print(\"‚úÖ Few-shot dataset created with total examples:\", len(train_dataset))\n",
    "\n",
    "# Inspect first few examples\n",
    "for ex in train_dataset[:100]:\n",
    "    print(\"GUID:\", ex.guid, \"| Word:\", ex.text_a, \"| Label:\", ex.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc4872-30bc-41a0-8f78-7fbe0369c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Get user profile path\n",
    "user_profile = os.environ[\"USERPROFILE\"]\n",
    "\n",
    "# Paths to Hugging Face cached models\n",
    "cached_models = [\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--bert-base-multilingual-cased\"),\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--xlm-roberta-base\"),\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--roberta-base\")\n",
    "]\n",
    "\n",
    "# Remove cached models if they exist\n",
    "for path in cached_models:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed cache: {path}\")\n",
    "    else:\n",
    "        print(f\"No cache found at: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341e816b-18eb-4632-b386-39da3d12cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3184503b-4f34-4d2d-9858-9cf63d93846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4080 SUPER\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Imports\n",
    "# ==============================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptForClassification, PromptDataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "\n",
    "# ========================================\n",
    "# Check CUDA\n",
    "# ========================================\n",
    "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "\n",
    "# ========================================\n",
    "# Seeds for reproducibility\n",
    "# ========================================\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe91e8f-13a6-44ad-adf2-fd4d86cca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Balanced Batch Sampler\n",
    "# ==============================\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \"\"\"\n",
    "        dataset: list of InputExample\n",
    "        batch_size: total batch size (must be divisible by number of classes)\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.labels = [ex.label for ex in dataset]\n",
    "        self.classes = list(sorted(set(self.labels)))\n",
    "        self.num_classes = len(self.classes)\n",
    "        assert batch_size % self.num_classes == 0, \"Batch size must be divisible by number of classes\"\n",
    "        self.batch_size_per_class = batch_size // self.num_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        class_indices = {c: np.where(np.array(self.labels) == c)[0].tolist() for c in self.classes}\n",
    "        for c in self.classes:\n",
    "            np.random.shuffle(class_indices[c])\n",
    "\n",
    "        num_batches = min(len(class_indices[c]) // self.batch_size_per_class for c in self.classes)\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            batch = []\n",
    "            for c in self.classes:\n",
    "                start = i * self.batch_size_per_class\n",
    "                end = start + self.batch_size_per_class\n",
    "                batch.extend(class_indices[c][start:end])\n",
    "            np.random.shuffle(batch)\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(np.where(np.array(self.labels) == c)[0]) // self.batch_size_per_class for c in self.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b5c43a-215e-429d-bd7e-2790cf690c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique POS tags: ['PN' 'G' 'NN' 'P' 'U' 'VB' 'SM' 'PM' 'PP' 'CC' 'ADJ' 'CA' 'RP' 'SC' 'SE'\n",
      " 'ADV' 'EXP' 'I' 'NEG' 'TA' 'AP' 'Q' 'PD' 'WALA' 'KP' 'GR' 'REP' 'A' 'KD'\n",
      " 'AA' 'QW' 'KER' 'OR' 'AKP' 'MUL' 'INT' 'AD' 'FR' 'DATE' 'RD']\n",
      "Label map: {'PN': 0, 'G': 1, 'NN': 2, 'P': 3, 'U': 4, 'VB': 5, 'SM': 6, 'PM': 7, 'PP': 8, 'CC': 9, 'ADJ': 10, 'CA': 11, 'RP': 12, 'SC': 13, 'SE': 14, 'ADV': 15, 'EXP': 16, 'I': 17, 'NEG': 18, 'TA': 19, 'AP': 20, 'Q': 21, 'PD': 22, 'WALA': 23, 'KP': 24, 'GR': 25, 'REP': 26, 'A': 27, 'KD': 28, 'AA': 29, 'QW': 30, 'KER': 31, 'OR': 32, 'AKP': 33, 'MUL': 34, 'INT': 35, 'AD': 36, 'FR': 37, 'DATE': 38, 'RD': 39}\n",
      "‚úÖ Few-shot dataset created with total examples: 627\n",
      "GUID: 0 | Word: ‚Äô | Label: 0\n",
      "GUID: 1 | Word: ÿß€å | Label: 0\n",
      "GUID: 2 | Word: ÿ®ŸÑÿßŸÑ | Label: 0\n",
      "GUID: 3 | Word: ÿ®⁄æÿßÿ¶€å | Label: 0\n",
      "GUID: 4 | Word: ÿ≥ÿ™ŸÖÿ®ÿ± | Label: 0\n",
      "GUID: 5 | Word: ÿß⁄ë⁄æÿßÿ¶€å | Label: 0\n",
      "GUID: 6 | Word: ‚Äú | Label: 0\n",
      "GUID: 7 | Word: Ÿπ€å⁄Ø | Label: 0\n",
      "GUID: 8 | Word: ÿ±Ÿà | Label: 0\n",
      "GUID: 9 | Word: ŸÖ€åŸÜ | Label: 0\n",
      "GUID: 10 | Word: ÿπÿ¥ÿßÿ° | Label: 0\n",
      "GUID: 11 | Word: ÿ¨ÿ®ŸÑ | Label: 0\n",
      "GUID: 12 | Word: ÿ±ÿ∂€å | Label: 0\n",
      "GUID: 13 | Word: ÿßŸÑŸÑ€Å | Label: 0\n",
      "GUID: 14 | Word: Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ | Label: 0\n",
      "GUID: 15 | Word: ⁄©ÿ±⁄©Ÿπ | Label: 0\n",
      "GUID: 16 | Word: ŸÖ€åÿ±€í | Label: 1\n",
      "GUID: 17 | Word: ŸÖ€åÿ±ÿß | Label: 1\n",
      "GUID: 18 | Word: ŸÖ€åÿ±€å | Label: 1\n",
      "GUID: 19 | Word: ŸÖ€åÿ±ÿß | Label: 1\n",
      "Classes: ['PN', 'G', 'NN', 'P', 'U', 'VB', 'SM', 'PM', 'PP', 'CC', 'ADJ', 'CA', 'RP', 'SC', 'SE', 'ADV', 'EXP', 'I', 'NEG', 'TA', 'AP', 'Q', 'PD', 'WALA', 'KP', 'GR', 'REP', 'A', 'KD', 'AA', 'QW', 'KER', 'OR', 'AKP', 'MUL', 'INT', 'AD', 'FR', 'DATE', 'RD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# Load POS Train Dataset\n",
    "# ==============================\n",
    "df = pd.read_csv(r\"C:\\Users\\stdFurqan\\Desktop\\paft\\pos\\train.csv\")  # columns: 'word', 'tag'\n",
    "\n",
    "# Find unique tags\n",
    "unique_tags = df['tag'].unique()\n",
    "print(\"Unique POS tags:\", unique_tags)\n",
    "\n",
    "# Create label map\n",
    "label_map = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "print(\"Label map:\", label_map)\n",
    "\n",
    "# ==============================\n",
    "# Prepare Few-Shot Training Dataset: first 16 examples per tag\n",
    "# ==============================\n",
    "train_dataset = []\n",
    "guid = 0\n",
    "\n",
    "for tag in unique_tags:\n",
    "    tag_rows = df[df['tag'] == tag].head(16)  # first 16 examples\n",
    "    for _, row in tag_rows.iterrows():\n",
    "        example = InputExample(\n",
    "            guid=str(guid),\n",
    "            text_a=row['word'],\n",
    "            label=label_map[row['tag']]\n",
    "        )\n",
    "        train_dataset.append(example)\n",
    "        guid += 1\n",
    "\n",
    "print(\"‚úÖ Few-shot dataset created with total examples:\", len(train_dataset))\n",
    "\n",
    "# ==============================\n",
    "# Inspect first few examples\n",
    "# ==============================\n",
    "for ex in train_dataset[:20]:\n",
    "    print(\"GUID:\", ex.guid, \"| Word:\", ex.text_a, \"| Label:\", ex.label)\n",
    "\n",
    "# ==============================\n",
    "# Define Classes\n",
    "# ==============================\n",
    "classes = list(label_map.keys())\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "# ==============================\n",
    "# Load Pretrained Model and Tokenizer\n",
    "# ==============================\n",
    "\n",
    "# # # Step 1: Use load_plm with 'roberta' to get the correct WrapperClass\n",
    "# _, _, _, WrapperClass = load_plm(\"roberta\", \"roberta-base\")  # Just to get the wrapper\n",
    "\n",
    "# # # Step 2: Manually load XLM-RoBERTa model/tokenizer\n",
    "# model_name = \"xlm-roberta-base\"\n",
    "# tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "# plm = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# ==============================\n",
    "# Load Pretrained Language Model (mBERT)\n",
    "# ==============================\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63d6684-e57f-4841-8dce-398675436d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò FINAL VERBALIZER CONTENT:\n",
      "\n",
      "PN ‚Üí ['PN', '‚Äô', 'ÿß€å']\n",
      "G ‚Üí ['G', 'ŸÖ€åÿ±€í', 'ŸÖ€åÿ±ÿß']\n",
      "NN ‚Üí ['NN', 'ÿ®⁄æÿßÿ¶€å', 'ŸÖÿ≠ŸÜÿ™']\n",
      "P ‚Üí ['P', '⁄©ÿß', '⁄©€å']\n",
      "U ‚Üí ['U', 'ŸÖ€åŸÑ', 'ŸÖŸÜ']\n",
      "VB ‚Üí ['VB', 'ÿ¢€åÿß€Å€í', '€Å€å⁄∫']\n",
      "SM ‚Üí ['SM', '€î', '!']\n",
      "PM ‚Üí ['PM', 'ÿå', '\"']\n",
      "PP ‚Üí ['PP', '€ÅŸÖ', 'ÿ¢Ÿæ']\n",
      "CC ‚Üí ['CC', 'ÿßŸàÿ±', 'Ÿà']\n",
      "ADJ ‚Üí ['ADJ', 'ŸÇÿßÿ¶ŸÑ', 'ŸÖ€åÿ±€åÿ¶Ÿπ']\n",
      "CA ‚Üí ['CA', 'ÿß⁄©€åÿ≥', 'ÿØŸà']\n",
      "RP ‚Üí ['RP', 'ÿÆŸàÿØ', 'ÿßŸæŸÜ€íÿ¢Ÿæ']\n",
      "SC ‚Üí ['SC', 'ÿ¨ÿ®⁄©€Å', '⁄©€Å']\n",
      "SE ‚Üí ['SE', 'ÿ≥€í']\n",
      "ADV ‚Üí ['ADV', 'ÿ®ÿ±ÿßÿ¶€í', 'ÿ≤€åÿßÿØ€Å']\n",
      "EXP ‚Üí ['EXP', '‚Äù', '(']\n",
      "I ‚Üí ['I', 'ÿ™Ÿà', '€Å€å']\n",
      "NEG ‚Üí ['NEG', 'ŸÜ€Å€å⁄∫', 'ŸÜ€Å']\n",
      "TA ‚Üí ['TA', '€Å€å⁄∫', '€Å€í']\n",
      "AP ‚Üí ['AP', 'Ÿà€Åÿß⁄∫', 'ÿßÿ®']\n",
      "Q ‚Üí ['Q', '⁄©⁄Ü⁄æ', '€Åÿ±']\n",
      "PD ‚Üí ['PD', 'ÿßÿ≥', 'ÿßŸÜ']\n",
      "WALA ‚Üí ['WALA', 'ŸàÿßŸÑÿß', 'ŸàÿßŸÑ€í']\n",
      "KP ‚Üí ['KP', '⁄©ÿ≥', '⁄©€åÿß']\n",
      "GR ‚Üí ['GR', 'ÿßŸæŸÜ€å', 'ÿßŸæŸÜÿß']\n",
      "REP ‚Üí ['REP', 'ÿ¨ÿ≥', 'ÿ¨ŸÜ']\n",
      "A ‚Üí ['A', 'ÿ≥€å', 'ÿ≥ÿß']\n",
      "KD ‚Üí ['KD', '⁄©ÿ≥€å', '⁄©ÿ≥']\n",
      "AA ‚Üí ['AA', 'ÿ≥⁄©ÿ™€í', '⁄Ø€åÿß']\n",
      "QW ‚Üí ['QW', '⁄©€åÿß', '⁄©€åŸà⁄∫']\n",
      "KER ‚Üí ['KER', '⁄©ÿ±']\n",
      "OR ‚Üí ['OR', 'ÿØŸàŸÜŸà⁄∫', 'ÿ≥ÿßÿ™Ÿà€å⁄∫']\n",
      "AKP ‚Üí ['AKP', '⁄©€Å€å⁄∫', '⁄©€åÿ≥€í']\n",
      "MUL ‚Üí ['MUL', 'ÿ¥ÿßŸæŸÜ⁄Ø', 'ÿßŸÅÿ∑ÿßÿ±']\n",
      "INT ‚Üí ['INT', 'Ÿàÿß€Å', 'ÿßŸÑŸÇÿßÿØÿ±€å']\n",
      "AD ‚Üí ['AD', 'ÿß€åÿ≥€å']\n",
      "FR ‚Üí ['FR', 'ÿ¢ÿØ⁄æ€í', 'ÿ≥ÿß⁄ë⁄æ€í']\n",
      "DATE ‚Üí ['DATE', '‚Äô', '‚Äù']\n",
      "RD ‚Üí ['RD', 'ÿ¨ÿ™ŸÜ€í', 'js']\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Define Prompt Template\n",
    "# ==============================\n",
    "template = ManualTemplate(\n",
    "    text='ŸÑŸÅÿ∏: {\"placeholder\":\"text_a\"} ‚Üí ÿ≠ÿµ€Å ⁄©ŸÑÿßŸÖ: {\"mask\"}',\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "templates = [\n",
    "    (\"P1\", ManualTemplate(\n",
    "        text='ŸÑŸÅÿ∏: {\"placeholder\":\"text_a\"} ‚Üí ÿ≠ÿµ€Å ⁄©ŸÑÿßŸÖ: {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P2\", ManualTemplate(\n",
    "        text='ÿ¨ŸÖŸÑ€í ŸÖ€å⁄∫ €å€Å ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ⁄©ÿ≥ ŸÇÿ≥ŸÖ ⁄©ÿß €Å€í: {\"mask\"}€î',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P3\", ManualTemplate(\n",
    "        text='ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ⁄©ÿß POS tag ⁄©€åÿß €Å€íÿü {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P4\", ManualTemplate(\n",
    "        text='€å€Å ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ÿ¨ŸÖŸÑ€í ŸÖ€å⁄∫ ⁄©ÿ≥ ÿ≤ŸÖÿ±€í ⁄©ÿß €Å€íÿü {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P5\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} ‚Üí ÿ≠ÿµ€Å ⁄©ŸÑÿßŸÖ: {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P6\", ManualTemplate(\n",
    "        text='ŸÑŸÅÿ∏: {\"placeholder\":\"text_a\"} ÿßÿ≥ ⁄©ÿß ÿ≠ÿµ€Å ⁄©ŸÑÿßŸÖ {\"mask\"} €Å€í€î',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P7\", ManualTemplate(\n",
    "        text='POS tag for {\"placeholder\":\"text_a\"} is {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P8\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} ⁄©ÿ≥ ŸÇÿ≥ŸÖ ⁄©ÿß ŸÑŸÅÿ∏ €Å€í: {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P9\", ManualTemplate(\n",
    "        text='ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ‚Üí POS: {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "    (\"P10\", ManualTemplate(\n",
    "        text='ÿ¨ŸÖŸÑ€í ŸÖ€å⁄∫ ŸÑŸÅÿ∏ {\"placeholder\":\"text_a\"} ⁄©ÿ≥ POS category ÿ≥€í ÿ™ÿπŸÑŸÇ ÿ±⁄©⁄æÿ™ÿß €Å€íÿü {\"mask\"}',\n",
    "        tokenizer=tokenizer,\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Group words by POS tag\n",
    "tag_to_words = defaultdict(list)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    word = str(row[\"word\"]).strip()\n",
    "    tag = row[\"tag\"]\n",
    "    if word and word not in tag_to_words[tag]:\n",
    "        tag_to_words[tag].append(word)\n",
    "\n",
    "# Build verbalizer map\n",
    "label_words_map = {}\n",
    "\n",
    "for tag in classes:\n",
    "    examples = tag_to_words.get(tag, [])\n",
    "\n",
    "    # Take first 2 words if available\n",
    "    sample_words = examples[:2]\n",
    "\n",
    "    # Always include the tag itself\n",
    "    label_words_map[tag] = [tag] + sample_words\n",
    "\n",
    "\n",
    "verbalizer = ManualVerbalizer(\n",
    "    classes=classes,\n",
    "    label_words=label_words_map,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nüìò FINAL VERBALIZER CONTENT:\\n\")\n",
    "\n",
    "for tag, words in label_words_map.items():\n",
    "    print(f\"{tag} ‚Üí {words}\")\n",
    "\n",
    " \n",
    "\n",
    "# # ==============================\n",
    "# # Create Prompt Model\n",
    "# # ==============================\n",
    "# prompt_model = PromptForClassification(\n",
    "#     template=template,\n",
    "#     plm=plm,\n",
    "#     verbalizer=verbalizer\n",
    "# )\n",
    "# prompt_model = prompt_model.to(device)\n",
    "# # prompt_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158087c9-189a-4983-a5d9-1006e093ee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¶ Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3930.51it/s]\n",
      "tokenizing: 627it [00:00, 3460.71it/s]\n",
      "tokenizing: 627it [00:00, 4061.10it/s]\n",
      "tokenizing: 627it [00:00, 3017.90it/s]\n",
      "tokenizing: 627it [00:00, 3095.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.3387\n",
      "Prompt pattern: ['P2', 'P1', 'P5', 'P4']\n",
      "\n",
      "üü¶ Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 4376.91it/s]\n",
      "tokenizing: 627it [00:00, 2823.66it/s]\n",
      "tokenizing: 627it [00:00, 4118.42it/s]\n",
      "tokenizing: 627it [00:00, 2821.93it/s]\n",
      "tokenizing: 627it [00:00, 2662.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 13.3603\n",
      "Prompt pattern: ['P3', 'P2', 'P9', 'P2']\n",
      "\n",
      "üü¶ Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 4763.46it/s]\n",
      "tokenizing: 627it [00:00, 3446.32it/s]\n",
      "tokenizing: 627it [00:00, 3393.90it/s]\n",
      "tokenizing: 627it [00:00, 2890.65it/s]\n",
      "tokenizing: 627it [00:00, 3058.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 11.0571\n",
      "Prompt pattern: ['P7', 'P1', 'P1', 'P2']\n",
      "\n",
      "üü¶ Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 4339.09it/s]\n",
      "tokenizing: 627it [00:00, 3470.38it/s]\n",
      "tokenizing: 627it [00:00, 2778.95it/s]\n",
      "tokenizing: 627it [00:00, 3364.90it/s]\n",
      "tokenizing: 627it [00:00, 3549.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 10.4392\n",
      "Prompt pattern: ['P4', 'P9', 'P10', 'P1']\n",
      "\n",
      "üü¶ Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 4163.62it/s]\n",
      "tokenizing: 627it [00:00, 3628.27it/s]\n",
      "tokenizing: 627it [00:00, 3688.62it/s]\n",
      "tokenizing: 627it [00:00, 3030.02it/s]\n",
      "tokenizing: 627it [00:00, 3839.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 8.0312\n",
      "Prompt pattern: ['P4', 'P9', 'P7', 'P4']\n",
      "\n",
      "üü¶ Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3611.81it/s]\n",
      "tokenizing: 627it [00:00, 4117.22it/s]\n",
      "tokenizing: 627it [00:00, 3193.66it/s]\n",
      "tokenizing: 627it [00:00, 3481.87it/s]\n",
      "tokenizing: 627it [00:00, 4829.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 7.0636\n",
      "Prompt pattern: ['P10', 'P5', 'P1', 'P3']\n",
      "\n",
      "üü¶ Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3911.61it/s]\n",
      "tokenizing: 627it [00:00, 4024.55it/s]\n",
      "tokenizing: 627it [00:00, 3337.70it/s]\n",
      "tokenizing: 627it [00:00, 3050.19it/s]\n",
      "tokenizing: 627it [00:00, 3009.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 8.3692\n",
      "Prompt pattern: ['P6', 'P5', 'P3', 'P4']\n",
      "\n",
      "üü¶ Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3687.95it/s]\n",
      "tokenizing: 627it [00:00, 2734.13it/s]\n",
      "tokenizing: 627it [00:00, 3480.52it/s]\n",
      "tokenizing: 627it [00:00, 2762.74it/s]\n",
      "tokenizing: 627it [00:00, 2893.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 6.3517\n",
      "Prompt pattern: ['P2', 'P2', 'P7', 'P2']\n",
      "\n",
      "üü¶ Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3842.30it/s]\n",
      "tokenizing: 627it [00:00, 2661.29it/s]\n",
      "tokenizing: 627it [00:00, 4080.70it/s]\n",
      "tokenizing: 627it [00:00, 3235.30it/s]\n",
      "tokenizing: 627it [00:00, 3839.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 5.9036\n",
      "Prompt pattern: ['P6', 'P10', 'P5', 'P1']\n",
      "\n",
      "üü¶ Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3726.24it/s]\n",
      "tokenizing: 627it [00:00, 2782.61it/s]\n",
      "tokenizing: 627it [00:00, 3554.54it/s]\n",
      "tokenizing: 627it [00:00, 2787.69it/s]\n",
      "tokenizing: 627it [00:00, 3471.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 5.9262\n",
      "Prompt pattern: ['P9', 'P2', 'P7', 'P2']\n",
      "\n",
      "üü¶ Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 5691.89it/s]\n",
      "tokenizing: 627it [00:00, 2859.25it/s]\n",
      "tokenizing: 627it [00:00, 2921.66it/s]\n",
      "tokenizing: 627it [00:00, 2798.22it/s]\n",
      "tokenizing: 627it [00:00, 3052.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 5.7527\n",
      "Prompt pattern: ['P5', 'P10', 'P6', 'P10']\n",
      "\n",
      "üü¶ Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3624.68it/s]\n",
      "tokenizing: 627it [00:00, 3340.24it/s]\n",
      "tokenizing: 627it [00:00, 3100.28it/s]\n",
      "tokenizing: 627it [00:00, 4015.10it/s]\n",
      "tokenizing: 627it [00:00, 2675.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 4.3611\n",
      "Prompt pattern: ['P2', 'P1', 'P4', 'P5']\n",
      "\n",
      "üü¶ Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3772.54it/s]\n",
      "tokenizing: 627it [00:00, 3259.06it/s]\n",
      "tokenizing: 627it [00:00, 1654.75it/s]\n",
      "tokenizing: 627it [00:00, 4414.33it/s]\n",
      "tokenizing: 627it [00:00, 4116.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 4.1477\n",
      "Prompt pattern: ['P4', 'P2', 'P7', 'P5']\n",
      "\n",
      "üü¶ Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3869.34it/s]\n",
      "tokenizing: 627it [00:00, 4092.04it/s]\n",
      "tokenizing: 627it [00:00, 3097.32it/s]\n",
      "tokenizing: 627it [00:00, 3034.88it/s]\n",
      "tokenizing: 627it [00:00, 3098.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 3.8404\n",
      "Prompt pattern: ['P6', 'P3', 'P6', 'P6']\n",
      "\n",
      "üü¶ Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 5759.41it/s]\n",
      "tokenizing: 627it [00:00, 3095.49it/s]\n",
      "tokenizing: 627it [00:00, 3401.64it/s]\n",
      "tokenizing: 627it [00:00, 3306.68it/s]\n",
      "tokenizing: 627it [00:00, 4833.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 3.9897\n",
      "Prompt pattern: ['P5', 'P2', 'P10', 'P3']\n",
      "\n",
      "üü¶ Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3963.93it/s]\n",
      "tokenizing: 627it [00:00, 4035.10it/s]\n",
      "tokenizing: 627it [00:00, 4316.01it/s]\n",
      "tokenizing: 627it [00:00, 4673.11it/s]\n",
      "tokenizing: 627it [00:00, 4305.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 3.1192\n",
      "Prompt pattern: ['P4', 'P3', 'P8', 'P7']\n",
      "\n",
      "üü¶ Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 4599.30it/s]\n",
      "tokenizing: 627it [00:00, 3445.77it/s]\n",
      "tokenizing: 627it [00:00, 3413.07it/s]\n",
      "tokenizing: 627it [00:00, 3088.73it/s]\n",
      "tokenizing: 627it [00:00, 3762.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 3.1988\n",
      "Prompt pattern: ['P9', 'P4', 'P6', 'P1']\n",
      "\n",
      "üü¶ Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 5015.16it/s]\n",
      "tokenizing: 627it [00:00, 4553.60it/s]\n",
      "tokenizing: 627it [00:00, 3944.65it/s]\n",
      "tokenizing: 627it [00:00, 4480.90it/s]\n",
      "tokenizing: 627it [00:00, 3424.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 2.9344\n",
      "Prompt pattern: ['P1', 'P6', 'P7', 'P5']\n",
      "\n",
      "üü¶ Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 4021.16it/s]\n",
      "tokenizing: 627it [00:00, 2680.85it/s]\n",
      "tokenizing: 627it [00:00, 4300.08it/s]\n",
      "tokenizing: 627it [00:00, 4023.46it/s]\n",
      "tokenizing: 627it [00:00, 5442.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 2.3602\n",
      "Prompt pattern: ['P4', 'P10', 'P6', 'P4']\n",
      "\n",
      "üü¶ Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3888.69it/s]\n",
      "tokenizing: 627it [00:00, 4008.83it/s]\n",
      "tokenizing: 627it [00:00, 1434.41it/s]\n",
      "tokenizing: 627it [00:00, 5165.18it/s]\n",
      "tokenizing: 627it [00:00, 3613.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 3.3284\n",
      "Prompt pattern: ['P7', 'P8', 'P3', 'P5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Create Prompt Model\n",
    "# ==============================\n",
    "prompt_model = PromptForClassification(\n",
    "    template=template,\n",
    "    plm=plm,\n",
    "    verbalizer=verbalizer\n",
    ")\n",
    "# prompt_model.to(device)\n",
    "\n",
    "# ==============================\n",
    "# Training loop with BalancedBatchSampler + random template switching\n",
    "# ==============================\n",
    "T = 20   # epochs\n",
    "K = 1    # steps per prompt\n",
    "batch_size = 157\n",
    "\n",
    "prompt_model.train()\n",
    "optimizer = AdamW(prompt_model.parameters(), lr=1e-5)\n",
    "all_epoch_patterns = {}\n",
    "\n",
    "# for epoch in range(T):\n",
    "#     print(f\"\\nüü¶ Epoch {epoch+1}/{T}\")\n",
    "\n",
    "#     # Random initial template\n",
    "#     prompt_name, current_template = random.choice(templates)\n",
    "#     epoch_pattern = []\n",
    "\n",
    "#     # Create PromptDataLoader with BalancedBatchSampler\n",
    "#     sampler = BalancedBatchSampler(train_dataset, batch_size=batch_size)\n",
    "#     train_loader = PromptDataLoader(\n",
    "#         dataset=train_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#         template=current_template,\n",
    "#         tokenizer_wrapper_class=WrapperClass,\n",
    "#         max_seq_length=128,\n",
    "#         batch_size=batch_size,\n",
    "#         batch_sampler=sampler,\n",
    "#         shuffle=False  # shuffle is ignored when batch_sampler is used\n",
    "#     )\n",
    "\n",
    "#     step_counter = 0\n",
    "#     epoch_loss = 0.0\n",
    "\n",
    "#     for batch in train_loader:\n",
    "#         # Move batch to device\n",
    "#         # batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "    \n",
    "#         optimizer.zero_grad()\n",
    "#         logits = prompt_model(batch)\n",
    "#         loss = torch.nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "#         epoch_pattern.append(prompt_name)\n",
    "\n",
    "#         step_counter += 1\n",
    "\n",
    "#         # Switch template every K steps\n",
    "#         if step_counter % K == 0:\n",
    "#             prompt_name, current_template = random.choice(templates)\n",
    "\n",
    "#             # Rebuild PromptDataLoader with new template but same sampler\n",
    "#             train_loader = PromptDataLoader(\n",
    "#                 dataset=train_dataset,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 template=current_template,\n",
    "#                 tokenizer_wrapper_class=WrapperClass,\n",
    "#                 max_seq_length=128,\n",
    "#                 batch_size=batch_size,\n",
    "#                 batch_sampler=sampler,\n",
    "#                 shuffle=False\n",
    "#             )\n",
    "\n",
    "#     all_epoch_patterns[f\"epoch_{epoch+1}\"] = epoch_pattern\n",
    "#     print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "#     print(f\"Prompt pattern: {epoch_pattern}\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(T):\n",
    "    print(f\"\\nüü¶ Epoch {epoch+1}/{T}\")\n",
    "\n",
    "    # Random initial prompt\n",
    "    prompt_name, current_template = random.choice(templates)\n",
    "\n",
    "    step_counter = 0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Record prompt usage pattern\n",
    "    epoch_pattern = []\n",
    "\n",
    "    train_loader = PromptDataLoader(\n",
    "        dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        template=current_template,\n",
    "        tokenizer_wrapper_class=WrapperClass,\n",
    "        max_seq_length=128,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = prompt_model(batch)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Log which prompt was used at this step\n",
    "        epoch_pattern.append(prompt_name)\n",
    "\n",
    "        step_counter += 1\n",
    "\n",
    "        # Switch prompt every K steps\n",
    "        if step_counter % K == 0:\n",
    "            prompt_name, current_template = random.choice(templates)\n",
    "\n",
    "            train_loader = PromptDataLoader(\n",
    "                dataset=train_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                template=current_template,\n",
    "                tokenizer_wrapper_class=WrapperClass,\n",
    "                max_seq_length=128,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "    all_epoch_patterns[f\"epoch_{epoch+1}\"] = epoch_pattern\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Prompt pattern: {epoch_pattern}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8fccc0-b7dd-44cf-8dbc-ea3cc4fb89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 86676it [00:19, 4510.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä POS Tagging Classification Report:\n",
      "\n",
      "üìä POS Tagging Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PN     0.2514    0.1484    0.1866      6065\n",
      "           G     0.4085    0.9093    0.5638       474\n",
      "          NN     0.5827    0.1459    0.2334     21792\n",
      "           P     0.9362    0.8859    0.9104     10256\n",
      "           U     0.0285    0.8500    0.0552        40\n",
      "          VB     0.5777    0.1079    0.1818     10060\n",
      "          SM     0.9988    0.9281    0.9621      3464\n",
      "          PM     0.9940    0.9496    0.9713      1924\n",
      "          PP     0.6236    0.4511    0.5235      3316\n",
      "          CC     0.9585    0.9066    0.9318      1938\n",
      "         ADJ     0.1328    0.1417    0.1371      5342\n",
      "          CA     0.3013    0.6551    0.4127      1763\n",
      "          RP     0.8830    0.9881    0.9326        84\n",
      "          SC     0.7899    0.5811    0.6696      2504\n",
      "          SE     0.8659    1.0000    0.9281      1440\n",
      "         ADV     0.2285    0.6541    0.3386      1480\n",
      "         EXP     0.3000    0.8223    0.4396       197\n",
      "           I     0.7456    0.9394    0.8314      1800\n",
      "         NEG     0.8565    1.0000    0.9227      1062\n",
      "          TA     0.5965    0.7362    0.6591      3181\n",
      "          AP     0.3940    0.9239    0.5524       710\n",
      "           Q     0.5964    0.3503    0.4413      1219\n",
      "          PD     0.4240    0.9948    0.5946      1164\n",
      "        WALA     0.8057    1.0000    0.8924       253\n",
      "          KP     0.0958    0.6577    0.1672       111\n",
      "          GR     0.7470    1.0000    0.8552       437\n",
      "         REP     0.5758    1.0000    0.7308       589\n",
      "           A     0.1899    0.9175    0.3147       206\n",
      "          KD     0.3917    0.8404    0.5343       213\n",
      "          AA     0.3392    0.6204    0.4386      2571\n",
      "          QW     0.3288    1.0000    0.4949       219\n",
      "         KER     0.2193    1.0000    0.3598       211\n",
      "          OR     0.2504    0.8596    0.3879       171\n",
      "         AKP     0.2869    0.9300    0.4385       257\n",
      "         MUL     0.0012    0.2963    0.0023        27\n",
      "         INT     0.0502    0.9153    0.0952        59\n",
      "          AD     0.2090    1.0000    0.3458        51\n",
      "          FR     0.0086    1.0000    0.0170        26\n",
      "        DATE     0.0000    0.0000    0.0000         0\n",
      "          RD     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.4688     86676\n",
      "   macro avg     0.4493    0.7277    0.4864     86676\n",
      "weighted avg     0.5981    0.4688    0.4669     86676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# Prepare Evaluation Dataset\n",
    "# ==============================\n",
    "df_eval = pd.read_csv(r\"C:\\Users\\stdFurqan\\Desktop\\paft\\pos\\test.csv\")  # columns: 'word', 'tag'\n",
    "eval_dataset = [\n",
    "    InputExample(guid=str(i), text_a=row['word'], label=label_map[row['tag']])\n",
    "    for i, row in df_eval.iterrows()\n",
    "]\n",
    "\n",
    "# ==============================\n",
    "# DataLoader for Evaluation\n",
    "# ==============================\n",
    "eval_loader = PromptDataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=template,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Evaluate Model\n",
    "# ==============================\n",
    "prompt_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        # Move tensors to device\n",
    "        # batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "        logits = prompt_model(batch)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(batch['label'].cpu().tolist())\n",
    "\n",
    "# ==============================\n",
    "# Classification Report\n",
    "# ==============================\n",
    "print(\"\\nüìä POS Tagging Classification Report:\")\n",
    "# print(classification_report(all_labels, all_preds, target_names=classes, digits=4))\n",
    "# ==============================\n",
    "# Classification Report (FIXED)\n",
    "# ==============================\n",
    "\n",
    "all_label_ids = list(range(len(classes)))  # [0, 1, 2, ..., 39]\n",
    "\n",
    "print(\"\\nüìä POS Tagging Classification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        labels=all_label_ids,      # üëà IMPORTANT FIX\n",
    "        target_names=classes,\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b35b08-7cd7-41a5-8470-8d3ccc5a3162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
