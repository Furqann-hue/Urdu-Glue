{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16eda61e-811a-4253-ba36-8a31de2dd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb629eb8-8a84-4391-97bb-c52fb676dc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4080 SUPER\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n",
      "Unique POS tags: ['PN' 'G' 'NN' 'P' 'U' 'VB' 'SM' 'PM' 'PP' 'CC' 'ADJ' 'CA' 'RP' 'SC' 'SE'\n",
      " 'ADV' 'EXP' 'I' 'NEG' 'TA' 'AP' 'Q' 'PD' 'WALA' 'KP' 'GR' 'REP' 'A' 'KD'\n",
      " 'AA' 'QW' 'KER' 'OR' 'AKP' 'MUL' 'INT' 'AD' 'FR' 'DATE' 'RD']\n",
      "Label map: {'PN': 0, 'G': 1, 'NN': 2, 'P': 3, 'U': 4, 'VB': 5, 'SM': 6, 'PM': 7, 'PP': 8, 'CC': 9, 'ADJ': 10, 'CA': 11, 'RP': 12, 'SC': 13, 'SE': 14, 'ADV': 15, 'EXP': 16, 'I': 17, 'NEG': 18, 'TA': 19, 'AP': 20, 'Q': 21, 'PD': 22, 'WALA': 23, 'KP': 24, 'GR': 25, 'REP': 26, 'A': 27, 'KD': 28, 'AA': 29, 'QW': 30, 'KER': 31, 'OR': 32, 'AKP': 33, 'MUL': 34, 'INT': 35, 'AD': 36, 'FR': 37, 'DATE': 38, 'RD': 39}\n",
      "‚úÖ Few-shot dataset created with total examples: 627\n",
      "GUID: 0 | Word: ‚Äô | Label: 0\n",
      "GUID: 1 | Word: ÿß€å | Label: 0\n",
      "GUID: 2 | Word: ÿ®ŸÑÿßŸÑ | Label: 0\n",
      "GUID: 3 | Word: ÿ®⁄æÿßÿ¶€å | Label: 0\n",
      "GUID: 4 | Word: ÿ≥ÿ™ŸÖÿ®ÿ± | Label: 0\n",
      "GUID: 5 | Word: ÿß⁄ë⁄æÿßÿ¶€å | Label: 0\n",
      "GUID: 6 | Word: ‚Äú | Label: 0\n",
      "GUID: 7 | Word: Ÿπ€å⁄Ø | Label: 0\n",
      "GUID: 8 | Word: ÿ±Ÿà | Label: 0\n",
      "GUID: 9 | Word: ŸÖ€åŸÜ | Label: 0\n",
      "GUID: 10 | Word: ÿπÿ¥ÿßÿ° | Label: 0\n",
      "GUID: 11 | Word: ÿ¨ÿ®ŸÑ | Label: 0\n",
      "GUID: 12 | Word: ÿ±ÿ∂€å | Label: 0\n",
      "GUID: 13 | Word: ÿßŸÑŸÑ€Å | Label: 0\n",
      "GUID: 14 | Word: Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ | Label: 0\n",
      "GUID: 15 | Word: ⁄©ÿ±⁄©Ÿπ | Label: 0\n",
      "GUID: 16 | Word: ŸÖ€åÿ±€í | Label: 1\n",
      "GUID: 17 | Word: ŸÖ€åÿ±ÿß | Label: 1\n",
      "GUID: 18 | Word: ŸÖ€åÿ±€å | Label: 1\n",
      "GUID: 19 | Word: ŸÖ€åÿ±ÿß | Label: 1\n",
      "Classes: ['PN', 'G', 'NN', 'P', 'U', 'VB', 'SM', 'PM', 'PP', 'CC', 'ADJ', 'CA', 'RP', 'SC', 'SE', 'ADV', 'EXP', 'I', 'NEG', 'TA', 'AP', 'Q', 'PD', 'WALA', 'KP', 'GR', 'REP', 'A', 'KD', 'AA', 'QW', 'KER', 'OR', 'AKP', 'MUL', 'INT', 'AD', 'FR', 'DATE', 'RD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Imports\n",
    "# ==============================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptForClassification, PromptDataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "# ========================================\n",
    "# Check CUDA\n",
    "# ========================================\n",
    "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "\n",
    "# ========================================\n",
    "# Seeds for reproducibility\n",
    "# ========================================\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ==============================\n",
    "# Load POS Train Dataset\n",
    "# ==============================\n",
    "df = pd.read_csv(r\"C:\\Users\\stdFurqan\\Desktop\\paft\\pos\\train.csv\")  # columns: 'word', 'tag'\n",
    "\n",
    "# Find unique tags\n",
    "unique_tags = df['tag'].unique()\n",
    "print(\"Unique POS tags:\", unique_tags)\n",
    "\n",
    "# Create label map\n",
    "label_map = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "print(\"Label map:\", label_map)\n",
    "\n",
    "# ==============================\n",
    "# Prepare Few-Shot Training Dataset: first 16 examples per tag\n",
    "# ==============================\n",
    "train_dataset = []\n",
    "guid = 0\n",
    "\n",
    "for tag in unique_tags:\n",
    "    tag_rows = df[df['tag'] == tag].head(16)  # first 16 examples\n",
    "    for _, row in tag_rows.iterrows():\n",
    "        example = InputExample(\n",
    "            guid=str(guid),\n",
    "            text_a=row['word'],\n",
    "            label=label_map[row['tag']]\n",
    "        )\n",
    "        train_dataset.append(example)\n",
    "        guid += 1\n",
    "\n",
    "print(\"‚úÖ Few-shot dataset created with total examples:\", len(train_dataset))\n",
    "\n",
    "# ==============================\n",
    "# Inspect first few examples\n",
    "# ==============================\n",
    "for ex in train_dataset[:20]:\n",
    "    print(\"GUID:\", ex.guid, \"| Word:\", ex.text_a, \"| Label:\", ex.label)\n",
    "\n",
    "# ==============================\n",
    "# Define Classes\n",
    "# ==============================\n",
    "classes = list(label_map.keys())\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "# ==============================\n",
    "# Load Pretrained Model and Tokenizer\n",
    "# ==============================\n",
    "# # Step 1: Use load_plm with 'roberta' to get the correct WrapperClass\n",
    "_, _, _, WrapperClass = load_plm(\"roberta\", \"roberta-base\")  # Just to get the wrapper\n",
    "\n",
    "# # Step 2: Manually load XLM-RoBERTa model/tokenizer\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "plm = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# ==============================\n",
    "# Load Pretrained Language Model (mBERT)\n",
    "# ==============================\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea0e859-4a66-4361-98e4-353587534c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò FINAL VERBALIZER CONTENT:\n",
      "\n",
      "PN ‚Üí ['PN', '‚Äô', 'ÿß€å']\n",
      "G ‚Üí ['G', 'ŸÖ€åÿ±€í', 'ŸÖ€åÿ±ÿß']\n",
      "NN ‚Üí ['NN', 'ÿ®⁄æÿßÿ¶€å', 'ŸÖÿ≠ŸÜÿ™']\n",
      "P ‚Üí ['P', '⁄©ÿß', '⁄©€å']\n",
      "U ‚Üí ['U', 'ŸÖ€åŸÑ', 'ŸÖŸÜ']\n",
      "VB ‚Üí ['VB', 'ÿ¢€åÿß€Å€í', '€Å€å⁄∫']\n",
      "SM ‚Üí ['SM', '€î', '!']\n",
      "PM ‚Üí ['PM', 'ÿå', '\"']\n",
      "PP ‚Üí ['PP', '€ÅŸÖ', 'ÿ¢Ÿæ']\n",
      "CC ‚Üí ['CC', 'ÿßŸàÿ±', 'Ÿà']\n",
      "ADJ ‚Üí ['ADJ', 'ŸÇÿßÿ¶ŸÑ', 'ŸÖ€åÿ±€åÿ¶Ÿπ']\n",
      "CA ‚Üí ['CA', 'ÿß⁄©€åÿ≥', 'ÿØŸà']\n",
      "RP ‚Üí ['RP', 'ÿÆŸàÿØ', 'ÿßŸæŸÜ€íÿ¢Ÿæ']\n",
      "SC ‚Üí ['SC', 'ÿ¨ÿ®⁄©€Å', '⁄©€Å']\n",
      "SE ‚Üí ['SE', 'ÿ≥€í']\n",
      "ADV ‚Üí ['ADV', 'ÿ®ÿ±ÿßÿ¶€í', 'ÿ≤€åÿßÿØ€Å']\n",
      "EXP ‚Üí ['EXP', '‚Äù', '(']\n",
      "I ‚Üí ['I', 'ÿ™Ÿà', '€Å€å']\n",
      "NEG ‚Üí ['NEG', 'ŸÜ€Å€å⁄∫', 'ŸÜ€Å']\n",
      "TA ‚Üí ['TA', '€Å€å⁄∫', '€Å€í']\n",
      "AP ‚Üí ['AP', 'Ÿà€Åÿß⁄∫', 'ÿßÿ®']\n",
      "Q ‚Üí ['Q', '⁄©⁄Ü⁄æ', '€Åÿ±']\n",
      "PD ‚Üí ['PD', 'ÿßÿ≥', 'ÿßŸÜ']\n",
      "WALA ‚Üí ['WALA', 'ŸàÿßŸÑÿß', 'ŸàÿßŸÑ€í']\n",
      "KP ‚Üí ['KP', '⁄©ÿ≥', '⁄©€åÿß']\n",
      "GR ‚Üí ['GR', 'ÿßŸæŸÜ€å', 'ÿßŸæŸÜÿß']\n",
      "REP ‚Üí ['REP', 'ÿ¨ÿ≥', 'ÿ¨ŸÜ']\n",
      "A ‚Üí ['A', 'ÿ≥€å', 'ÿ≥ÿß']\n",
      "KD ‚Üí ['KD', '⁄©ÿ≥€å', '⁄©ÿ≥']\n",
      "AA ‚Üí ['AA', 'ÿ≥⁄©ÿ™€í', '⁄Ø€åÿß']\n",
      "QW ‚Üí ['QW', '⁄©€åÿß', '⁄©€åŸà⁄∫']\n",
      "KER ‚Üí ['KER', '⁄©ÿ±']\n",
      "OR ‚Üí ['OR', 'ÿØŸàŸÜŸà⁄∫', 'ÿ≥ÿßÿ™Ÿà€å⁄∫']\n",
      "AKP ‚Üí ['AKP', '⁄©€Å€å⁄∫', '⁄©€åÿ≥€í']\n",
      "MUL ‚Üí ['MUL', 'ÿ¥ÿßŸæŸÜ⁄Ø', 'ÿßŸÅÿ∑ÿßÿ±']\n",
      "INT ‚Üí ['INT', 'Ÿàÿß€Å', 'ÿßŸÑŸÇÿßÿØÿ±€å']\n",
      "AD ‚Üí ['AD', 'ÿß€åÿ≥€å']\n",
      "FR ‚Üí ['FR', 'ÿ¢ÿØ⁄æ€í', 'ÿ≥ÿß⁄ë⁄æ€í']\n",
      "DATE ‚Üí ['DATE', '‚Äô', '‚Äù']\n",
      "RD ‚Üí ['RD', 'ÿ¨ÿ™ŸÜ€í', 'js']\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Define Prompt Template\n",
    "# ==============================\n",
    "template = ManualTemplate(\n",
    "    text='ŸÑŸÅÿ∏: {\"placeholder\":\"text_a\"} ‚Üí ÿ≠ÿµ€Å ⁄©ŸÑÿßŸÖ: {\"mask\"}',\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    " \n",
    "# Group words by POS tag\n",
    "tag_to_words = defaultdict(list)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    word = str(row[\"word\"]).strip()\n",
    "    tag = row[\"tag\"]\n",
    "    if word and word not in tag_to_words[tag]:\n",
    "        tag_to_words[tag].append(word)\n",
    "\n",
    "# Build verbalizer map\n",
    "label_words_map = {}\n",
    "\n",
    "for tag in classes:\n",
    "    examples = tag_to_words.get(tag, [])\n",
    "\n",
    "    # Take first 2 words if available\n",
    "    sample_words = examples[:2]\n",
    "\n",
    "    # Always include the tag itself\n",
    "    label_words_map[tag] = [tag] + sample_words\n",
    "\n",
    "\n",
    "verbalizer = ManualVerbalizer(\n",
    "    classes=classes,\n",
    "    label_words=label_words_map,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nüìò FINAL VERBALIZER CONTENT:\\n\")\n",
    "\n",
    "for tag, words in label_words_map.items():\n",
    "    print(f\"{tag} ‚Üí {words}\")\n",
    "\n",
    " \n",
    "\n",
    "# ==============================\n",
    "# Create Prompt Model\n",
    "# ==============================\n",
    "prompt_model = PromptForClassification(\n",
    "    template=template,\n",
    "    plm=plm,\n",
    "    verbalizer=verbalizer\n",
    ")\n",
    "# prompt_model = prompt_model.to(device)\n",
    "# prompt_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8399c4e-74e7-4bba-aabe-7b28e7171837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 627it [00:00, 3676.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 200.2912\n",
      "Epoch 2 Loss: 102.3899\n",
      "Epoch 3 Loss: 75.1153\n",
      "Epoch 4 Loss: 50.4981\n",
      "Epoch 5 Loss: 47.3664\n",
      "Epoch 6 Loss: 34.5308\n",
      "Epoch 7 Loss: 32.8333\n",
      "Epoch 8 Loss: 30.6545\n",
      "Epoch 9 Loss: 33.0180\n",
      "Epoch 10 Loss: 39.0485\n",
      "Epoch 11 Loss: 25.8850\n",
      "Epoch 12 Loss: 24.2237\n",
      "Epoch 13 Loss: 24.4681\n",
      "Epoch 14 Loss: 23.0062\n",
      "Epoch 15 Loss: 29.9855\n",
      "Epoch 16 Loss: 27.4319\n",
      "Epoch 17 Loss: 24.1167\n",
      "Epoch 18 Loss: 22.4266\n",
      "Epoch 19 Loss: 21.8995\n",
      "Epoch 20 Loss: 21.4848\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# DataLoader for Training\n",
    "# ==============================\n",
    "train_loader = PromptDataLoader(\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=template,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=128,\n",
    "    batch_size=4,\n",
    "    shuffle=True  # reproducibility preserved by seed\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt_model.train()\n",
    "optimizer = AdamW(prompt_model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = prompt_model(batch)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
    "# ==============================\n",
    "# Fine-Tuning the Prompt Model\n",
    "# ==============================\n",
    "# prompt_model.train()\n",
    "# optimizer = AdamW(prompt_model.parameters(), lr=1e-5)\n",
    "\n",
    "# for epoch in range(20):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     total_loss = 0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         # Move batch to device\n",
    "#         batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "#         logits = prompt_model(batch)\n",
    "#         loss = torch.nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "#         if torch.isnan(loss):\n",
    "#             print(\"NaN detected!\")\n",
    "#             print(\"Logits:\", logits)\n",
    "#             print(\"Labels:\", batch[\"label\"])\n",
    "#             break        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30306326-c693-48de-9292-dfaf8aeaf898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74588ec5-e799-4fd3-9c1a-ef8060f10184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 86676it [00:25, 3384.89it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# Prepare Evaluation Dataset\n",
    "# ==============================\n",
    "df_eval = pd.read_csv(r\"C:\\Users\\stdFurqan\\Desktop\\paft\\pos\\test.csv\")  # columns: 'word', 'tag'\n",
    "eval_dataset = [\n",
    "    InputExample(guid=str(i), text_a=row['word'], label=label_map[row['tag']])\n",
    "    for i, row in df_eval.iterrows()\n",
    "]\n",
    "\n",
    "# ==============================\n",
    "# DataLoader for Evaluation\n",
    "# ==============================\n",
    "eval_loader = PromptDataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=template,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Evaluate Model\n",
    "# ==============================\n",
    "prompt_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        # Move tensors to device\n",
    "        # batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "        logits = prompt_model(batch)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(batch['label'].cpu().tolist())\n",
    "\n",
    "# # ==============================\n",
    "# # Classification Report\n",
    "# # ==============================\n",
    "# print(\"\\nüìä POS Tagging Classification Report:\")\n",
    "# print(classification_report(all_labels, all_preds, target_names=classes, digits=4))\n",
    "# # ==============================\n",
    "# # Classification Report (FIXED)\n",
    "# # ==============================\n",
    "\n",
    "# all_label_ids = list(range(len(classes)))  # [0, 1, 2, ..., 39]\n",
    "\n",
    "# print(\"\\nüìä POS Tagging Classification Report:\")\n",
    "# print(\n",
    "#     classification_report(\n",
    "#         all_labels,\n",
    "#         all_preds,\n",
    "#         labels=all_label_ids,      # üëà IMPORTANT FIX\n",
    "#         target_names=classes,\n",
    "#         digits=4,\n",
    "#         zero_division=0\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a497d6f-cbce-4262-8fab-d16a9c0a30e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä POS Tagging Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PN     0.5046    0.3237    0.3944      6065\n",
      "           G     0.8947    0.9684    0.9301       474\n",
      "          NN     0.8279    0.5565    0.6656     21792\n",
      "           P     0.9455    0.8859    0.9147     10256\n",
      "           U     0.0250    1.0000    0.0488        40\n",
      "          VB     0.7036    0.2374    0.3550     10060\n",
      "          SM     0.9976    0.9752    0.9863      3464\n",
      "          PM     0.9804    0.7796    0.8686      1924\n",
      "          PP     0.5414    0.5579    0.5495      3316\n",
      "          CC     0.9431    0.9066    0.9245      1938\n",
      "         ADJ     0.3681    0.3414    0.3543      5342\n",
      "          CA     0.4747    0.8463    0.6082      1763\n",
      "          RP     0.5461    0.9881    0.7034        84\n",
      "          SC     0.7118    0.8994    0.7946      2504\n",
      "          SE     0.9697    1.0000    0.9846      1440\n",
      "         ADV     0.3385    0.7216    0.4608      1480\n",
      "         EXP     0.2679    0.8173    0.4035       197\n",
      "           I     0.9530    0.7206    0.8206      1800\n",
      "         NEG     0.8213    1.0000    0.9019      1062\n",
      "          TA     0.5219    0.6891    0.5940      3181\n",
      "          AP     0.4641    1.0000    0.6339       710\n",
      "           Q     0.7581    0.5783    0.6561      1219\n",
      "          PD     0.4285    0.9338    0.5874      1164\n",
      "        WALA     0.8083    1.0000    0.8940       253\n",
      "          KP     0.1122    1.0000    0.2018       111\n",
      "          GR     0.9583    1.0000    0.9787       437\n",
      "         REP     0.8713    1.0000    0.9312       589\n",
      "           A     0.5780    0.9175    0.7092       206\n",
      "          KD     0.9372    0.8404    0.8861       213\n",
      "          AA     0.4684    0.7040    0.5625      2571\n",
      "          QW     0.5743    0.3881    0.4632       219\n",
      "         KER     0.2331    1.0000    0.3781       211\n",
      "          OR     0.2879    0.8889    0.4349       171\n",
      "         AKP     0.5750    0.9844    0.7260       257\n",
      "         MUL     0.0014    0.1852    0.0029        27\n",
      "         INT     0.1459    0.8136    0.2474        59\n",
      "          AD     0.9107    1.0000    0.9533        51\n",
      "          FR     0.0492    1.0000    0.0939        26\n",
      "        DATE     0.0000    0.0000    0.0000         0\n",
      "          RD     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.6267     86676\n",
      "   macro avg     0.5625    0.7612    0.5901     86676\n",
      "weighted avg     0.7263    0.6267    0.6456     86676\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Classification Report\n",
    "# ==============================\n",
    "print(\"\\nüìä POS Tagging Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=classes, digits=4))\n",
    "# ==============================\n",
    "# Classification Report (FIXED)\n",
    "# ==============================\n",
    "\n",
    "# all_label_ids = list(range(len(classes)))  # [0, 1, 2, ..., 39]\n",
    "\n",
    "# print(\"\\nüìä POS Tagging Classification Report:\")\n",
    "# print(\n",
    "#     classification_report(\n",
    "#         all_labels,\n",
    "#         all_preds,\n",
    "#         labels=all_label_ids,      # üëà IMPORTANT FIX\n",
    "#         target_names=classes,\n",
    "#         digits=4,\n",
    "#         zero_division=0\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6578e-c8b5-4547-8c7e-952dfb540ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f9dd3-9814-401b-a021-a5880b983e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "roberta\n",
    "üìä POS Tagging Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          PN     0.5046    0.3237    0.3944      6065\n",
    "           G     0.8947    0.9684    0.9301       474\n",
    "          NN     0.8279    0.5565    0.6656     21792\n",
    "           P     0.9455    0.8859    0.9147     10256\n",
    "           U     0.0250    1.0000    0.0488        40\n",
    "          VB     0.7036    0.2374    0.3550     10060\n",
    "          SM     0.9976    0.9752    0.9863      3464\n",
    "          PM     0.9804    0.7796    0.8686      1924\n",
    "          PP     0.5414    0.5579    0.5495      3316\n",
    "          CC     0.9431    0.9066    0.9245      1938\n",
    "         ADJ     0.3681    0.3414    0.3543      5342\n",
    "          CA     0.4747    0.8463    0.6082      1763\n",
    "          RP     0.5461    0.9881    0.7034        84\n",
    "          SC     0.7118    0.8994    0.7946      2504\n",
    "          SE     0.9697    1.0000    0.9846      1440\n",
    "         ADV     0.3385    0.7216    0.4608      1480\n",
    "         EXP     0.2679    0.8173    0.4035       197\n",
    "           I     0.9530    0.7206    0.8206      1800\n",
    "         NEG     0.8213    1.0000    0.9019      1062\n",
    "          TA     0.5219    0.6891    0.5940      3181\n",
    "          AP     0.4641    1.0000    0.6339       710\n",
    "           Q     0.7581    0.5783    0.6561      1219\n",
    "          PD     0.4285    0.9338    0.5874      1164\n",
    "        WALA     0.8083    1.0000    0.8940       253\n",
    "          KP     0.1122    1.0000    0.2018       111\n",
    "          GR     0.9583    1.0000    0.9787       437\n",
    "         REP     0.8713    1.0000    0.9312       589\n",
    "           A     0.5780    0.9175    0.7092       206\n",
    "          KD     0.9372    0.8404    0.8861       213\n",
    "          AA     0.4684    0.7040    0.5625      2571\n",
    "          QW     0.5743    0.3881    0.4632       219\n",
    "         KER     0.2331    1.0000    0.3781       211\n",
    "          OR     0.2879    0.8889    0.4349       171\n",
    "         AKP     0.5750    0.9844    0.7260       257\n",
    "         MUL     0.0014    0.1852    0.0029        27\n",
    "         INT     0.1459    0.8136    0.2474        59\n",
    "          AD     0.9107    1.0000    0.9533        51\n",
    "          FR     0.0492    1.0000    0.0939        26\n",
    "        DATE     0.0000    0.0000    0.0000         0\n",
    "          RD     0.0000    0.0000    0.0000         0\n",
    "\n",
    "    accuracy                         0.6267     86676\n",
    "   macro avg     0.5625    0.7612    0.5901     86676\n",
    "weighted avg     0.7263    0.6267    0.6456     86676"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
