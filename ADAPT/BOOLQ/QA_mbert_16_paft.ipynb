{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788df296-8902-4ff8-82dc-87cd26768c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Get user profile path\n",
    "user_profile = os.environ[\"USERPROFILE\"]\n",
    "\n",
    "# Paths to Hugging Face cached models\n",
    "cached_models = [\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--bert-base-multilingual-cased\"),\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--xlm-roberta-base\"),\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--roberta-base\")\n",
    "]\n",
    "\n",
    "# Remove cached models if they exist\n",
    "for path in cached_models:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed cache: {path}\")\n",
    "    else:\n",
    "        print(f\"No cache found at: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79986564-188a-40f3-85f9-9add90197ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d479b8-2d2c-4fce-9b41-3c43e21f5524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PromptForClassification(\n",
       "  (prompt_model): PromptModel(\n",
       "    (plm): BertForMaskedLM(\n",
       "      (bert): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cls): BertOnlyMLMHead(\n",
       "        (predictions): BertLMPredictionHead(\n",
       "          (transform): BertPredictionHeadTransform(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (transform_act_fn): GELUActivation()\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (template): ManualTemplate()\n",
       "  )\n",
       "  (verbalizer): ManualVerbalizer()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================\n",
    "# Imports and CUDA check\n",
    "# ==============================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptForClassification, PromptDataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ==============================\n",
    "# Seeds\n",
    "# ==============================\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ==============================\n",
    "# BalancedBatchSampler for QA (based on label)\n",
    "# ==============================\n",
    "# class BalancedBatchSampler(Sampler):\n",
    "#     def __init__(self, dataset, batch_size):\n",
    "#         self.dataset = dataset\n",
    "#         self.labels = [ex.label for ex in dataset]\n",
    "#         self.classes = list(sorted(set(self.labels)))\n",
    "#         self.num_classes = len(self.classes)\n",
    "#         assert batch_size % self.num_classes == 0, \"Batch size must be divisible by number of classes\"\n",
    "#         self.batch_size_per_class = batch_size // self.num_classes\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         class_indices = {c: np.where(np.array(self.labels) == c)[0].tolist() for c in self.classes}\n",
    "#         for c in self.classes:\n",
    "#             np.random.shuffle(class_indices[c])\n",
    "\n",
    "#         num_batches = min(len(class_indices[c]) // self.batch_size_per_class for c in self.classes)\n",
    "\n",
    "#         for i in range(num_batches):\n",
    "#             batch = []\n",
    "#             for c in self.classes:\n",
    "#                 start = i * self.batch_size_per_class\n",
    "#                 end = start + self.batch_size_per_class\n",
    "#                 batch.extend(class_indices[c][start:end])\n",
    "#             np.random.shuffle(batch)\n",
    "#             yield batch\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return min(len(np.where(np.array(self.labels) == c)[0]) // self.batch_size_per_class for c in self.classes)\n",
    "\n",
    "\n",
    "\n",
    "# class QABalancedBatchSampler(Sampler):\n",
    "#     def __init__(self, dataset, batch_size, options_per_question=4):\n",
    "#         \"\"\"\n",
    "#         dataset: list of InputExample\n",
    "#         batch_size: total examples per batch (must be multiple of options_per_question)\n",
    "#         options_per_question: number of options per question (usually 4)\n",
    "#         \"\"\"\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.options_per_question = options_per_question\n",
    "        \n",
    "#         assert batch_size % options_per_question == 0, \"Batch size must be divisible by options per question\"\n",
    "#         self.questions_per_batch = batch_size // options_per_question\n",
    "        \n",
    "#         # Group examples by question id\n",
    "#         self.question_dict = {}\n",
    "#         for ex in dataset:\n",
    "#             if ex.qid not in self.question_dict:\n",
    "#                 self.question_dict[ex.qid] = []\n",
    "#             self.question_dict[ex.qid].append(ex)\n",
    "        \n",
    "#         self.qids = list(self.question_dict.keys())\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         qids = self.qids.copy()\n",
    "#         np.random.shuffle(qids)\n",
    "        \n",
    "#         # Split qids into batches\n",
    "#         for i in range(0, len(qids), self.questions_per_batch):\n",
    "#             batch_qids = qids[i:i+self.questions_per_batch]\n",
    "#             batch = []\n",
    "#             for qid in batch_qids:\n",
    "#                 batch.extend(self.question_dict[qid])  # include all options\n",
    "#             yield [self.dataset.index(ex) for ex in batch]  # yield indices\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.qids) // self.questions_per_batch\n",
    "\n",
    "\n",
    "\n",
    "class QABalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, guid_to_qid, batch_size, options_per_question=4):\n",
    "        self.dataset = dataset\n",
    "        self.guid_to_qid = guid_to_qid\n",
    "        self.batch_size = batch_size\n",
    "        self.options_per_question = options_per_question\n",
    "        assert batch_size % options_per_question == 0, \"Batch size must be divisible by options per question\"\n",
    "        self.questions_per_batch = batch_size // options_per_question\n",
    "\n",
    "        # Group examples by qid\n",
    "        self.question_dict = {}\n",
    "        for ex in dataset:\n",
    "            qid = self.guid_to_qid[ex.guid]\n",
    "            if qid not in self.question_dict:\n",
    "                self.question_dict[qid] = []\n",
    "            self.question_dict[qid].append(ex)\n",
    "\n",
    "        self.qids = list(self.question_dict.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        qids = self.qids.copy()\n",
    "        np.random.shuffle(qids)\n",
    "        for i in range(0, len(qids), self.questions_per_batch):\n",
    "            batch_qids = qids[i:i+self.questions_per_batch]\n",
    "            batch = []\n",
    "            for qid in batch_qids:\n",
    "                batch.extend(self.question_dict[qid])\n",
    "            yield [self.dataset.index(ex) for ex in batch]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qids) // self.questions_per_batch\n",
    "\n",
    "\n",
    "\n",
    "# Raw data: first 16 questions\n",
    "data_16 = [\n",
    "    {\n",
    "        \"question\": \"Ø§ÛŒÚ© Ø¨ÛŒÙ¹Ø±ÛŒ Ú©Ø³ Ú†ÛŒØ² Ú©Ùˆ Ú†Ù„Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒÙ…ÛŒØ§Ø¦ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ú©Ùˆ Ø¨Ø±Ù‚ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªÛŒ ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"Ø§Ø³ÙÙ†Ø¬\", \"Ù¾ØªÚ¾Ø±\", \"Ø§Ø³Ù¹ÛŒÙ¾Ù„Ø±\", \"Ú©ÛŒÙ„Ú©ÙˆÙ„ÛŒÙ¹Ø±\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©ÛŒØ§ ØªÙ…Ø§Ù… Ø­Ø´Ø±Ø§Øª Ú©Ùˆ Ù…Ú©Ù…Ù„ Ø¨Ø§Ù„Øº ÛÙˆÙ†Û’ Ø³Û’ Ù¾ÛÙ„Û’ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Û’ ÛØ± Ù…Ø±Ø­Ù„Û’ Ø³Û’ Ú¯Ø²Ø±Ù†Ø§ Ù¾Ú‘ØªØ§ ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"ÛŒÛ ØªÙ…Ø§Ù…\", \"Ø­Ø´Ø±Ø§Øª Ø²Ù†Ø¯Û Ù¾ÛŒØ¯Ø§ ÛÙˆØªÛ’ ÛÛŒÚº\",\n",
    "                             \"Ù¾ÛŒÙˆÙ¾Ø§ Ú©Ø§ Ù…Ø±Ø­Ù„Û Ú©Ø¨Ú¾ÛŒ Ú©Ø¨Ú¾ÛŒ Ú†Ú¾ÙˆÚ‘ Ø¯ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’\",\n",
    "                             \"Ù¾ÛŒÙˆÙ¾Ø§ Ø§ÛŒÚ© Ù„Ø§Ø²Ù…ÛŒ Ù…Ø±Ø­Ù„Û ÛÛ’\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"C\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©ÙˆÙ† Ø³ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…ÙˆØ²ÙˆÚº ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"Ú©ÙˆØ¦Ù„Û\", \"Ù¾ÛŒÙ¹Ø±ÙˆÙ„ÛŒÙ…\", \"Ù‚Ø¯Ø±ØªÛŒ Ú¯ÛŒØ³\", \"Ø³ÙˆØ±Ø¬ Ú©ÛŒ Ø±ÙˆØ´Ù†ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§ÛŒÚ© Ù…Ú¯Ø±Ù…Ú†Ú¾ Ú©ÛŒØ§ Ú©Ú¾Ø§Ø¦Û’ Ú¯Ø§ØŸ\",\n",
    "        \"choices\": np.array([\"Ù…Ú©Ú‘ÛŒ\", \"Ú©ÛŒÚ©Ú‘Ø§\", \"Ø³Ú©ÙˆÛŒÚˆ\", \"ØªÙ„Ø§Ù¾ÛŒØ§\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø­Ø±Ø§Ø±Øª Ú©Ø§ Ø°Ø±ÛŒØ¹Û ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’\",\n",
    "        \"choices\": np.array([\"Ù†Ø§Ú© Ø±Ú¯Ú‘Ù†Ø§\", \"Ø§Ù†Ù¹Ø§Ø±Ú©Ù¹ÛŒÚ©Ø§ Ù…ÛŒÚº ØªÛŒØ±Ø§Ú©ÛŒ Ú©Ø±Ù†Ø§\", \"Ø¨Ø±Ù Ú©Ùˆ Ú†Ú¾ÙˆÙ†Ø§\",\n",
    "                             \"ÙØ±ÛŒØ²Ø± Ù…ÛŒÚº Ø¨ÛŒÙ¹Ú¾Ù†Ø§\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"ÙÙ†Ú†ÙˆÚº Ú©Ùˆ Ù¾Ø³Ù†Ø¯ ÛÛ’\",\n",
    "        \"choices\": np.array([\"Ù„Ø·ÛŒÙÛ’\", \"Ø¬Ø§Ø¯Ùˆ\", \"Ø§ÛŒÙˆØ±ÛŒ ÙˆÙ† Ù„ÙˆØ² Ø±ÛŒÙ…Ù†Úˆ\", \"Ù…ÛŒÙ¹Ú¾Û’ Ø³Ù„Ø·Ø§Ù†\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø±ÙˆØ´Ù†ÛŒ Ú©Ø³ Ú†ÛŒØ² Ù…ÛŒÚº Ø³Û’ Ù†ÛÛŒÚº Ú¯Ø²Ø± Ø³Ú©ØªÛŒØŸ\",\n",
    "        \"choices\": np.array([\"Ù„Ú©Ú‘ÛŒ\", \"Ù¾Ø§Ù†ÛŒ\", \"ÛÙˆØ§\", \"Ø´ÛŒØ´Û\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ù…Ù„Ú† Ú©Ø§ Ø§ÛŒÚ© ÚˆÚ¾ÛŒØ± Ú¯Ù„ Ø³Ú‘ Ú¯ÛŒØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø¬Ø¨ Ø§Ø³ Ú©Ø§ Ø³Ø¨Ø¨ ØªÙ„Ø§Ø´ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ ØªÙˆ Ø§ÛŒÚ© Ø¨Ø§ØºØ¨Ø§Ù† Ú©Ùˆ Ù†Ø¸Ø± Ø¢ØªÛ’ ÛÛŒÚº\",\n",
    "        \"choices\": np.array([\"Ø¨Ù„ Ú©Ú¾Ø§ØªÛ’ ÛÙˆØ¦Û’ Ø¬Ø§Ù†Ø¯Ø§Ø±\", \"Ù„Ú©Ú¾Ù†Û’ ÙˆØ§Ù„Û’ Ø¬Ø§Ù†Ø¯Ø§Ø±\", \"Ø¨Ú‘Û’ Ø¯Ø±Ø®Øª\", \"Ø¬Ù†Ú¯Ù„ Ú©ÛŒ Ø¢Ú¯\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú†Ø§Ù†Ø¯ Ø·Ù„ÙˆØ¹ ÛÙˆØªØ§ ÛÛ’\",\n",
    "        \"choices\": np.array([\"Ø§ÛŒÚ© Ø¯ÛØ§Ø¦ÛŒ Ù…ÛŒÚº Ø§ÛŒÚ© Ø¨Ø§Ø±\", \"Ø³Ø§Ù„ Ù…ÛŒÚº 4 Ø¨Ø§Ø±\", \"ÛÙØªÛ’ Ù…ÛŒÚº 7 Ø¨Ø§Ø±\", \"Ù…ÛÛŒÙ†Û’ Ù…ÛŒÚº 1 Ø¨Ø§Ø±\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"C\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ù¾ÛØ§Ú‘ Ú©ÛŒ ÚˆÚ¾Ù„ÙˆØ§Ù† Ù¾Ø± Ú†Ù¹Ø§Ù†ÙˆÚº Ú©Û’ Ù¹ÙˆÙ¹Ù†Û’ Ú©ÛŒ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ù…Ú©Ù†Û ÙˆØ¬Û Ú©ÛŒØ§ ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"ØµÙ†ÙˆØ¨Ø± Ú©Û’ Ø¯Ø±Ø®Øª Ú©ÛŒ Ø¬Ú‘ÛŒÚº\", \"Ø­Ù‚ÛŒÙ‚ÛŒ Ø¹Ø±ÙØ§Ù†\", \"Ø³Ù…Ù†Ø¯Ø±ÛŒ Ø®Ø§Ø±Ù¾Ø´Øª\", \"ÛÛŒÙˆÛŒ Ù…ÛŒÙ¹Ù„ Ù…ÙˆØ³ÛŒÙ‚ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©Ú†Ú¾ Ù¾Ø±Ù†Ø¯Û’ Ù…Ù‚Ø§Ù…Ø§Øª ØªÙ„Ø§Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚº\",\n",
    "        \"choices\": np.array([\"Ø²Ù…ÛŒÙ†ÛŒ Ù†Ø´Ø§Ù†ÛŒÙˆÚº Ø³Û’\", \"Ø³Ú‘Ú© Ú©Û’ Ù†Ø´Ø§Ù†Ø§Øª Ø³Û’\", \"Ø§Ù†ÚˆÙˆÚº Ø³Û’\", \"Ù…Ù‚Ù†Ø§Ø·ÛŒØ³ÛŒ Ù†Ù…ÙˆÙ†ÙˆÚº Ø³Û’\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©Ø³ Ø³Û’ Ø¨ÛŒØ¬ Ù¾Ú¾ÛŒÙ„Ù†Û’ Ú©Ø§ Ø§Ù…Ú©Ø§Ù† ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"Ø§ÛŒÚ© Ú©Ø§Ø±\", \"Ø³ÙˆØ±Ø¬ Ú©ÛŒ Ø§ÛŒÚ© Ú©Ø±Ù†\", \"Ø§ÛŒÚ© ÙˆÛÛŒÙ„\", \"Ø§ÛŒÚ© ÛÙ…Ù†Ú¯ Ø¨Ø±Úˆ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§ÛŒÚ© Ú©Ø§Ø± Ø¬Ùˆ Ø§ÛŒÚ© Ø§ÛŒØ³Û’ ÙˆØ³ÛŒÙ„Û’ Ù¾Ø± Ú†Ù„ØªÛŒ ÛÛ’ Ø¬Ùˆ Ø¨Ø§Ù„Ø¢Ø®Ø± Ø®ØªÙ… ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯Ø§\",\n",
    "        \"choices\": np.array([\"ÚˆØ§Ø¦Ù†ÙˆØ³Ø§Ø± Ú©ÛŒ Ø¨Ø§Ù‚ÛŒØ§Øª\", \"Ù¾Ø§Ù†ÛŒ Ú©ÛŒ Ø·Ø§Ù‚Øª\", \"Ø´Ù…Ø³ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ\", \"Ø¨Ø¬Ù„ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø¨Ø¬Ù„ÛŒ Ú©Û’ Ú©Ù†ÚˆÚ©Ù¹Ø±Ø² Ø§Ù† Ø·Ø±ÛŒÙ‚ÙˆÚº Ø³Û’ Ú©Ø§Ù… Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Û’\",\n",
    "        \"choices\": np.array([\"Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆÙˆÛŒÙˆ Ú©Ùˆ Ú†Ù„Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø³Ø§Ú©Ù¹ Ù…ÛŒÚº Ù¾Ù„Ú¯ Ù„Ú¯Ø§Ù†Ø§\",\n",
    "                             \"ØµØ¨Ø­ Ú©Û’ ÙˆÙ‚Øª Ú©Ø§ÙÛŒ Ù¾Ø§Ù¹ Ø¢Ù† Ú©Ø±Ù†Ø§\",\n",
    "                             \"Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ø¨Ø¹Ø¯ Ø§Ø³ Ø¨Ø§Øª Ú©Ùˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§Ù†Ø§ Ú©Û ÛÛŒØ¦Ø± ÚˆØ±Ø§Ø¦Ø± Ø§Ù† Ù¾Ù„Ú¯ ÛÛ’\",\n",
    "                             \"Ú©Ù…Ø±Û’ Ú©ÛŒ Ù„Ø§Ø¦Ù¹ÛŒÚº Ø¬Ù„Ø§Ù†Ø§\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§Ú¯Ø± Ú©Ø³ÛŒ Ù¾Ø±Ù†Ø¯Û’ Ú©ÛŒ Ú†ÙˆÙ†Ú† Ø§Ù¾Ù†Û’ Ø³Ø§ØªÚ¾ÛŒÙˆÚº Ø³Û’ Ø¨Ú‘ÛŒ ÛÛ’ØŒ ØªÙˆ ØºØ§Ù„Ø¨ Ø§Ù…Ú©Ø§Ù† ÛÛ’ Ú©Û Ø§Ø³ Ú©Û’ ØªÚ¾Û’\",\n",
    "        \"choices\": np.array([\"Ø¨ØºÛŒØ± Ú†ÙˆÙ†Ú† ÙˆØ§Ù„ÛŒ Ù…Ø§Úº\", \"Ù„Ù…Ø¨ÛŒ Ø³ÙˆÙ†Úˆ ÙˆØ§Ù„Ø§ Ø¨Ø§Ù¾\",\n",
    "                             \"Ú†Ú¾ÙˆÙ¹ÛŒ Ú†ÙˆÙ†Ú†ÙˆÚº ÙˆØ§Ù„Û’ ÙˆØ§Ù„Ø¯ÛŒÙ†\", \"Ø§ÛŒØ³ÛŒ ÛÛŒ Ú†ÙˆÙ†Ú†ÙˆÚº ÙˆØ§Ù„Û’ Ø¢Ø¨Ø§Ø¤ Ø§Ø¬Ø¯Ø§Ø¯\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§Ú¯Ø± Ú©ÙˆØ¦ÛŒ Ø´Ø®Øµ Ø±ÙˆØ²Ø§Ù†Û Ù…Ù†Ú©Û’ Ø¨ÛŒÚ† Ø±ÛØ§ ÛÛ’ Ø§ÙˆØ± Ù¾Ú¾Ø± Ú©ÙˆØ¦ÛŒ Ø¨Ú¾ÛŒ Ù…Ø²ÛŒØ¯ Ù…Ù†Ú©Û’ Ù†ÛÛŒÚº Ø®Ø±ÛŒØ¯ØªØ§ØŒ ØªÙˆ Ø¨ÛŒÚ†Ù†Û’ ÙˆØ§Ù„Û’ Ú©Ø§ Ú©ÛŒØ§ ÛÙˆÚ¯Ø§ØŸ\",\n",
    "        \"choices\": np.array([\"ÙˆÛ Ø²ÛŒØ§Ø¯Û Ù¾ÛŒØ³Û’ Ú©Ù…Ø§Ø¦Û’ Ú¯Ø§\", \"Ø§Ø³ Ú©ÛŒ Ø¢Ù…Ø¯Ù†ÛŒ Ø±Ú© Ø¬Ø§Ø¦Û’ Ú¯ÛŒ\",\n",
    "                             \"Ø§Ø³ Ú©Ø§ Ù…Ù†Ø§ÙØ¹ Ø¨Ú‘Ú¾ Ø¬Ø§Ø¦Û’ Ú¯Ø§\", \"Ø§Ø³ Ú©ÛŒ Ø¢Ù…Ø¯Ù†ÛŒ Ø²ÛŒØ§Ø¯Û ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"B\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to OpenPrompt InputExample format\n",
    "# train_dataset = []\n",
    "# guid = 0\n",
    "\n",
    "# for idx, item in enumerate(data_16):\n",
    "#     question = item[\"question\"]\n",
    "#     labels = item[\"labels\"]\n",
    "#     choices = item[\"choices\"]\n",
    "#     correct_answer = item[\"answer\"]\n",
    "\n",
    "#     for label, choice in zip(labels, choices):\n",
    "#         train_dataset.append(\n",
    "#             InputExample(\n",
    "#                 guid=f\"q{idx}_{label}\",\n",
    "#                 text_a=question,\n",
    "#                 text_b=choice,\n",
    "#                 label=1 if label == correct_answer else 0,\n",
    "#                 qid=f\"q{idx}\"   # <<< add this line\n",
    "#             )\n",
    "#         )\n",
    "#         guid += 1\n",
    "\n",
    "\n",
    "# Create InputExamples and qid mapping\n",
    "train_dataset = []\n",
    "guid = 0\n",
    "guid_to_qid = {}  # map example guid to question id\n",
    "\n",
    "for idx, item in enumerate(data_16):\n",
    "    question = item[\"question\"]\n",
    "    labels = item[\"labels\"]\n",
    "    choices = item[\"choices\"]\n",
    "    correct_answer = item[\"answer\"]\n",
    "    qid = f\"q{idx}\"\n",
    "\n",
    "    for label, choice in zip(labels, choices):\n",
    "        ex = InputExample(\n",
    "            guid=f\"{qid}_{label}\",\n",
    "            text_a=question,\n",
    "            text_b=choice,\n",
    "            label=1 if label == correct_answer else 0\n",
    "        )\n",
    "        train_dataset.append(ex)\n",
    "        guid_to_qid[ex.guid] = qid\n",
    "        guid += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Quick stats\n",
    "# print(\"Total examples:\", len(train_dataset))\n",
    "# print(\"Correct examples:\", sum(ex.label==1 for ex in train_dataset))\n",
    "# print(\"Incorrect examples:\", sum(ex.label==0 for ex in train_dataset))\n",
    "\n",
    "# train_dataset = []\n",
    "# for idx, item in enumerate(data_16):\n",
    "#     q = item[\"question\"]\n",
    "#     choices = item[\"choices\"]\n",
    "#     labels = item[\"labels\"]\n",
    "#     answer = item[\"answer\"]\n",
    "#     for l, c in zip(labels, choices):\n",
    "#         train_dataset.append(InputExample(\n",
    "#             guid=f\"q{idx}_{l}\",\n",
    "#             text_a=q,\n",
    "#             text_b=c,\n",
    "#             label=1 if l == answer else 0\n",
    "#         ))\n",
    "\n",
    "# ==============================\n",
    "# Classes\n",
    "# ==============================\n",
    "classes = [\"incorrect\", \"correct\"]\n",
    "label_map = {\"incorrect\":0, \"correct\":1}\n",
    "\n",
    "# ==============================\n",
    "# Load PLM + Tokenizer\n",
    "# ==============================\n",
    "# # # Step 1: Use load_plm with 'roberta' to get the correct WrapperClass\n",
    "# _, _, _, WrapperClass = load_plm(\"roberta\", \"roberta-base\")  # Just to get the wrapper\n",
    "\n",
    "# # # Step 2: Manually load XLM-RoBERTa model/tokenizer\n",
    "# model_name = \"xlm-roberta-base\"\n",
    "# tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "# plm = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# ==============================\n",
    "# Load Pretrained Language Model (mBERT)\n",
    "# ==============================\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-multilingual-cased\")\n",
    "\n",
    "\n",
    "\n",
    "template = ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø¬ÙˆØ§Ø¨: {\"placeholder\":\"text_b\"} ÛŒÛ Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "templates = [\n",
    "    # First 5: mask in the middle\n",
    "    (\"P1\", ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø¬ÙˆØ§Ø¨: {\"placeholder\":\"text_b\"} ÛŒÛ Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P2\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ù„ÛŒÛ’ Ø¬ÙˆØ§Ø¨ {\"placeholder\":\"text_b\"} ØµØ­ÛŒØ­ Ø·ÙˆØ± Ù¾Ø± {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P3\", ManualTemplate(\n",
    "        text='Ù…Ù†Ø¯Ø±Ø¬Û Ø°ÛŒÙ„ Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø§Ù†ØªØ®Ø§Ø¨: {\"placeholder\":\"text_b\"} Ù†ØªÛŒØ¬Û {\"mask\"} ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P4\", ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø§ÙˆØ± Ø¢Ù¾Ø´Ù†: {\"placeholder\":\"text_b\"} Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P5\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø§Ù†ØªØ®Ø§Ø¨ {\"placeholder\":\"text_b\"} Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "\n",
    "    # Last 5: mask at the end\n",
    "    (\"P6\", ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø¬ÙˆØ§Ø¨: {\"placeholder\":\"text_b\"} ÛŒÛ Ø¬ÙˆØ§Ø¨ ÛÛ’ {\"mask\"}',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P7\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ù„ÛŒÛ’ Ø¢Ù¾Ø´Ù†: {\"placeholder\":\"text_b\"} ØµØ­ÛŒØ­ Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P8\", ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø§Ù†ØªØ®Ø§Ø¨: {\"placeholder\":\"text_b\"} Ù†ØªÛŒØ¬Û {\"mask\"}',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P9\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ {\"placeholder\":\"text_b\"} Ù†ØªÛŒØ¬Û {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P10\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± Ø§Ù†ØªØ®Ø§Ø¨: {\"placeholder\":\"text_b\"} ØµØ­ÛŒØ­ Ø¬ÙˆØ§Ø¨ {\"mask\"}',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Define Verbalizer (Manual)\n",
    "# ==============================\n",
    "verbalizer = ManualVerbalizer(\n",
    "    classes=classes,\n",
    "    label_words = {\n",
    "    \"correct\": [\"Ø¯Ø±Ø³Øª\", \"ØµØ­ÛŒØ­\"], \n",
    "    \"incorrect\": [\"ØºÙ„Ø·\", \"Ù†Ø§ Ø¯Ø±Ø³Øª\"]\n",
    "    },\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Prompt Model\n",
    "# ==============================\n",
    "prompt_model = PromptForClassification(\n",
    "    template=template,  # initial template\n",
    "    plm=plm,\n",
    "    verbalizer=verbalizer\n",
    ")\n",
    "prompt_model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687ee36b-abbb-4401-a5e1-f02c500c8fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¦ Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 2233.35it/s]\n",
      "tokenizing: 64it [00:00, 1783.00it/s]\n",
      "tokenizing: 64it [00:00, 834.66it/s]\n",
      "tokenizing: 64it [00:00, 2979.01it/s]\n",
      "tokenizing: 64it [00:00, 1930.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: nan\n",
      "Prompt pattern: ['P2', 'P1', 'P5', 'P4']\n",
      "\n",
      "ğŸŸ¦ Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 1763.68it/s]\n",
      "tokenizing: 64it [00:00, 2622.29it/s]\n",
      "tokenizing: 64it [00:00, 2134.40it/s]\n",
      "tokenizing: 64it [00:00, 1913.61it/s]\n",
      "tokenizing: 64it [00:00, 2481.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: nan\n",
      "Prompt pattern: ['P3', 'P2', 'P9', 'P2']\n",
      "\n",
      "ğŸŸ¦ Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 2069.07it/s]\n",
      "tokenizing: 64it [00:00, 1125.01it/s]\n",
      "tokenizing: 64it [00:00, 1282.39it/s]\n",
      "tokenizing: 64it [00:00, 1461.74it/s]\n",
      "tokenizing: 64it [00:00, 2570.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: nan\n",
      "Prompt pattern: ['P7', 'P1', 'P1', 'P2']\n",
      "\n",
      "ğŸŸ¦ Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 1597.40it/s]\n",
      "tokenizing: 64it [00:00, 1897.49it/s]\n",
      "tokenizing: 64it [00:00, 1917.75it/s]\n",
      "tokenizing: 64it [00:00, 2082.88it/s]\n",
      "tokenizing: 64it [00:00, 2641.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: nan\n",
      "Prompt pattern: ['P4', 'P9', 'P10', 'P1']\n",
      "\n",
      "ğŸŸ¦ Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 1927.34it/s]\n",
      "tokenizing: 64it [00:00, 1157.61it/s]\n",
      "tokenizing: 64it [00:00, 2145.10it/s]\n",
      "tokenizing: 64it [00:00, 1711.81it/s]\n",
      "tokenizing: 64it [00:00, 2341.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: nan\n",
      "Prompt pattern: ['P4', 'P9', 'P7', 'P4']\n",
      "\n",
      "ğŸŸ¦ Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, ?it/s]\n",
      "tokenizing: 64it [00:00, 2466.33it/s]\n",
      "tokenizing: 64it [00:00, 776.82it/s]\n",
      "tokenizing: 64it [00:00, 2121.83it/s]\n",
      "tokenizing: 64it [00:00, 665.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: nan\n",
      "Prompt pattern: ['P10', 'P5', 'P1', 'P3']\n",
      "\n",
      "ğŸŸ¦ Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 1502.58it/s]\n",
      "tokenizing: 64it [00:00, 1502.19it/s]\n",
      "tokenizing: 64it [00:00, 2624.18it/s]\n",
      "tokenizing: 64it [00:00, 1166.97it/s]\n",
      "tokenizing: 64it [00:00, 3144.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: nan\n",
      "Prompt pattern: ['P6', 'P5', 'P3', 'P4']\n",
      "\n",
      "ğŸŸ¦ Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 989.29it/s]\n",
      "tokenizing: 64it [00:00, 2041.92it/s]\n",
      "tokenizing: 64it [00:00, 2509.92it/s]\n",
      "tokenizing: 64it [00:00, 1256.81it/s]\n",
      "tokenizing: 64it [00:00, 2529.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: nan\n",
      "Prompt pattern: ['P2', 'P2', 'P7', 'P2']\n",
      "\n",
      "ğŸŸ¦ Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 3455.97it/s]\n",
      "tokenizing: 64it [00:00, 2714.57it/s]\n",
      "tokenizing: 64it [00:00, 1578.04it/s]\n",
      "tokenizing: 64it [00:00, 1987.26it/s]\n",
      "tokenizing: 64it [00:00, 1858.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: nan\n",
      "Prompt pattern: ['P6', 'P10', 'P5', 'P1']\n",
      "\n",
      "ğŸŸ¦ Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 2486.18it/s]\n",
      "tokenizing: 64it [00:00, 1345.08it/s]\n",
      "tokenizing: 64it [00:00, 3106.21it/s]\n",
      "tokenizing: 64it [00:00, 8011.09it/s]\n",
      "tokenizing: 64it [00:00, 3418.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: nan\n",
      "Prompt pattern: ['P9', 'P2', 'P7', 'P2']\n",
      "\n",
      "ğŸŸ¦ Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 1941.91it/s]\n",
      "tokenizing: 64it [00:00, 2042.50it/s]\n",
      "tokenizing: 64it [00:00, 979.54it/s]\n",
      "tokenizing: 64it [00:00, 2200.92it/s]\n",
      "tokenizing: 64it [00:00, 864.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: nan\n",
      "Prompt pattern: ['P5', 'P10', 'P6', 'P10']\n",
      "\n",
      "ğŸŸ¦ Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 815.14it/s]\n",
      "tokenizing: 64it [00:00, 1996.19it/s]\n",
      "tokenizing: 64it [00:00, 1328.46it/s]\n",
      "tokenizing: 64it [00:00, 1625.90it/s]\n",
      "tokenizing: 64it [00:00, 798.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: nan\n",
      "Prompt pattern: ['P2', 'P1', 'P4', 'P5']\n",
      "\n",
      "ğŸŸ¦ Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 655.14it/s]\n",
      "tokenizing: 64it [00:00, 1997.21it/s]\n",
      "tokenizing: 64it [00:00, 1715.47it/s]\n",
      "tokenizing: 64it [00:00, 2043.59it/s]\n",
      "tokenizing: 64it [00:00, 1898.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: nan\n",
      "Prompt pattern: ['P4', 'P2', 'P7', 'P5']\n",
      "\n",
      "ğŸŸ¦ Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 1214.35it/s]\n",
      "tokenizing: 64it [00:00, 2057.47it/s]\n",
      "tokenizing: 64it [00:00, 785.33it/s]\n",
      "tokenizing: 64it [00:00, 1446.01it/s]\n",
      "tokenizing: 64it [00:00, 2414.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: nan\n",
      "Prompt pattern: ['P6', 'P3', 'P6', 'P6']\n",
      "\n",
      "ğŸŸ¦ Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 4708.98it/s]\n",
      "tokenizing: 64it [00:00, 2553.59it/s]\n",
      "tokenizing: 64it [00:00, 2204.39it/s]\n",
      "tokenizing: 64it [00:00, 2130.19it/s]\n",
      "tokenizing: 64it [00:00, 1816.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: nan\n",
      "Prompt pattern: ['P5', 'P2', 'P10', 'P3']\n",
      "\n",
      "ğŸŸ¦ Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 804.18it/s]\n",
      "tokenizing: 64it [00:00, 2022.84it/s]\n",
      "tokenizing: 64it [00:00, 2095.45it/s]\n",
      "tokenizing: 64it [00:00, 2063.52it/s]\n",
      "tokenizing: 64it [00:00, 1710.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: nan\n",
      "Prompt pattern: ['P4', 'P3', 'P8', 'P7']\n",
      "\n",
      "ğŸŸ¦ Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 1395.73it/s]\n",
      "tokenizing: 64it [00:00, 735.69it/s]\n",
      "tokenizing: 64it [00:00, 5324.30it/s]\n",
      "tokenizing: 64it [00:00, 1976.29it/s]\n",
      "tokenizing: 64it [00:00, 714.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: nan\n",
      "Prompt pattern: ['P9', 'P4', 'P6', 'P1']\n",
      "\n",
      "ğŸŸ¦ Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 2001.58it/s]\n",
      "tokenizing: 64it [00:00, 2011.05it/s]\n",
      "tokenizing: 64it [00:00, 803.81it/s]\n",
      "tokenizing: 64it [00:00, 2378.25it/s]\n",
      "tokenizing: 64it [00:00, 2576.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: nan\n",
      "Prompt pattern: ['P1', 'P6', 'P7', 'P5']\n",
      "\n",
      "ğŸŸ¦ Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 653.67it/s]\n",
      "tokenizing: 64it [00:00, 2462.58it/s]\n",
      "tokenizing: 64it [00:00, 882.36it/s]\n",
      "tokenizing: 64it [00:00, 2322.31it/s]\n",
      "tokenizing: 64it [00:00, 2376.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: nan\n",
      "Prompt pattern: ['P4', 'P10', 'P6', 'P4']\n",
      "\n",
      "ğŸŸ¦ Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 64it [00:00, 1598.35it/s]\n",
      "tokenizing: 64it [00:00, 2905.71it/s]\n",
      "tokenizing: 64it [00:00, 1757.13it/s]\n",
      "tokenizing: 64it [00:00, 1521.22it/s]\n",
      "tokenizing: 64it [00:00, 1357.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: nan\n",
      "Prompt pattern: ['P7', 'P8', 'P3', 'P5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Training loop with multi-template switching + QABalancedBatchSampler\n",
    "# ==============================\n",
    "T = 20       # epochs\n",
    "K = 1        # steps per template\n",
    "batch_size = 16  # total examples per batch (must be multiple of 4)\n",
    "optimizer = AdamW(prompt_model.parameters(), lr=1e-5)\n",
    "\n",
    "prompt_model.train()\n",
    "all_epoch_patterns = {}\n",
    "\n",
    "for epoch in range(T):\n",
    "    print(f\"\\nğŸŸ¦ Epoch {epoch+1}/{T}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Random initial template\n",
    "    prompt_name, current_template = random.choice(templates)\n",
    "    epoch_pattern = []\n",
    "\n",
    "    # QABalancedBatchSampler ensures full questions per batch\n",
    "    # sampler = QABalancedBatchSampler(train_dataset, batch_size=batch_size, options_per_question=4)\n",
    "    sampler = QABalancedBatchSampler(train_dataset, guid_to_qid=guid_to_qid, batch_size=batch_size, options_per_question=4)\n",
    "    train_loader = PromptDataLoader(\n",
    "        dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        template=current_template,\n",
    "        tokenizer_wrapper_class=WrapperClass,\n",
    "        max_seq_length=128,\n",
    "        batch_size=batch_size,\n",
    "        batch_sampler=sampler,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # # ğŸ”¹ Debug: check what the loader yields\n",
    "    # for batch_idx, batch in enumerate(train_loader):\n",
    "    #     print(batch_idx, batch.keys(), len(batch['input_ids']))\n",
    "    #     break  # just check the first batch\n",
    "\n",
    "    \n",
    "    step_counter = 0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Only move tensor entries to device\n",
    "        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = prompt_model(batch)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_pattern.append(prompt_name)\n",
    "    \n",
    "        step_counter += 1\n",
    "    \n",
    "        # Switch template every K steps\n",
    "        if step_counter % K == 0:\n",
    "            prompt_name, current_template = random.choice(templates)\n",
    "    \n",
    "            train_loader = PromptDataLoader(\n",
    "                dataset=train_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                template=current_template,\n",
    "                tokenizer_wrapper_class=WrapperClass,\n",
    "                max_seq_length=128,\n",
    "                batch_size=batch_size,\n",
    "                batch_sampler=sampler,\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "\n",
    "    all_epoch_patterns[f\"epoch_{epoch+1}\"] = epoch_pattern\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Prompt pattern: {epoch_pattern}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3f6e48-7d99-43db-b0e2-4698d3c37509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 3968it [00:02, 1565.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š QA Accuracy: 0.2692\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Load evaluation dataset\n",
    "# ==============================\n",
    "csv_path = r\"C:\\Users\\stdFurqan\\Desktop\\paft\\QA\\openbookqa_urdu_test_cleaned.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "eval_dataset = []\n",
    "eval_qids = []   # keeps question ids aligned with examples\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    q = row['urdu_question_stem']\n",
    "    choices_dict = eval(row['clean_choices'])\n",
    "    labels = choices_dict['label']\n",
    "    texts = choices_dict['text']\n",
    "    answer = row['answerKey']\n",
    "\n",
    "    for l, t in zip(labels, texts):\n",
    "        eval_dataset.append(\n",
    "            InputExample(\n",
    "                guid=f\"eval_{i}_{l}\",\n",
    "                text_a=q,\n",
    "                text_b=t,\n",
    "                label=1 if l == answer else 0\n",
    "            )\n",
    "        )\n",
    "        eval_qids.append(i)   # question id\n",
    "\n",
    "\n",
    "eval_loader = PromptDataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=template,   # single template OR loop over templates\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "prompt_model.eval()\n",
    "\n",
    "question_scores = {}   # qid -> list of (score, label)\n",
    "\n",
    "with torch.no_grad():\n",
    "    example_idx = 0\n",
    "\n",
    "    for batch in eval_loader:\n",
    "        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "        logits = prompt_model(batch)          # [B, 2]\n",
    "        pos_scores = logits[:, 1]              # score for \"correct\" class\n",
    "\n",
    "        for score, label in zip(pos_scores.cpu(), batch['label'].cpu()):\n",
    "            qid = eval_qids[example_idx]\n",
    "\n",
    "            if qid not in question_scores:\n",
    "                question_scores[qid] = []\n",
    "\n",
    "            question_scores[qid].append((score.item(), label.item()))\n",
    "            example_idx += 1\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for qid, options in question_scores.items():\n",
    "    # pick option with highest score\n",
    "    pred_score, pred_label = max(options, key=lambda x: x[0])\n",
    "\n",
    "    if pred_label == 1:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "qa_accuracy = correct / total\n",
    "print(f\"\\nğŸ“Š QA Accuracy: {qa_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d90dd0e-caf3-42db-9856-0bd70cb4ccaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
