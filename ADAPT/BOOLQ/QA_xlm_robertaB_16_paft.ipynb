{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788df296-8902-4ff8-82dc-87cd26768c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Get user profile path\n",
    "user_profile = os.environ[\"USERPROFILE\"]\n",
    "\n",
    "# Paths to Hugging Face cached models\n",
    "cached_models = [\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--bert-base-multilingual-cased\"),\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--xlm-roberta-base\"),\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--roberta-base\")\n",
    "]\n",
    "\n",
    "# Remove cached models if they exist\n",
    "for path in cached_models:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed cache: {path}\")\n",
    "    else:\n",
    "        print(f\"No cache found at: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79986564-188a-40f3-85f9-9add90197ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d479b8-2d2c-4fce-9b41-3c43e21f5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Imports and CUDA check\n",
    "# ==============================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptForClassification, PromptDataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ==============================\n",
    "# Seeds\n",
    "# ==============================\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ==============================\n",
    "# BalancedBatchSampler for QA (based on label)\n",
    "# ==============================\n",
    "# class BalancedBatchSampler(Sampler):\n",
    "#     def __init__(self, dataset, batch_size):\n",
    "#         self.dataset = dataset\n",
    "#         self.labels = [ex.label for ex in dataset]\n",
    "#         self.classes = list(sorted(set(self.labels)))\n",
    "#         self.num_classes = len(self.classes)\n",
    "#         assert batch_size % self.num_classes == 0, \"Batch size must be divisible by number of classes\"\n",
    "#         self.batch_size_per_class = batch_size // self.num_classes\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         class_indices = {c: np.where(np.array(self.labels) == c)[0].tolist() for c in self.classes}\n",
    "#         for c in self.classes:\n",
    "#             np.random.shuffle(class_indices[c])\n",
    "\n",
    "#         num_batches = min(len(class_indices[c]) // self.batch_size_per_class for c in self.classes)\n",
    "\n",
    "#         for i in range(num_batches):\n",
    "#             batch = []\n",
    "#             for c in self.classes:\n",
    "#                 start = i * self.batch_size_per_class\n",
    "#                 end = start + self.batch_size_per_class\n",
    "#                 batch.extend(class_indices[c][start:end])\n",
    "#             np.random.shuffle(batch)\n",
    "#             yield batch\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return min(len(np.where(np.array(self.labels) == c)[0]) // self.batch_size_per_class for c in self.classes)\n",
    "\n",
    "\n",
    "\n",
    "# class QABalancedBatchSampler(Sampler):\n",
    "#     def __init__(self, dataset, batch_size, options_per_question=4):\n",
    "#         \"\"\"\n",
    "#         dataset: list of InputExample\n",
    "#         batch_size: total examples per batch (must be multiple of options_per_question)\n",
    "#         options_per_question: number of options per question (usually 4)\n",
    "#         \"\"\"\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.options_per_question = options_per_question\n",
    "        \n",
    "#         assert batch_size % options_per_question == 0, \"Batch size must be divisible by options per question\"\n",
    "#         self.questions_per_batch = batch_size // options_per_question\n",
    "        \n",
    "#         # Group examples by question id\n",
    "#         self.question_dict = {}\n",
    "#         for ex in dataset:\n",
    "#             if ex.qid not in self.question_dict:\n",
    "#                 self.question_dict[ex.qid] = []\n",
    "#             self.question_dict[ex.qid].append(ex)\n",
    "        \n",
    "#         self.qids = list(self.question_dict.keys())\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         qids = self.qids.copy()\n",
    "#         np.random.shuffle(qids)\n",
    "        \n",
    "#         # Split qids into batches\n",
    "#         for i in range(0, len(qids), self.questions_per_batch):\n",
    "#             batch_qids = qids[i:i+self.questions_per_batch]\n",
    "#             batch = []\n",
    "#             for qid in batch_qids:\n",
    "#                 batch.extend(self.question_dict[qid])  # include all options\n",
    "#             yield [self.dataset.index(ex) for ex in batch]  # yield indices\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.qids) // self.questions_per_batch\n",
    "\n",
    "\n",
    "\n",
    "class QABalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, guid_to_qid, batch_size, options_per_question=4):\n",
    "        self.dataset = dataset\n",
    "        self.guid_to_qid = guid_to_qid\n",
    "        self.batch_size = batch_size\n",
    "        self.options_per_question = options_per_question\n",
    "        assert batch_size % options_per_question == 0, \"Batch size must be divisible by options per question\"\n",
    "        self.questions_per_batch = batch_size // options_per_question\n",
    "\n",
    "        # Group examples by qid\n",
    "        self.question_dict = {}\n",
    "        for ex in dataset:\n",
    "            qid = self.guid_to_qid[ex.guid]\n",
    "            if qid not in self.question_dict:\n",
    "                self.question_dict[qid] = []\n",
    "            self.question_dict[qid].append(ex)\n",
    "\n",
    "        self.qids = list(self.question_dict.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        qids = self.qids.copy()\n",
    "        np.random.shuffle(qids)\n",
    "        for i in range(0, len(qids), self.questions_per_batch):\n",
    "            batch_qids = qids[i:i+self.questions_per_batch]\n",
    "            batch = []\n",
    "            for qid in batch_qids:\n",
    "                batch.extend(self.question_dict[qid])\n",
    "            yield [self.dataset.index(ex) for ex in batch]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qids) // self.questions_per_batch\n",
    "\n",
    "\n",
    "\n",
    "# Raw data: first 16 questions\n",
    "data_16 = [\n",
    "    {\n",
    "        \"question\": \"Ø§ÛŒÚ© Ø¨ÛŒÙ¹Ø±ÛŒ Ú©Ø³ Ú†ÛŒØ² Ú©Ùˆ Ú†Ù„Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒÙ…ÛŒØ§Ø¦ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ú©Ùˆ Ø¨Ø±Ù‚ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªÛŒ ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"Ø§Ø³ÙÙ†Ø¬\", \"Ù¾ØªÚ¾Ø±\", \"Ø§Ø³Ù¹ÛŒÙ¾Ù„Ø±\", \"Ú©ÛŒÙ„Ú©ÙˆÙ„ÛŒÙ¹Ø±\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©ÛŒØ§ ØªÙ…Ø§Ù… Ø­Ø´Ø±Ø§Øª Ú©Ùˆ Ù…Ú©Ù…Ù„ Ø¨Ø§Ù„Øº ÛÙˆÙ†Û’ Ø³Û’ Ù¾ÛÙ„Û’ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Û’ ÛØ± Ù…Ø±Ø­Ù„Û’ Ø³Û’ Ú¯Ø²Ø±Ù†Ø§ Ù¾Ú‘ØªØ§ ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"ÛŒÛ ØªÙ…Ø§Ù…\", \"Ø­Ø´Ø±Ø§Øª Ø²Ù†Ø¯Û Ù¾ÛŒØ¯Ø§ ÛÙˆØªÛ’ ÛÛŒÚº\",\n",
    "                             \"Ù¾ÛŒÙˆÙ¾Ø§ Ú©Ø§ Ù…Ø±Ø­Ù„Û Ú©Ø¨Ú¾ÛŒ Ú©Ø¨Ú¾ÛŒ Ú†Ú¾ÙˆÚ‘ Ø¯ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’\",\n",
    "                             \"Ù¾ÛŒÙˆÙ¾Ø§ Ø§ÛŒÚ© Ù„Ø§Ø²Ù…ÛŒ Ù…Ø±Ø­Ù„Û ÛÛ’\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"C\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©ÙˆÙ† Ø³ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…ÙˆØ²ÙˆÚº ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"Ú©ÙˆØ¦Ù„Û\", \"Ù¾ÛŒÙ¹Ø±ÙˆÙ„ÛŒÙ…\", \"Ù‚Ø¯Ø±ØªÛŒ Ú¯ÛŒØ³\", \"Ø³ÙˆØ±Ø¬ Ú©ÛŒ Ø±ÙˆØ´Ù†ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§ÛŒÚ© Ù…Ú¯Ø±Ù…Ú†Ú¾ Ú©ÛŒØ§ Ú©Ú¾Ø§Ø¦Û’ Ú¯Ø§ØŸ\",\n",
    "        \"choices\": np.array([\"Ù…Ú©Ú‘ÛŒ\", \"Ú©ÛŒÚ©Ú‘Ø§\", \"Ø³Ú©ÙˆÛŒÚˆ\", \"ØªÙ„Ø§Ù¾ÛŒØ§\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø­Ø±Ø§Ø±Øª Ú©Ø§ Ø°Ø±ÛŒØ¹Û ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’\",\n",
    "        \"choices\": np.array([\"Ù†Ø§Ú© Ø±Ú¯Ú‘Ù†Ø§\", \"Ø§Ù†Ù¹Ø§Ø±Ú©Ù¹ÛŒÚ©Ø§ Ù…ÛŒÚº ØªÛŒØ±Ø§Ú©ÛŒ Ú©Ø±Ù†Ø§\", \"Ø¨Ø±Ù Ú©Ùˆ Ú†Ú¾ÙˆÙ†Ø§\",\n",
    "                             \"ÙØ±ÛŒØ²Ø± Ù…ÛŒÚº Ø¨ÛŒÙ¹Ú¾Ù†Ø§\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"ÙÙ†Ú†ÙˆÚº Ú©Ùˆ Ù¾Ø³Ù†Ø¯ ÛÛ’\",\n",
    "        \"choices\": np.array([\"Ù„Ø·ÛŒÙÛ’\", \"Ø¬Ø§Ø¯Ùˆ\", \"Ø§ÛŒÙˆØ±ÛŒ ÙˆÙ† Ù„ÙˆØ² Ø±ÛŒÙ…Ù†Úˆ\", \"Ù…ÛŒÙ¹Ú¾Û’ Ø³Ù„Ø·Ø§Ù†\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø±ÙˆØ´Ù†ÛŒ Ú©Ø³ Ú†ÛŒØ² Ù…ÛŒÚº Ø³Û’ Ù†ÛÛŒÚº Ú¯Ø²Ø± Ø³Ú©ØªÛŒØŸ\",\n",
    "        \"choices\": np.array([\"Ù„Ú©Ú‘ÛŒ\", \"Ù¾Ø§Ù†ÛŒ\", \"ÛÙˆØ§\", \"Ø´ÛŒØ´Û\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ù…Ù„Ú† Ú©Ø§ Ø§ÛŒÚ© ÚˆÚ¾ÛŒØ± Ú¯Ù„ Ø³Ú‘ Ú¯ÛŒØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø¬Ø¨ Ø§Ø³ Ú©Ø§ Ø³Ø¨Ø¨ ØªÙ„Ø§Ø´ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ ØªÙˆ Ø§ÛŒÚ© Ø¨Ø§ØºØ¨Ø§Ù† Ú©Ùˆ Ù†Ø¸Ø± Ø¢ØªÛ’ ÛÛŒÚº\",\n",
    "        \"choices\": np.array([\"Ø¨Ù„ Ú©Ú¾Ø§ØªÛ’ ÛÙˆØ¦Û’ Ø¬Ø§Ù†Ø¯Ø§Ø±\", \"Ù„Ú©Ú¾Ù†Û’ ÙˆØ§Ù„Û’ Ø¬Ø§Ù†Ø¯Ø§Ø±\", \"Ø¨Ú‘Û’ Ø¯Ø±Ø®Øª\", \"Ø¬Ù†Ú¯Ù„ Ú©ÛŒ Ø¢Ú¯\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú†Ø§Ù†Ø¯ Ø·Ù„ÙˆØ¹ ÛÙˆØªØ§ ÛÛ’\",\n",
    "        \"choices\": np.array([\"Ø§ÛŒÚ© Ø¯ÛØ§Ø¦ÛŒ Ù…ÛŒÚº Ø§ÛŒÚ© Ø¨Ø§Ø±\", \"Ø³Ø§Ù„ Ù…ÛŒÚº 4 Ø¨Ø§Ø±\", \"ÛÙØªÛ’ Ù…ÛŒÚº 7 Ø¨Ø§Ø±\", \"Ù…ÛÛŒÙ†Û’ Ù…ÛŒÚº 1 Ø¨Ø§Ø±\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"C\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ù¾ÛØ§Ú‘ Ú©ÛŒ ÚˆÚ¾Ù„ÙˆØ§Ù† Ù¾Ø± Ú†Ù¹Ø§Ù†ÙˆÚº Ú©Û’ Ù¹ÙˆÙ¹Ù†Û’ Ú©ÛŒ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ù…Ú©Ù†Û ÙˆØ¬Û Ú©ÛŒØ§ ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"ØµÙ†ÙˆØ¨Ø± Ú©Û’ Ø¯Ø±Ø®Øª Ú©ÛŒ Ø¬Ú‘ÛŒÚº\", \"Ø­Ù‚ÛŒÙ‚ÛŒ Ø¹Ø±ÙØ§Ù†\", \"Ø³Ù…Ù†Ø¯Ø±ÛŒ Ø®Ø§Ø±Ù¾Ø´Øª\", \"ÛÛŒÙˆÛŒ Ù…ÛŒÙ¹Ù„ Ù…ÙˆØ³ÛŒÙ‚ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©Ú†Ú¾ Ù¾Ø±Ù†Ø¯Û’ Ù…Ù‚Ø§Ù…Ø§Øª ØªÙ„Ø§Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚº\",\n",
    "        \"choices\": np.array([\"Ø²Ù…ÛŒÙ†ÛŒ Ù†Ø´Ø§Ù†ÛŒÙˆÚº Ø³Û’\", \"Ø³Ú‘Ú© Ú©Û’ Ù†Ø´Ø§Ù†Ø§Øª Ø³Û’\", \"Ø§Ù†ÚˆÙˆÚº Ø³Û’\", \"Ù…Ù‚Ù†Ø§Ø·ÛŒØ³ÛŒ Ù†Ù…ÙˆÙ†ÙˆÚº Ø³Û’\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©Ø³ Ø³Û’ Ø¨ÛŒØ¬ Ù¾Ú¾ÛŒÙ„Ù†Û’ Ú©Ø§ Ø§Ù…Ú©Ø§Ù† ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"Ø§ÛŒÚ© Ú©Ø§Ø±\", \"Ø³ÙˆØ±Ø¬ Ú©ÛŒ Ø§ÛŒÚ© Ú©Ø±Ù†\", \"Ø§ÛŒÚ© ÙˆÛÛŒÙ„\", \"Ø§ÛŒÚ© ÛÙ…Ù†Ú¯ Ø¨Ø±Úˆ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§ÛŒÚ© Ú©Ø§Ø± Ø¬Ùˆ Ø§ÛŒÚ© Ø§ÛŒØ³Û’ ÙˆØ³ÛŒÙ„Û’ Ù¾Ø± Ú†Ù„ØªÛŒ ÛÛ’ Ø¬Ùˆ Ø¨Ø§Ù„Ø¢Ø®Ø± Ø®ØªÙ… ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯Ø§\",\n",
    "        \"choices\": np.array([\"ÚˆØ§Ø¦Ù†ÙˆØ³Ø§Ø± Ú©ÛŒ Ø¨Ø§Ù‚ÛŒØ§Øª\", \"Ù¾Ø§Ù†ÛŒ Ú©ÛŒ Ø·Ø§Ù‚Øª\", \"Ø´Ù…Ø³ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ\", \"Ø¨Ø¬Ù„ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø¨Ø¬Ù„ÛŒ Ú©Û’ Ú©Ù†ÚˆÚ©Ù¹Ø±Ø² Ø§Ù† Ø·Ø±ÛŒÙ‚ÙˆÚº Ø³Û’ Ú©Ø§Ù… Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Û’\",\n",
    "        \"choices\": np.array([\"Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆÙˆÛŒÙˆ Ú©Ùˆ Ú†Ù„Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø³Ø§Ú©Ù¹ Ù…ÛŒÚº Ù¾Ù„Ú¯ Ù„Ú¯Ø§Ù†Ø§\",\n",
    "                             \"ØµØ¨Ø­ Ú©Û’ ÙˆÙ‚Øª Ú©Ø§ÙÛŒ Ù¾Ø§Ù¹ Ø¢Ù† Ú©Ø±Ù†Ø§\",\n",
    "                             \"Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ø¨Ø¹Ø¯ Ø§Ø³ Ø¨Ø§Øª Ú©Ùˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§Ù†Ø§ Ú©Û ÛÛŒØ¦Ø± ÚˆØ±Ø§Ø¦Ø± Ø§Ù† Ù¾Ù„Ú¯ ÛÛ’\",\n",
    "                             \"Ú©Ù…Ø±Û’ Ú©ÛŒ Ù„Ø§Ø¦Ù¹ÛŒÚº Ø¬Ù„Ø§Ù†Ø§\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§Ú¯Ø± Ú©Ø³ÛŒ Ù¾Ø±Ù†Ø¯Û’ Ú©ÛŒ Ú†ÙˆÙ†Ú† Ø§Ù¾Ù†Û’ Ø³Ø§ØªÚ¾ÛŒÙˆÚº Ø³Û’ Ø¨Ú‘ÛŒ ÛÛ’ØŒ ØªÙˆ ØºØ§Ù„Ø¨ Ø§Ù…Ú©Ø§Ù† ÛÛ’ Ú©Û Ø§Ø³ Ú©Û’ ØªÚ¾Û’\",\n",
    "        \"choices\": np.array([\"Ø¨ØºÛŒØ± Ú†ÙˆÙ†Ú† ÙˆØ§Ù„ÛŒ Ù…Ø§Úº\", \"Ù„Ù…Ø¨ÛŒ Ø³ÙˆÙ†Úˆ ÙˆØ§Ù„Ø§ Ø¨Ø§Ù¾\",\n",
    "                             \"Ú†Ú¾ÙˆÙ¹ÛŒ Ú†ÙˆÙ†Ú†ÙˆÚº ÙˆØ§Ù„Û’ ÙˆØ§Ù„Ø¯ÛŒÙ†\", \"Ø§ÛŒØ³ÛŒ ÛÛŒ Ú†ÙˆÙ†Ú†ÙˆÚº ÙˆØ§Ù„Û’ Ø¢Ø¨Ø§Ø¤ Ø§Ø¬Ø¯Ø§Ø¯\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§Ú¯Ø± Ú©ÙˆØ¦ÛŒ Ø´Ø®Øµ Ø±ÙˆØ²Ø§Ù†Û Ù…Ù†Ú©Û’ Ø¨ÛŒÚ† Ø±ÛØ§ ÛÛ’ Ø§ÙˆØ± Ù¾Ú¾Ø± Ú©ÙˆØ¦ÛŒ Ø¨Ú¾ÛŒ Ù…Ø²ÛŒØ¯ Ù…Ù†Ú©Û’ Ù†ÛÛŒÚº Ø®Ø±ÛŒØ¯ØªØ§ØŒ ØªÙˆ Ø¨ÛŒÚ†Ù†Û’ ÙˆØ§Ù„Û’ Ú©Ø§ Ú©ÛŒØ§ ÛÙˆÚ¯Ø§ØŸ\",\n",
    "        \"choices\": np.array([\"ÙˆÛ Ø²ÛŒØ§Ø¯Û Ù¾ÛŒØ³Û’ Ú©Ù…Ø§Ø¦Û’ Ú¯Ø§\", \"Ø§Ø³ Ú©ÛŒ Ø¢Ù…Ø¯Ù†ÛŒ Ø±Ú© Ø¬Ø§Ø¦Û’ Ú¯ÛŒ\",\n",
    "                             \"Ø§Ø³ Ú©Ø§ Ù…Ù†Ø§ÙØ¹ Ø¨Ú‘Ú¾ Ø¬Ø§Ø¦Û’ Ú¯Ø§\", \"Ø§Ø³ Ú©ÛŒ Ø¢Ù…Ø¯Ù†ÛŒ Ø²ÛŒØ§Ø¯Û ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"B\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to OpenPrompt InputExample format\n",
    "# train_dataset = []\n",
    "# guid = 0\n",
    "\n",
    "# for idx, item in enumerate(data_16):\n",
    "#     question = item[\"question\"]\n",
    "#     labels = item[\"labels\"]\n",
    "#     choices = item[\"choices\"]\n",
    "#     correct_answer = item[\"answer\"]\n",
    "\n",
    "#     for label, choice in zip(labels, choices):\n",
    "#         train_dataset.append(\n",
    "#             InputExample(\n",
    "#                 guid=f\"q{idx}_{label}\",\n",
    "#                 text_a=question,\n",
    "#                 text_b=choice,\n",
    "#                 label=1 if label == correct_answer else 0,\n",
    "#                 qid=f\"q{idx}\"   # <<< add this line\n",
    "#             )\n",
    "#         )\n",
    "#         guid += 1\n",
    "\n",
    "\n",
    "# Create InputExamples and qid mapping\n",
    "train_dataset = []\n",
    "guid = 0\n",
    "guid_to_qid = {}  # map example guid to question id\n",
    "\n",
    "for idx, item in enumerate(data_16):\n",
    "    question = item[\"question\"]\n",
    "    labels = item[\"labels\"]\n",
    "    choices = item[\"choices\"]\n",
    "    correct_answer = item[\"answer\"]\n",
    "    qid = f\"q{idx}\"\n",
    "\n",
    "    for label, choice in zip(labels, choices):\n",
    "        ex = InputExample(\n",
    "            guid=f\"{qid}_{label}\",\n",
    "            text_a=question,\n",
    "            text_b=choice,\n",
    "            label=1 if label == correct_answer else 0\n",
    "        )\n",
    "        train_dataset.append(ex)\n",
    "        guid_to_qid[ex.guid] = qid\n",
    "        guid += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Quick stats\n",
    "# print(\"Total examples:\", len(train_dataset))\n",
    "# print(\"Correct examples:\", sum(ex.label==1 for ex in train_dataset))\n",
    "# print(\"Incorrect examples:\", sum(ex.label==0 for ex in train_dataset))\n",
    "\n",
    "# train_dataset = []\n",
    "# for idx, item in enumerate(data_16):\n",
    "#     q = item[\"question\"]\n",
    "#     choices = item[\"choices\"]\n",
    "#     labels = item[\"labels\"]\n",
    "#     answer = item[\"answer\"]\n",
    "#     for l, c in zip(labels, choices):\n",
    "#         train_dataset.append(InputExample(\n",
    "#             guid=f\"q{idx}_{l}\",\n",
    "#             text_a=q,\n",
    "#             text_b=c,\n",
    "#             label=1 if l == answer else 0\n",
    "#         ))\n",
    "\n",
    "# ==============================\n",
    "# Classes\n",
    "# ==============================\n",
    "classes = [\"incorrect\", \"correct\"]\n",
    "label_map = {\"incorrect\":0, \"correct\":1}\n",
    "\n",
    "# ==============================\n",
    "# Load PLM + Tokenizer\n",
    "# ==============================\n",
    "_, _, _, WrapperClass = load_plm(\"roberta\", \"roberta-base\")\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "plm = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "template = ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø¬ÙˆØ§Ø¨: {\"placeholder\":\"text_b\"} ÛŒÛ Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "templates = [\n",
    "    # First 5: mask in the middle\n",
    "    (\"P1\", ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø¬ÙˆØ§Ø¨: {\"placeholder\":\"text_b\"} ÛŒÛ Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P2\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ù„ÛŒÛ’ Ø¬ÙˆØ§Ø¨ {\"placeholder\":\"text_b\"} ØµØ­ÛŒØ­ Ø·ÙˆØ± Ù¾Ø± {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P3\", ManualTemplate(\n",
    "        text='Ù…Ù†Ø¯Ø±Ø¬Û Ø°ÛŒÙ„ Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø§Ù†ØªØ®Ø§Ø¨: {\"placeholder\":\"text_b\"} Ù†ØªÛŒØ¬Û {\"mask\"} ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P4\", ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø§ÙˆØ± Ø¢Ù¾Ø´Ù†: {\"placeholder\":\"text_b\"} Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P5\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø§Ù†ØªØ®Ø§Ø¨ {\"placeholder\":\"text_b\"} Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "\n",
    "    # Last 5: mask at the end\n",
    "    (\"P6\", ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø¬ÙˆØ§Ø¨: {\"placeholder\":\"text_b\"} ÛŒÛ Ø¬ÙˆØ§Ø¨ ÛÛ’ {\"mask\"}',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P7\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ù„ÛŒÛ’ Ø¢Ù¾Ø´Ù†: {\"placeholder\":\"text_b\"} ØµØ­ÛŒØ­ Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P8\", ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø§Ù†ØªØ®Ø§Ø¨: {\"placeholder\":\"text_b\"} Ù†ØªÛŒØ¬Û {\"mask\"}',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P9\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ {\"placeholder\":\"text_b\"} Ù†ØªÛŒØ¬Û {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "    (\"P10\", ManualTemplate(\n",
    "        text='{\"placeholder\":\"text_a\"} Ø§ÙˆØ± Ø§Ù†ØªØ®Ø§Ø¨: {\"placeholder\":\"text_b\"} ØµØ­ÛŒØ­ Ø¬ÙˆØ§Ø¨ {\"mask\"}',\n",
    "        tokenizer=tokenizer\n",
    "    )),\n",
    "]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Define Verbalizer (Manual)\n",
    "# ==============================\n",
    "verbalizer = ManualVerbalizer(\n",
    "    classes=classes,\n",
    "    label_words = {\n",
    "    \"correct\": [\"Ø¯Ø±Ø³Øª\", \"ØµØ­ÛŒØ­\"], \n",
    "    \"incorrect\": [\"ØºÙ„Ø·\", \"Ù†Ø§ Ø¯Ø±Ø³Øª\"]\n",
    "    },\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Prompt Model\n",
    "# ==============================\n",
    "prompt_model = PromptForClassification(\n",
    "    template=template,  # initial template\n",
    "    plm=plm,\n",
    "    verbalizer=verbalizer\n",
    ")\n",
    "# prompt_model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ee36b-abbb-4401-a5e1-f02c500c8fd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Training loop with multi-template switching + QABalancedBatchSampler\n",
    "# ==============================\n",
    "T = 20       # epochs\n",
    "K = 1        # steps per template\n",
    "batch_size = 16  # total examples per batch (must be multiple of 4)\n",
    "optimizer = AdamW(prompt_model.parameters(), lr=1e-5)\n",
    "\n",
    "prompt_model.train()\n",
    "all_epoch_patterns = {}\n",
    "\n",
    "for epoch in range(T):\n",
    "    print(f\"\\nğŸŸ¦ Epoch {epoch+1}/{T}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Random initial template\n",
    "    prompt_name, current_template = random.choice(templates)\n",
    "    epoch_pattern = []\n",
    "\n",
    "    # QABalancedBatchSampler ensures full questions per batch\n",
    "    # sampler = QABalancedBatchSampler(train_dataset, batch_size=batch_size, options_per_question=4)\n",
    "    sampler = QABalancedBatchSampler(train_dataset, guid_to_qid=guid_to_qid, batch_size=batch_size, options_per_question=4)\n",
    "    train_loader = PromptDataLoader(\n",
    "        dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        template=current_template,\n",
    "        tokenizer_wrapper_class=WrapperClass,\n",
    "        max_seq_length=128,\n",
    "        batch_size=batch_size,\n",
    "        batch_sampler=sampler,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # # ğŸ”¹ Debug: check what the loader yields\n",
    "    # for batch_idx, batch in enumerate(train_loader):\n",
    "    #     print(batch_idx, batch.keys(), len(batch['input_ids']))\n",
    "    #     break  # just check the first batch\n",
    "\n",
    "    \n",
    "    step_counter = 0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Only move tensor entries to device\n",
    "        # batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = prompt_model(batch)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_pattern.append(prompt_name)\n",
    "    \n",
    "        step_counter += 1\n",
    "    \n",
    "        # Switch template every K steps\n",
    "        if step_counter % K == 0:\n",
    "            prompt_name, current_template = random.choice(templates)\n",
    "    \n",
    "            train_loader = PromptDataLoader(\n",
    "                dataset=train_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                template=current_template,\n",
    "                tokenizer_wrapper_class=WrapperClass,\n",
    "                max_seq_length=128,\n",
    "                batch_size=batch_size,\n",
    "                batch_sampler=sampler,\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "\n",
    "    all_epoch_patterns[f\"epoch_{epoch+1}\"] = epoch_pattern\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Prompt pattern: {epoch_pattern}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39280a7f-2699-4028-a6e9-2e5143770e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# # ==============================\n",
    "# # Training loop with multi-template switching + QABalancedBatchSampler\n",
    "# # ==============================\n",
    "# T = 20\n",
    "# K = 1\n",
    "# batch_size = 16\n",
    "# optimizer = AdamW(prompt_model.parameters(), lr=1e-5)\n",
    "\n",
    "# prompt_model.train()\n",
    "# all_epoch_patterns = {}\n",
    "\n",
    "# MAX_GRAD_NORM = 1.0   # â­ recommended value\n",
    "\n",
    "# for epoch in range(T):\n",
    "#     print(f\"\\nğŸŸ¦ Epoch {epoch+1}/{T}\")\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     prompt_name, current_template = random.choice(templates)\n",
    "#     epoch_pattern = []\n",
    "\n",
    "#     sampler = QABalancedBatchSampler(\n",
    "#         train_dataset,\n",
    "#         guid_to_qid=guid_to_qid,\n",
    "#         batch_size=batch_size,\n",
    "#         options_per_question=4\n",
    "#     )\n",
    "\n",
    "#     train_loader = PromptDataLoader(\n",
    "#         dataset=train_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#         template=current_template,\n",
    "#         tokenizer_wrapper_class=WrapperClass,\n",
    "#         max_seq_length=128,\n",
    "#         batch_size=batch_size,\n",
    "#         batch_sampler=sampler,\n",
    "#         shuffle=False\n",
    "#     )\n",
    "\n",
    "#     step_counter = 0\n",
    "#     epoch_loss = 0.0\n",
    "\n",
    "#     for batch in train_loader:\n",
    "#         batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = prompt_model(batch)\n",
    "#         loss = torch.nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "\n",
    "#         # ğŸ”´ NaN guard (important)\n",
    "#         if torch.isnan(loss):\n",
    "#             print(\"âŒ NaN detected in loss\")\n",
    "#             print(\"Logits:\", logits)\n",
    "#             print(\"Labels:\", batch[\"label\"])\n",
    "#             break\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         # â­ GRADIENT CLIPPING (THIS PREVENTS NaNs)\n",
    "#         clip_grad_norm_(prompt_model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "#         epoch_pattern.append(prompt_name)\n",
    "\n",
    "#         step_counter += 1\n",
    "\n",
    "#         if step_counter % K == 0:\n",
    "#             prompt_name, current_template = random.choice(templates)\n",
    "\n",
    "#             train_loader = PromptDataLoader(\n",
    "#                 dataset=train_dataset,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 template=current_template,\n",
    "#                 tokenizer_wrapper_class=WrapperClass,\n",
    "#                 max_seq_length=128,\n",
    "#                 batch_size=batch_size,\n",
    "#                 batch_sampler=sampler,\n",
    "#                 shuffle=False\n",
    "#             )\n",
    "\n",
    "#     all_epoch_patterns[f\"epoch_{epoch+1}\"] = epoch_pattern\n",
    "#     print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "#     print(f\"Prompt pattern: {epoch_pattern}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f6e48-7d99-43db-b0e2-4698d3c37509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Load evaluation dataset\n",
    "# ==============================\n",
    "csv_path = r\"C:\\Users\\stdFurqan\\Desktop\\paft\\QA\\openbookqa_urdu_test_cleaned.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "eval_dataset = []\n",
    "eval_qids = []   # keeps question ids aligned with examples\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    q = row['urdu_question_stem']\n",
    "    choices_dict = eval(row['clean_choices'])\n",
    "    labels = choices_dict['label']\n",
    "    texts = choices_dict['text']\n",
    "    answer = row['answerKey']\n",
    "\n",
    "    for l, t in zip(labels, texts):\n",
    "        eval_dataset.append(\n",
    "            InputExample(\n",
    "                guid=f\"eval_{i}_{l}\",\n",
    "                text_a=q,\n",
    "                text_b=t,\n",
    "                label=1 if l == answer else 0\n",
    "            )\n",
    "        )\n",
    "        eval_qids.append(i)   # question id\n",
    "\n",
    "\n",
    "eval_loader = PromptDataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=template,   # single template OR loop over templates\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "prompt_model.eval()\n",
    "\n",
    "question_scores = {}   # qid -> list of (score, label)\n",
    "\n",
    "with torch.no_grad():\n",
    "    example_idx = 0\n",
    "\n",
    "    for batch in eval_loader:\n",
    "        # batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "        logits = prompt_model(batch)          # [B, 2]\n",
    "        pos_scores = logits[:, 1]              # score for \"correct\" class\n",
    "\n",
    "        for score, label in zip(pos_scores.cpu(), batch['label'].cpu()):\n",
    "            qid = eval_qids[example_idx]\n",
    "\n",
    "            if qid not in question_scores:\n",
    "                question_scores[qid] = []\n",
    "\n",
    "            question_scores[qid].append((score.item(), label.item()))\n",
    "            example_idx += 1\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for qid, options in question_scores.items():\n",
    "    # pick option with highest score\n",
    "    pred_score, pred_label = max(options, key=lambda x: x[0])\n",
    "\n",
    "    if pred_label == 1:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "qa_accuracy = correct / total\n",
    "print(f\"\\nğŸ“Š QA Accuracy: {qa_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fbde2-bdec-4942-b506-e7c05c5ad9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
