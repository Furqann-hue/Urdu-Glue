{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6489560-54fb-4722-9264-4c9623143918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Get user profile path\n",
    "user_profile = os.environ[\"USERPROFILE\"]\n",
    "\n",
    "# Paths to Hugging Face cached models\n",
    "cached_models = [\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--bert-base-multilingual-cased\"),\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--xlm-roberta-base\"),\n",
    "    os.path.join(user_profile, r\".cache\\huggingface\\hub\\models--roberta-base\")\n",
    "]\n",
    "\n",
    "# Remove cached models if they exist\n",
    "for path in cached_models:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed cache: {path}\")\n",
    "    else:\n",
    "        print(f\"No cache found at: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbbfc314-4bd2-4318-9146-e6e24ede336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82423a3-e1ff-40d3-8f91-c9688d915a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4080 SUPER\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 64\n",
      "Correct examples: 16\n",
      "Incorrect examples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "tokenizing: 64it [00:00, 3189.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.0778\n",
      "Epoch 2 Loss: 4.6023\n",
      "Epoch 3 Loss: 5.1812\n",
      "Epoch 4 Loss: 5.1124\n",
      "Epoch 5 Loss: 3.8831\n",
      "Epoch 6 Loss: 3.4160\n",
      "Epoch 7 Loss: 3.1498\n",
      "Epoch 8 Loss: 3.1425\n",
      "Epoch 9 Loss: 3.0782\n",
      "Epoch 10 Loss: 3.1835\n",
      "Epoch 11 Loss: 2.2595\n",
      "Epoch 12 Loss: 1.5400\n",
      "Epoch 13 Loss: 0.7323\n",
      "Epoch 14 Loss: 0.8203\n",
      "Epoch 15 Loss: 0.3788\n",
      "Epoch 16 Loss: 0.6603\n",
      "Epoch 17 Loss: 0.1897\n",
      "Epoch 18 Loss: 0.6692\n",
      "Epoch 19 Loss: 0.1827\n",
      "Epoch 20 Loss: 0.4518\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Imports\n",
    "# ==============================\n",
    "import ast\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt.prompts import SoftVerbalizer\n",
    "from openprompt.prompts import AutomaticVerbalizer\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptForClassification, PromptDataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Set random seeds for reproducibility\n",
    "# ==============================\n",
    "# ðŸ’¡ Added this block to ensure consistent results across runs\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    " \n",
    "# Raw data: first 16 questions\n",
    "data_16 = [\n",
    "    {\n",
    "        \"question\": \"Ø§ÛŒÚ© Ø¨ÛŒÙ¹Ø±ÛŒ Ú©Ø³ Ú†ÛŒØ² Ú©Ùˆ Ú†Ù„Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒÙ…ÛŒØ§Ø¦ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ú©Ùˆ Ø¨Ø±Ù‚ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªÛŒ ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"Ø§Ø³ÙÙ†Ø¬\", \"Ù¾ØªÚ¾Ø±\", \"Ø§Ø³Ù¹ÛŒÙ¾Ù„Ø±\", \"Ú©ÛŒÙ„Ú©ÙˆÙ„ÛŒÙ¹Ø±\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©ÛŒØ§ ØªÙ…Ø§Ù… Ø­Ø´Ø±Ø§Øª Ú©Ùˆ Ù…Ú©Ù…Ù„ Ø¨Ø§Ù„Øº ÛÙˆÙ†Û’ Ø³Û’ Ù¾ÛÙ„Û’ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Û’ ÛØ± Ù…Ø±Ø­Ù„Û’ Ø³Û’ Ú¯Ø²Ø±Ù†Ø§ Ù¾Ú‘ØªØ§ ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"ÛŒÛ ØªÙ…Ø§Ù…\", \"Ø­Ø´Ø±Ø§Øª Ø²Ù†Ø¯Û Ù¾ÛŒØ¯Ø§ ÛÙˆØªÛ’ ÛÛŒÚº\",\n",
    "                             \"Ù¾ÛŒÙˆÙ¾Ø§ Ú©Ø§ Ù…Ø±Ø­Ù„Û Ú©Ø¨Ú¾ÛŒ Ú©Ø¨Ú¾ÛŒ Ú†Ú¾ÙˆÚ‘ Ø¯ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’\",\n",
    "                             \"Ù¾ÛŒÙˆÙ¾Ø§ Ø§ÛŒÚ© Ù„Ø§Ø²Ù…ÛŒ Ù…Ø±Ø­Ù„Û ÛÛ’\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"C\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©ÙˆÙ† Ø³ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…ÙˆØ²ÙˆÚº ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"Ú©ÙˆØ¦Ù„Û\", \"Ù¾ÛŒÙ¹Ø±ÙˆÙ„ÛŒÙ…\", \"Ù‚Ø¯Ø±ØªÛŒ Ú¯ÛŒØ³\", \"Ø³ÙˆØ±Ø¬ Ú©ÛŒ Ø±ÙˆØ´Ù†ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§ÛŒÚ© Ù…Ú¯Ø±Ù…Ú†Ú¾ Ú©ÛŒØ§ Ú©Ú¾Ø§Ø¦Û’ Ú¯Ø§ØŸ\",\n",
    "        \"choices\": np.array([\"Ù…Ú©Ú‘ÛŒ\", \"Ú©ÛŒÚ©Ú‘Ø§\", \"Ø³Ú©ÙˆÛŒÚˆ\", \"ØªÙ„Ø§Ù¾ÛŒØ§\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø­Ø±Ø§Ø±Øª Ú©Ø§ Ø°Ø±ÛŒØ¹Û ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’\",\n",
    "        \"choices\": np.array([\"Ù†Ø§Ú© Ø±Ú¯Ú‘Ù†Ø§\", \"Ø§Ù†Ù¹Ø§Ø±Ú©Ù¹ÛŒÚ©Ø§ Ù…ÛŒÚº ØªÛŒØ±Ø§Ú©ÛŒ Ú©Ø±Ù†Ø§\", \"Ø¨Ø±Ù Ú©Ùˆ Ú†Ú¾ÙˆÙ†Ø§\",\n",
    "                             \"ÙØ±ÛŒØ²Ø± Ù…ÛŒÚº Ø¨ÛŒÙ¹Ú¾Ù†Ø§\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"ÙÙ†Ú†ÙˆÚº Ú©Ùˆ Ù¾Ø³Ù†Ø¯ ÛÛ’\",\n",
    "        \"choices\": np.array([\"Ù„Ø·ÛŒÙÛ’\", \"Ø¬Ø§Ø¯Ùˆ\", \"Ø§ÛŒÙˆØ±ÛŒ ÙˆÙ† Ù„ÙˆØ² Ø±ÛŒÙ…Ù†Úˆ\", \"Ù…ÛŒÙ¹Ú¾Û’ Ø³Ù„Ø·Ø§Ù†\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø±ÙˆØ´Ù†ÛŒ Ú©Ø³ Ú†ÛŒØ² Ù…ÛŒÚº Ø³Û’ Ù†ÛÛŒÚº Ú¯Ø²Ø± Ø³Ú©ØªÛŒØŸ\",\n",
    "        \"choices\": np.array([\"Ù„Ú©Ú‘ÛŒ\", \"Ù¾Ø§Ù†ÛŒ\", \"ÛÙˆØ§\", \"Ø´ÛŒØ´Û\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ù…Ù„Ú† Ú©Ø§ Ø§ÛŒÚ© ÚˆÚ¾ÛŒØ± Ú¯Ù„ Ø³Ú‘ Ú¯ÛŒØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø¬Ø¨ Ø§Ø³ Ú©Ø§ Ø³Ø¨Ø¨ ØªÙ„Ø§Ø´ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ ØªÙˆ Ø§ÛŒÚ© Ø¨Ø§ØºØ¨Ø§Ù† Ú©Ùˆ Ù†Ø¸Ø± Ø¢ØªÛ’ ÛÛŒÚº\",\n",
    "        \"choices\": np.array([\"Ø¨Ù„ Ú©Ú¾Ø§ØªÛ’ ÛÙˆØ¦Û’ Ø¬Ø§Ù†Ø¯Ø§Ø±\", \"Ù„Ú©Ú¾Ù†Û’ ÙˆØ§Ù„Û’ Ø¬Ø§Ù†Ø¯Ø§Ø±\", \"Ø¨Ú‘Û’ Ø¯Ø±Ø®Øª\", \"Ø¬Ù†Ú¯Ù„ Ú©ÛŒ Ø¢Ú¯\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú†Ø§Ù†Ø¯ Ø·Ù„ÙˆØ¹ ÛÙˆØªØ§ ÛÛ’\",\n",
    "        \"choices\": np.array([\"Ø§ÛŒÚ© Ø¯ÛØ§Ø¦ÛŒ Ù…ÛŒÚº Ø§ÛŒÚ© Ø¨Ø§Ø±\", \"Ø³Ø§Ù„ Ù…ÛŒÚº 4 Ø¨Ø§Ø±\", \"ÛÙØªÛ’ Ù…ÛŒÚº 7 Ø¨Ø§Ø±\", \"Ù…ÛÛŒÙ†Û’ Ù…ÛŒÚº 1 Ø¨Ø§Ø±\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"C\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ù¾ÛØ§Ú‘ Ú©ÛŒ ÚˆÚ¾Ù„ÙˆØ§Ù† Ù¾Ø± Ú†Ù¹Ø§Ù†ÙˆÚº Ú©Û’ Ù¹ÙˆÙ¹Ù†Û’ Ú©ÛŒ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ù…Ú©Ù†Û ÙˆØ¬Û Ú©ÛŒØ§ ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"ØµÙ†ÙˆØ¨Ø± Ú©Û’ Ø¯Ø±Ø®Øª Ú©ÛŒ Ø¬Ú‘ÛŒÚº\", \"Ø­Ù‚ÛŒÙ‚ÛŒ Ø¹Ø±ÙØ§Ù†\", \"Ø³Ù…Ù†Ø¯Ø±ÛŒ Ø®Ø§Ø±Ù¾Ø´Øª\", \"ÛÛŒÙˆÛŒ Ù…ÛŒÙ¹Ù„ Ù…ÙˆØ³ÛŒÙ‚ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©Ú†Ú¾ Ù¾Ø±Ù†Ø¯Û’ Ù…Ù‚Ø§Ù…Ø§Øª ØªÙ„Ø§Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚº\",\n",
    "        \"choices\": np.array([\"Ø²Ù…ÛŒÙ†ÛŒ Ù†Ø´Ø§Ù†ÛŒÙˆÚº Ø³Û’\", \"Ø³Ú‘Ú© Ú©Û’ Ù†Ø´Ø§Ù†Ø§Øª Ø³Û’\", \"Ø§Ù†ÚˆÙˆÚº Ø³Û’\", \"Ù…Ù‚Ù†Ø§Ø·ÛŒØ³ÛŒ Ù†Ù…ÙˆÙ†ÙˆÚº Ø³Û’\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ú©Ø³ Ø³Û’ Ø¨ÛŒØ¬ Ù¾Ú¾ÛŒÙ„Ù†Û’ Ú©Ø§ Ø§Ù…Ú©Ø§Ù† ÛÛ’ØŸ\",\n",
    "        \"choices\": np.array([\"Ø§ÛŒÚ© Ú©Ø§Ø±\", \"Ø³ÙˆØ±Ø¬ Ú©ÛŒ Ø§ÛŒÚ© Ú©Ø±Ù†\", \"Ø§ÛŒÚ© ÙˆÛÛŒÙ„\", \"Ø§ÛŒÚ© ÛÙ…Ù†Ú¯ Ø¨Ø±Úˆ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§ÛŒÚ© Ú©Ø§Ø± Ø¬Ùˆ Ø§ÛŒÚ© Ø§ÛŒØ³Û’ ÙˆØ³ÛŒÙ„Û’ Ù¾Ø± Ú†Ù„ØªÛŒ ÛÛ’ Ø¬Ùˆ Ø¨Ø§Ù„Ø¢Ø®Ø± Ø®ØªÙ… ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯Ø§\",\n",
    "        \"choices\": np.array([\"ÚˆØ§Ø¦Ù†ÙˆØ³Ø§Ø± Ú©ÛŒ Ø¨Ø§Ù‚ÛŒØ§Øª\", \"Ù¾Ø§Ù†ÛŒ Ú©ÛŒ Ø·Ø§Ù‚Øª\", \"Ø´Ù…Ø³ÛŒ ØªÙˆØ§Ù†Ø§Ø¦ÛŒ\", \"Ø¨Ø¬Ù„ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø¨Ø¬Ù„ÛŒ Ú©Û’ Ú©Ù†ÚˆÚ©Ù¹Ø±Ø² Ø§Ù† Ø·Ø±ÛŒÙ‚ÙˆÚº Ø³Û’ Ú©Ø§Ù… Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Û’\",\n",
    "        \"choices\": np.array([\"Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆÙˆÛŒÙˆ Ú©Ùˆ Ú†Ù„Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø³Ø§Ú©Ù¹ Ù…ÛŒÚº Ù¾Ù„Ú¯ Ù„Ú¯Ø§Ù†Ø§\",\n",
    "                             \"ØµØ¨Ø­ Ú©Û’ ÙˆÙ‚Øª Ú©Ø§ÙÛŒ Ù¾Ø§Ù¹ Ø¢Ù† Ú©Ø±Ù†Ø§\",\n",
    "                             \"Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ø¨Ø¹Ø¯ Ø§Ø³ Ø¨Ø§Øª Ú©Ùˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§Ù†Ø§ Ú©Û ÛÛŒØ¦Ø± ÚˆØ±Ø§Ø¦Ø± Ø§Ù† Ù¾Ù„Ú¯ ÛÛ’\",\n",
    "                             \"Ú©Ù…Ø±Û’ Ú©ÛŒ Ù„Ø§Ø¦Ù¹ÛŒÚº Ø¬Ù„Ø§Ù†Ø§\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§Ú¯Ø± Ú©Ø³ÛŒ Ù¾Ø±Ù†Ø¯Û’ Ú©ÛŒ Ú†ÙˆÙ†Ú† Ø§Ù¾Ù†Û’ Ø³Ø§ØªÚ¾ÛŒÙˆÚº Ø³Û’ Ø¨Ú‘ÛŒ ÛÛ’ØŒ ØªÙˆ ØºØ§Ù„Ø¨ Ø§Ù…Ú©Ø§Ù† ÛÛ’ Ú©Û Ø§Ø³ Ú©Û’ ØªÚ¾Û’\",\n",
    "        \"choices\": np.array([\"Ø¨ØºÛŒØ± Ú†ÙˆÙ†Ú† ÙˆØ§Ù„ÛŒ Ù…Ø§Úº\", \"Ù„Ù…Ø¨ÛŒ Ø³ÙˆÙ†Úˆ ÙˆØ§Ù„Ø§ Ø¨Ø§Ù¾\",\n",
    "                             \"Ú†Ú¾ÙˆÙ¹ÛŒ Ú†ÙˆÙ†Ú†ÙˆÚº ÙˆØ§Ù„Û’ ÙˆØ§Ù„Ø¯ÛŒÙ†\", \"Ø§ÛŒØ³ÛŒ ÛÛŒ Ú†ÙˆÙ†Ú†ÙˆÚº ÙˆØ§Ù„Û’ Ø¢Ø¨Ø§Ø¤ Ø§Ø¬Ø¯Ø§Ø¯\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"D\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ø§Ú¯Ø± Ú©ÙˆØ¦ÛŒ Ø´Ø®Øµ Ø±ÙˆØ²Ø§Ù†Û Ù…Ù†Ú©Û’ Ø¨ÛŒÚ† Ø±ÛØ§ ÛÛ’ Ø§ÙˆØ± Ù¾Ú¾Ø± Ú©ÙˆØ¦ÛŒ Ø¨Ú¾ÛŒ Ù…Ø²ÛŒØ¯ Ù…Ù†Ú©Û’ Ù†ÛÛŒÚº Ø®Ø±ÛŒØ¯ØªØ§ØŒ ØªÙˆ Ø¨ÛŒÚ†Ù†Û’ ÙˆØ§Ù„Û’ Ú©Ø§ Ú©ÛŒØ§ ÛÙˆÚ¯Ø§ØŸ\",\n",
    "        \"choices\": np.array([\"ÙˆÛ Ø²ÛŒØ§Ø¯Û Ù¾ÛŒØ³Û’ Ú©Ù…Ø§Ø¦Û’ Ú¯Ø§\", \"Ø§Ø³ Ú©ÛŒ Ø¢Ù…Ø¯Ù†ÛŒ Ø±Ú© Ø¬Ø§Ø¦Û’ Ú¯ÛŒ\",\n",
    "                             \"Ø§Ø³ Ú©Ø§ Ù…Ù†Ø§ÙØ¹ Ø¨Ú‘Ú¾ Ø¬Ø§Ø¦Û’ Ú¯Ø§\", \"Ø§Ø³ Ú©ÛŒ Ø¢Ù…Ø¯Ù†ÛŒ Ø²ÛŒØ§Ø¯Û ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯ÛŒ\"]),\n",
    "        \"labels\": np.array([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        \"answer\": \"B\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to OpenPrompt InputExample format\n",
    "train_dataset = []\n",
    "guid = 0\n",
    "\n",
    "for idx, item in enumerate(data_16):\n",
    "    question = item[\"question\"]\n",
    "    labels = item[\"labels\"]\n",
    "    choices = item[\"choices\"]\n",
    "    correct_answer = item[\"answer\"]\n",
    "\n",
    "    for label, choice in zip(labels, choices):\n",
    "        train_dataset.append(\n",
    "            InputExample(\n",
    "                guid=f\"q{idx}_{label}\",\n",
    "                text_a=question,\n",
    "                text_b=choice,\n",
    "                label=1 if label == correct_answer else 0\n",
    "            )\n",
    "        )\n",
    "        guid += 1\n",
    "\n",
    "# Quick stats\n",
    "print(\"Total examples:\", len(train_dataset))\n",
    "print(\"Correct examples:\", sum(ex.label==1 for ex in train_dataset))\n",
    "print(\"Incorrect examples:\", sum(ex.label==0 for ex in train_dataset))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Define Classes\n",
    "# ==============================\n",
    "classes = [\"incorrect\", \"correct\"]\n",
    "label_map = {\"incorrect\": 0, \"correct\": 1}\n",
    "\n",
    "# # Step 1: Use load_plm with 'roberta' to get the correct WrapperClass\n",
    "_, _, _, WrapperClass = load_plm(\"roberta\", \"roberta-base\")  # Just to get the wrapper\n",
    "\n",
    "# # Step 2: Manually load XLM-RoBERTa model/tokenizer\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "plm = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# ==============================\n",
    "# Load Pretrained Language Model (mBERT)\n",
    "# ==============================\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-multilingual-cased\")\n",
    "\n",
    "\n",
    "\n",
    "template = ManualTemplate(\n",
    "        text='Ø³ÙˆØ§Ù„: {\"placeholder\":\"text_a\"} Ø¬ÙˆØ§Ø¨: {\"placeholder\":\"text_b\"} ÛŒÛ Ø¬ÙˆØ§Ø¨ {\"mask\"} ÛÛ’Û”',\n",
    "        tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Define Verbalizer (Manual)\n",
    "# ==============================\n",
    "verbalizer = ManualVerbalizer(\n",
    "    classes=classes,\n",
    "    label_words = {\n",
    "        \"incorrect\": [\"ØºÙ„Ø·\", \"Ù†Ø§ Ø¯Ø±Ø³Øª\"],\n",
    "        \"correct\": [\"Ø¯Ø±Ø³Øª\", \"ØµØ­ÛŒØ­\"]\n",
    "    },\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Prompt Model\n",
    "# ==============================\n",
    "prompt_model = PromptForClassification(\n",
    "    template=template,\n",
    "    plm=plm,\n",
    "    verbalizer=verbalizer\n",
    ")\n",
    "prompt_model.to(device)\n",
    "\n",
    "# ==============================\n",
    "# Training DataLoader\n",
    "# ==============================\n",
    "train_loader = PromptDataLoader(\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=template,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Fine-Tune\n",
    "# ==============================\n",
    "optimizer = AdamW(prompt_model.parameters(), lr=1e-5)\n",
    "prompt_model.train()\n",
    "for epoch in range(20):  # reduce epochs for testing\n",
    "    torch.cuda.empty_cache()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = {\n",
    "            k: v.to(device) if torch.is_tensor(v) else v\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "        optimizer.zero_grad()\n",
    "        logits = prompt_model(batch)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, batch['label'].long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df55010f-afe4-4ba4-8f19-2bac79710d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total eval examples: 3968\n",
      "âœ… Total questions: 992\n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"C:\\Users\\stdFurqan\\Desktop\\paft\\QA\\openbookqa_urdu_test_cleaned.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "eval_dataset = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    question = row['urdu_question_stem']\n",
    "    choices_dict = eval(row['clean_choices'])  # {'label': [...], 'text': [...]}\n",
    "\n",
    "    labels = choices_dict['label']\n",
    "    texts = choices_dict['text']\n",
    "    answer = row['answerKey']\n",
    "\n",
    "    for label, choice_text in zip(labels, texts):\n",
    "        eval_dataset.append(\n",
    "            InputExample(\n",
    "                guid=f\"eval_{i}_{label}\",  # keeps question id\n",
    "                text_a=question,\n",
    "                text_b=choice_text,\n",
    "                label=1 if label == answer else 0\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(\"âœ… Total eval examples:\", len(eval_dataset))\n",
    "print(\"âœ… Total questions:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98891542-04bf-4363-b404-795ad60a5fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 3968it [00:01, 2674.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š QA Accuracy: 0.2641\n"
     ]
    }
   ],
   "source": [
    "eval_loader = PromptDataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    template=template,              # single fixed template for eval\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=128,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "prompt_model.eval()\n",
    "\n",
    "question_scores = {}   # qid -> [(score, gold_label)]\n",
    "example_ptr = 0        # pointer over eval_dataset\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "        logits = prompt_model(batch)      # [B, 2]\n",
    "        pos_scores = logits[:, 1]          # confidence of \"correct\" class\n",
    "\n",
    "        for score, label in zip(pos_scores.cpu(), batch['label'].cpu()):\n",
    "            # extract question id from guid: eval_<qid>_<choice>\n",
    "            qid = eval_dataset[example_ptr].guid.split(\"_\")[1]\n",
    "\n",
    "            if qid not in question_scores:\n",
    "                question_scores[qid] = []\n",
    "\n",
    "            question_scores[qid].append((score.item(), label.item()))\n",
    "            example_ptr += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for qid, options in question_scores.items():\n",
    "    # pick option with highest score\n",
    "    _, predicted_label = max(options, key=lambda x: x[0])\n",
    "\n",
    "    if predicted_label == 1:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "qa_accuracy = correct / total\n",
    "print(f\"\\nðŸ“Š QA Accuracy: {qa_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fa579-f054-45d5-9d71-f6e2ff433a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
