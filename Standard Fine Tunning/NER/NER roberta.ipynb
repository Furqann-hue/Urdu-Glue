{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce030439-886e-409c-84f4-51c862e461ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "<LOCATION>برطانیہ</LOCATION> کے سابق فوجی سربراہ کا کہنا ہے کہ فوجی حکام نے <LOCATION>افغانستان</LOCATION> میں دہشت گردی کے خلاف جنگ کی شدت کا غلط اندازہ لگایا تھا۔\n",
      "<ORGANIZATION>بی بی سی</ORGANIZATION> کو دیے گئے ایک خصوصی انٹرویو میں <DESIGNATION>جنرل</DESIGNATION> <PERSON>سر پیٹر وال</PERSON> کا کہنا تھا کہ ان کے خیال میں <LOCATION>افغانستان</LOCATION> میں ’محدود‘ مقاصد حاصل کرنے کے لیے موجود ’فوجی افرادی قوت کافی‘ تھی، لیکن اب وہ سوچتے ہیں کہ انھوں نے غلط اندازہ لگایا تھا۔\n",
      "<DESIGNATION>بریگیڈیئر</DESIGNATION> <PERSON>ایڈ بٹلر</PERSON> <DATE>سنہ دوہزارچھ</DATE> میں <LOCATION>ہلمند</LOCATION> کے فوجی کمانڈر تھے۔\n",
      "ان کے بقول نہ تو ان کے فوجی اس مشکل جنگ کے لیے تیار تھے اور نہ ان کے پاس لڑائی کے لیے درکار اسلحہ موجود تھا۔\n",
      "لیکن برطانوی وزارت دفاع کا کہنا ہے کہ انھیں <LOCATION>افغانستان</LOCATION> میں حاصل کی گئی فتوحات پر فخر ہے۔\n",
      "میرے پیش کردہ منصوبے میں <LOCATION>افغانستان</LOCATION> بھیجے گئے فوجیوں کی تعداد کو محدود مقاصد کے حصول کے لیے کافی قرار دیا گیا تھا لیکن ابھی میں کھل کر اپنے اس اندازے کے غلط ہونے کا اقرار کرتا ہوں۔\n",
      "<PERSON>جنرل وال</PERSON> <DATE>سنہ دوہزارایک</DATE> سے <LOCATION>افغانستان</LOCATION> میں جاری دہشت گردی کے خلاف جنگ میں اب تک <NUMBER>چار سو ترپن</NUMBER> برطانوی فوجی ہلاک ہو چکے ہیں۔\n",
      "تک برطانوی فوج <DATE>سنہ دوہزار چار</DATE> <LOCATION>عراق</LOCATION> اور <LOCATION>افغانستان</LOCATION> دونوں محاذوں پر لڑ رہی تھی اور برطانوی فوجی حکام کا خیال ہے کہ انھیں اس وقت اندازہ تھا کہ ان کے پاس ایک سے زیادہ محاذوں پر لڑنے کے لیے درکار وسائل نہیں تھے۔\n",
      "لیکن اس کے باوجود انھوں نے <NUMBER>تینتیس سو</NUMBER> برطانوی فوجی <LOCATION>افغانستان</LOCATION> بھیجنے کا <ORGANIZATION>نیٹو</ORGANIZATION> سے کیا گیا وعدہ پورا کیا۔\n",
      "<ORGANIZATION>بی بی سی</ORGANIZATION> ٹو سے بات کرتے ہوئے <PERSON>جنرل وال</PERSON> کا کہنا تھا کہ ’ان کے پیش کردہ منصوبے میں <LOCATION>افغانستان</LOCATION> بھیجے گئے فوجیوں کی تعداد کو محدود مقاصد کے حصول کے لیے کافی قرار دیا گیا تھا لیکن ابھی وہ کھل کر اپنے اس اندازے کے غلط ہونے کا اقرار کرتے ہیں‘۔\n",
      "<LOCATION>عراق</LOCATION> اور <LOCATION>افغانستان</LOCATION> کی جنگ میں ناقص اندازے:فوج میں ایک مشہور کہاوت ہے کہ بہتر نتائج کی توقع رکھیں لیکن خود کو بدترین نتائج کے لیے تیار رکھیں، لیکن ہم صرف بہتر نتائج کی توقع کر رہے تھے اور خراب نتائج کے تیار نہیں تھے۔\n",
      "میرا مطلب یہ ہے کہ میرے پاس اس جنگ کے لیے درکار وسائل ہیں نہیں تھے۔\n",
      "<PERSON>لارڈ رچرڈ</PERSON> <LOCATION>افغانستان</LOCATION> میں برطانوی فوجی بھیجنے کے بعد <LOCATION>برطانیہ</LOCATION> کے فوجی حکام کا خیال تھا کہ <DATE>سنہ دوہزارپانچ</DATE> تک <LOCATION>عراق</LOCATION> کی صورت حال میں بہتری آئے گی لیکن ان کا یہ اندازہ بھی غلط ثابت ہوا۔\n",
      "سے <DATE>سنہ دو ہزارنو</DATE> تک برطانوی فوج کے <DATE>سنہ دوہزار چھ</DATE> سربراہ <PERSON>لارڈ ڈانٹ</PERSON> کا کہنا ہے کہ ’شاید مجھے اس بات کا اندازہ لگا لینا چاہیے تھا کہ <LOCATION>عراق</LOCATION> کے حالات بہتر نہیں ہوں گے اور <DATE>سنہ دوہزارچھ</DATE> میں <LOCATION>عراق</LOCATION> میں برطانوی فوجیوں کی تعداد <NUMBER>پندرہ سو</NUMBER> تک کم کر دینا درست فیصلہ نہیں تھا۔\n",
      "<PERSON>لارڈ رچرڈ</PERSON> <DATE>سنہ دوہزارچھ</DATE> اور <DATE>سنہ دوہزار سات</DATE> میں <ORGANIZATION>ایساف</ORGANIZATION> کے <NUMBER>پینتیس ہزار</NUMBER> فوجیوں کی کمانڈ کر چکے ہیں۔\n",
      "ان کا کہنا تھا کہ اس وقت کوئی یہ ماننے کو تیار نہیں تھا کہ اس جنگ میں شدت آئے گی۔\n",
      "یہ حقیقت دستاویزات کا حصہ ہے کہ <LOCATION>افغان</LOCATION> جنگ کے اوائل میں یقیناً رکاوٹیں تھیں لیکن <LOCATION>افغانستان</LOCATION> میں حاصل کی گئی کامیابوں پر ہمیں یقیناً فخر ہے۔\n",
      "برطانوی <ORGANIZATION>وزارت دفاع</ORGANIZATION> <DATE>سنہ دوہزارچھ</DATE> میں <LOCATION>افغانستان</LOCATION> کے <LOCATION>ہلمند</LOCATION> صوبے میں برطانوی فوج کو انتہائی مشکل حالات کا سامنا تھا۔\n",
      "انھیں طالبان کی جانب سے شدید مزاحمت کا سامنا تھا، ہتھیاروں اور خوراک کی قلت تھی اور ان کا واحد آسرا ہیلی کاپٹر تھے۔\n",
      "<LOCATION>ہلمند</LOCATION> میں اس وقت کے برطانوی فوجی کمانڈر <DESIGNATION>بریگیڈیئر</DESIGNATION> <PERSON>ایڈ بٹلر</PERSON> کا کہنا ہے کہ ’ہم نہ تو اس جنگ کے لیے تیار تھے اور نہ ہمارے پاس زیادہ ہتھیار اور وسائل تھے، بلکہ سب سے اہم بات یہ ہے کہ ہمارے پاس فتح حاصل کرنے کے لیے واضح حکمت عملی بھی نہیں تھی۔\n",
      "لیکن برطانوی <ORGANIZATION>وزارت دفاع</ORGANIZATION> کے ترجمان کا کہنا ہے کہ یہ حقیقت دستاویزات کا حصہ ہے کہ افغان جنگ کے اوائل میں یقیناً رکاوٹیں تھیں لیکن <LOCATION>افغانستان</LOCATION> میں حاصل کی گئی کامیابوں پر انھیں یقیناً فخر ہے۔‘\n",
      "﻿﻿ایک اسلام نواز ویب سائٹ کے مطابق <LOCATION>صومالیہ</LOCATION> میں شدت پسند تنظیم <ORGANIZATION>الشباب</ORGANIZATION> کی عدالت نے ایک کم عمر نوجوان کو ایک خاتون کو ریپ کرنے کے الزام میں سنگسار کر کے ہلاک کر دیا ہے۔\n",
      "عدالت میں مقدمے کی آڈیو ریکارڈنگ سے سامنے آنے والی معلومات کے مطابق جج نے 18 سالہ <PERSON>حسن علی</PERSON> کو حکم دیا کہ وہ متاثرہ خاتون کو ازالے کے طور پر گائے کا ایک بچھڑا دے اور اس کے بعد لڑکے کو ہلاک کر دیا گیا۔\n",
      "علی نے 28 سالہ خاتون <PERSON>فادومو حسن محمد</PERSON> کو ریپ کرنے کے الزام کی صحت سے انکار کیا تھا۔\n",
      "<LOCATION>صومالیہ</LOCATION> کی شدت پسند تنظیم <ORGANIZATION>الشباب</ORGANIZATION> نے اپنے زیرِ قبضہ علاقوں میں سخت اسلامی قوانین نافذ کر رکھے ہیں۔\n",
      "گذشتہ مہینے ملک کے ساحلی قصبے <LOCATION>باراوی</LOCATION> میں ایک اسلامی عدالت نے ایک خاتون کو بیک وقت <NUMBER>چار</NUMBER> شوہر رکھنے کے جرم میں سنگسار کر کے ہلاک کر دیا گیا تھا۔\n",
      "الشباب کے حامی ریڈیو سٹیشن نے اپنی ویب سائٹ پر کہا کہ <PERSON>علی</PERSON> کو <LOCATION>صومالیہ</LOCATION> کے <LOCATION>شبیل</LOCATION> ا کے علاقے میں دہریورا گاؤں میں <PERSON>فدیومو حسن محمد</PERSON> کو بندوق کی نوک پر ریپ کرنے کا مرتکب پایا گیا۔\n",
      "تاہم <PERSON>علی</PERSON> کا کہنا ہے کہ انھوں نے اور فدیومو حسن محمد نے اپنی مرضی سے جنسی تعلق قائم کیا۔\n",
      "<ORGANIZATION>الشباب</ORGANIZATION> حال ہی میں <LOCATION>باراوی</LOCATION> سمیت کئی قصبوں پر کنٹرول کھو چکی ہے لیکن دیہاتی علاقوں میں اب بھی بہت سے علاقے ان کے قبضے میں ہیں۔\n",
      "خیال کیا جاتا ہے کہ <ORGANIZATION>القاعدہ</ORGANIZATION> سے منسلق <ORGANIZATION>الشباب گروپ</ORGANIZATION> کے پاس کم از کم <NUMBER>پانچ ہزار</NUMBER> جنگجو ہیں اور وہ <ORGANIZATION>اقوامِ متحدہ</ORGANIZATION> کی مدد سے چلنے والی حکومت کا خاتمہ چاہتی ہے۔\n",
      "\n",
      "﻿﻿<LOCATION>سعودی عرب</LOCATION> میں <NUMBER>چار</NUMBER> خواتین کو <ORGANIZATION>القاعدہ</ORGANIZATION> کی حمایت اور اپنے بیٹوں کو جہاد کے لیے تیار کرنے کے جرم میں <NUMBER>چھ</NUMBER> سے <NUMBER>دس</NUMBER> سال کی قید کی سزا دی گئی ہے۔\n",
      "سعودی میڈیا کے مطابق خِواتین پر بلاک کی گئی انٹرنیٹ سائٹس تک رسائی اور جہاد سے متعلق آڈیو ویڈیو مواد ڈاؤن لوڈ کرنے کا بھی الزام تھا۔\n",
      "<LOCATION>سعودی عرب</LOCATION> نے شہریوں کی جہادی گروہوں میں شمولیت کی حوصلہ شکنی کرنے کے لیے ایسا کرنے والوں کے لیے سخت سزائیں رکھی ہیں۔\n",
      "جو شہری دوسرے ممالک میں لڑنے کے لیے جائیں گے انھیں <NUMBER>بیس</NUMBER> سال تک قید کی سزا دی جا سکتی ہے۔\n",
      "سعودی انتظامیہ نے <DATE>دو ہزار گیارہ</DATE> میں القاعدہ سے تعلق رکھنے یا <DATE>دوہزار تین</DATE> اور <DATE>دو ہزار چھ</DATE> کے درمیان ملک میں پرتشدد حملے کرنے میں مبینہ طور پر ملوث <LOCATION>سعودی</LOCATION> اور غیر ملکی باشندوں کو سزائیں دینے کے لیے خصوصی ٹرائبیونل بنائے تھے۔\n",
      "ملک کے سب سے بڑے مذہبی عالم نے بھی نوجوان مسلمانوں کو کہا ہے کہ وہ جہاد کے پیغام سے متاثر نہ ہوں۔\n",
      "سعودی میڈیا کے مطابق اگست میں ملک کے <DESIGNATION>مفتی اعظم</DESIGNATION> <PERSON>شیخ عبدالعزیز الشیخ</PERSON> نے <ORGANIZATION>القاعدہ</ORGANIZATION> اور <ORGANIZATION>دولتِ اسلامیہ</ORGANIZATION> کے جہادیوں کو ’اول درجے کے دشمن‘ کہا تھا۔\n",
      "حالیہ سزائیں اس وقت سنائی گئی ہیں جب <LOCATION>سعودی عرب</LOCATION> اور اس کے ہمسایہ ممالک <LOCATION>شام</LOCATION> میں <ORGANIZATION>دولتِ اسلامیہ</ORGANIZATION> کے جنگجوؤں کے خلاف امریکی سربراہی والے فضائی حملوں میں شامل ہیں۔\n",
      "﻿امریکی محکمۂ محنت نے ایک ٹیکنالوجی کمپنی کو <LOCATION>بھارت</LOCATION> سے تعلق رکھنے والے آٹھ مزدوروں کو کم اجرت دینے پر جرمانہ عائد کیا ہے۔ <LOCATION>کیلیفورنیا</LOCATION>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(r\"C:\\Users\\areesa\\Documents\\Urdu_GLUE_xlm_roberta\\data\\raw\\uner.txt\")\n",
    "\n",
    "print(\"File exists:\", DATA_PATH.exists())\n",
    "\n",
    "with DATA_PATH.open(\"r\", encoding=\"utf-16\") as f:\n",
    "    for i in range(40):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line.rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5593b6-e40f-4d4a-bae7-bb88eee0913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LOCATION>برطانیہ</LOCATION> کے سابق فوجی سربراہ کا کہنا ہے کہ فوجی حکام نے <LOCATION>افغانستان</LOCATION> میں دہشت گردی کے خلاف جنگ کی شدت کا غلط اندازہ لگایا تھا۔\n",
      "<ORGANIZATION>بی بی سی</ORGANIZATION> کو دیے گئے ایک خصوصی انٹرویو میں <DESIGNATION>جنرل</DESIGNATION> <PERSON>سر پیٹر وال</PERSON> کا کہنا تھا کہ ان کے خیال میں <LOCATION>افغانستان</LOCATION> میں ’محدود‘ مقاصد حاصل کرنے کے لیے موجود ’فوجی افرادی قوت کافی‘ تھی، لیکن اب وہ سوچتے ہیں کہ انھوں نے غلط اندازہ لگایا تھا۔\n",
      "<DESIGNATION>بریگیڈیئر</DESIGNATION> <PERSON>ایڈ بٹلر</PERSON> <DATE>سنہ دوہزارچھ</DATE> میں <LOCATION>ہلمند</LOCATION> کے فوجی کمانڈر تھے۔\n",
      "ان کے بقول نہ تو ان کے فوجی اس مشکل جنگ کے لیے تیار تھے اور نہ ان کے پاس لڑائی کے لیے درکار اسلحہ موجود تھا۔\n",
      "لیکن برطانوی وزارت دفاع کا کہنا ہے کہ انھیں <LOCATION>افغانستان</LOCATION> میں حاصل کی گئی فتوحات پر فخر ہے۔\n",
      "میرے پیش کردہ منصوبے میں <LOCATION>افغانستان</LOCATION> بھیجے گئے فوجیوں کی تعداد کو محدود مقاصد کے حصول کے لیے کافی قرار دیا گیا تھا لیکن ابھی میں کھل کر اپنے \n"
     ]
    }
   ],
   "source": [
    "# 2.1 Read full text safely\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(r\"C:\\Users\\areesa\\Documents\\Urdu_GLUE_xlm_roberta\\data\\raw\\uner.txt\")\n",
    "\n",
    "with DATA_PATH.open(\"r\", encoding=\"utf-16\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(raw_text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e7d2ba-520c-45a9-b7f5-3aa207af41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Extract tagged and non-tagged text\n",
    "import re\n",
    "\n",
    "TAG_PATTERN = re.compile(r\"<(.*?)>(.*?)</\\1>\", re.DOTALL)\n",
    "\n",
    "def xml_to_bio(text):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    pos = 0\n",
    "    for match in TAG_PATTERN.finditer(text):\n",
    "        # Handle text before tag\n",
    "        before = text[pos:match.start()]\n",
    "        for tok in before.split():\n",
    "            tokens.append(tok)\n",
    "            labels.append(\"O\")\n",
    "\n",
    "        tag = match.group(1)\n",
    "        content = match.group(2)\n",
    "\n",
    "        content_tokens = content.split()\n",
    "        for i, tok in enumerate(content_tokens):\n",
    "            prefix = \"B-\" if i == 0 else \"I-\"\n",
    "            tokens.append(tok)\n",
    "            labels.append(prefix + tag)\n",
    "\n",
    "        pos = match.end()\n",
    "\n",
    "    # Remaining text\n",
    "    after = text[pos:]\n",
    "    for tok in after.split():\n",
    "        tokens.append(tok)\n",
    "        labels.append(\"O\")\n",
    "\n",
    "    return tokens, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829df092-7013-489e-83ec-3e533f5db68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سر\tB-PERSON\n",
      "پیٹر\tI-PERSON\n",
      "وال\tI-PERSON\n",
      "نے\tO\n",
      "افغانستان\tB-LOCATION\n",
      "کا\tO\n",
      "دورہ\tO\n",
      "کیا\tO\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Test conversion on a small sample\n",
    "sample_text = \"<PERSON>سر پیٹر وال</PERSON> نے <LOCATION>افغانستان</LOCATION> کا دورہ کیا\"\n",
    "\n",
    "toks, labs = xml_to_bio(sample_text)\n",
    "\n",
    "for t, l in zip(toks, labs):\n",
    "    print(f\"{t}\\t{l}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3024364c-1edf-4bc2-9c99-535551298c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Sentence splitting (Urdu-safe)\n",
    "import re\n",
    "\n",
    "def split_urdu_sentences(text):\n",
    "    # split on Urdu full stop\n",
    "    sentences = re.split(r\"۔\\s*\", text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a1efa8-a241-42c8-addd-37abc8c7b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1738\n",
      "Sample sentence:\n",
      "{'tokens': ['برطانیہ', 'کے', 'سابق', 'فوجی', 'سربراہ', 'کا', 'کہنا', 'ہے', 'کہ', 'فوجی', 'حکام', 'نے', 'افغانستان', 'میں', 'دہشت', 'گردی', 'کے', 'خلاف', 'جنگ', 'کی', 'شدت', 'کا', 'غلط', 'اندازہ', 'لگایا', 'تھا'], 'ner_tags': ['B-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Convert full dataset to BIO sentences\n",
    "all_sentences = split_urdu_sentences(raw_text)\n",
    "\n",
    "bio_sentences = []\n",
    "\n",
    "for sent in all_sentences:\n",
    "    tokens, labels = xml_to_bio(sent)\n",
    "    if len(tokens) > 0:\n",
    "        bio_sentences.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"ner_tags\": labels\n",
    "        })\n",
    "\n",
    "print(\"Total sentences:\", len(bio_sentences))\n",
    "print(\"Sample sentence:\")\n",
    "print(bio_sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "239538de-f7a7-4304-b692-d242ad8f3578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 0\n",
      "برطانیہ         B-LOCATION\n",
      "کے              O\n",
      "سابق            O\n",
      "فوجی            O\n",
      "سربراہ          O\n",
      "کا              O\n",
      "کہنا            O\n",
      "ہے              O\n",
      "کہ              O\n",
      "فوجی            O\n",
      "حکام            O\n",
      "نے              O\n",
      "افغانستان       B-LOCATION\n",
      "میں             O\n",
      "دہشت            O\n",
      "گردی            O\n",
      "کے              O\n",
      "خلاف            O\n",
      "جنگ             O\n",
      "کی              O\n",
      "شدت             O\n",
      "کا              O\n",
      "غلط             O\n",
      "اندازہ          O\n",
      "لگایا           O\n",
      "تھا             O\n",
      "\n",
      "Sentence 1\n",
      "بی              B-ORGANIZATION\n",
      "بی              I-ORGANIZATION\n",
      "سی              I-ORGANIZATION\n",
      "کو              O\n",
      "دیے             O\n",
      "گئے             O\n",
      "ایک             O\n",
      "خصوصی           O\n",
      "انٹرویو         O\n",
      "میں             O\n",
      "جنرل            B-DESIGNATION\n",
      "سر              B-PERSON\n",
      "پیٹر            I-PERSON\n",
      "وال             I-PERSON\n",
      "کا              O\n",
      "کہنا            O\n",
      "تھا             O\n",
      "کہ              O\n",
      "ان              O\n",
      "کے              O\n",
      "خیال            O\n",
      "میں             O\n",
      "افغانستان       B-LOCATION\n",
      "میں             O\n",
      "’محدود‘         O\n",
      "مقاصد           O\n",
      "حاصل            O\n",
      "کرنے            O\n",
      "کے              O\n",
      "لیے             O\n",
      "موجود           O\n",
      "’فوجی           O\n",
      "افرادی          O\n",
      "قوت             O\n",
      "کافی‘           O\n",
      "تھی،            O\n",
      "لیکن            O\n",
      "اب              O\n",
      "وہ              O\n",
      "سوچتے           O\n",
      "ہیں             O\n",
      "کہ              O\n",
      "انھوں           O\n",
      "نے              O\n",
      "غلط             O\n",
      "اندازہ          O\n",
      "لگایا           O\n",
      "تھا             O\n",
      "\n",
      "Sentence 2\n",
      "بریگیڈیئر       B-DESIGNATION\n",
      "ایڈ             B-PERSON\n",
      "بٹلر            I-PERSON\n",
      "سنہ             B-DATE\n",
      "دوہزارچھ        I-DATE\n",
      "میں             O\n",
      "ہلمند           B-LOCATION\n",
      "کے              O\n",
      "فوجی            O\n",
      "کمانڈر          O\n",
      "تھے             O\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Quick sanity check\n",
    "for i in range(3):\n",
    "    print(\"\\nSentence\", i)\n",
    "    for t, l in zip(bio_sentences[i][\"tokens\"], bio_sentences[i][\"ner_tags\"]):\n",
    "        print(f\"{t:15s} {l}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "901e71ca-2f5c-495e-ae82-b37e72fa238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID mapping:\n",
      "B-DATE          -> 0\n",
      "B-DESIGNATION   -> 1\n",
      "B-LOCATION      -> 2\n",
      "B-NUMBER        -> 3\n",
      "B-ORGANIZATION  -> 4\n",
      "B-PERSON        -> 5\n",
      "B-TIME          -> 6\n",
      "I-DATE          -> 7\n",
      "I-DESIGNATION   -> 8\n",
      "I-LOCATION      -> 9\n",
      "I-NUMBER        -> 10\n",
      "I-ORGANIZATION  -> 11\n",
      "I-PERSON        -> 12\n",
      "I-TIME          -> 13\n",
      "O               -> 14\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Build label ↔ id mappings \n",
    "\n",
    "unique_labels = set()\n",
    "\n",
    "for sent in bio_sentences:\n",
    "    unique_labels.update(sent[\"ner_tags\"])\n",
    "\n",
    "label_list = sorted(unique_labels)\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(\"Label to ID mapping:\")\n",
    "for k, v in label2id.items():\n",
    "    print(f\"{k:15s} -> {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e238a8c0-95a2-4316-868a-9f7edc846d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Convert labels to IDs\n",
    "for sent in bio_sentences:\n",
    "    sent[\"ner_tag_ids\"] = [label2id[l] for l in sent[\"ner_tags\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62c32d55-a8cb-41b3-860e-cf708fb64bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\areesa\\anaconda3\\envs\\urdu_glue_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 1738\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.3 Create HuggingFace Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "hf_dataset = Dataset.from_list([\n",
    "    {\n",
    "        \"tokens\": sent[\"tokens\"],\n",
    "        \"ner_tags\": sent[\"ner_tag_ids\"]\n",
    "    }\n",
    "    for sent in bio_sentences\n",
    "])\n",
    "\n",
    "hf_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "852013cb-0605-4fd0-84ab-bb232e2b8730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['برطانیہ',\n",
       "  'کے',\n",
       "  'سابق',\n",
       "  'فوجی',\n",
       "  'سربراہ',\n",
       "  'کا',\n",
       "  'کہنا',\n",
       "  'ہے',\n",
       "  'کہ',\n",
       "  'فوجی',\n",
       "  'حکام',\n",
       "  'نے',\n",
       "  'افغانستان',\n",
       "  'میں',\n",
       "  'دہشت',\n",
       "  'گردی',\n",
       "  'کے',\n",
       "  'خلاف',\n",
       "  'جنگ',\n",
       "  'کی',\n",
       "  'شدت',\n",
       "  'کا',\n",
       "  'غلط',\n",
       "  'اندازہ',\n",
       "  'لگایا',\n",
       "  'تھا'],\n",
       " 'ner_tags': [2,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  2,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.4 Final dataset check\n",
    "hf_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e14fee7-f3c6-4922-9d4a-5a86f441bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Load tokenizer (XLM-R Large)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "992dba34-2105-42bf-ba8d-fe89119e508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Tokenize + align label\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)   # ignored in loss\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])  # same label for subwords\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaed00cc-f889-4b9d-891d-19f92a0d6332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████| 1738/1738 [00:00<00:00, 15871.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 5.3 Apply tokenization to dataset\n",
    "tokenized_dataset = hf_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e4c9918-6245-40bc-948b-f2d23db19e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['<s>', '▁برطانیہ', '▁کے', '▁سابق', '▁فوجی', '▁سربراہ', '▁کا', '▁کہنا', '▁ہے', '▁کہ', '▁فوجی', '▁حکام', '▁نے', '▁افغانستان', '▁میں', '▁دہشت', '▁گردی', '▁کے', '▁خلاف', '▁جنگ', '▁کی', '▁شدت', '▁کا', '▁غلط', '▁اندازہ', '▁لگایا', '▁تھا', '</s>']\n",
      "\n",
      "Labels:\n",
      "[-100, 2, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 2, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, -100]\n"
     ]
    }
   ],
   "source": [
    "# 5.4 Final tokenization sanity check\n",
    "example = tokenized_dataset[0]\n",
    "\n",
    "print(\"Tokens:\")\n",
    "print(tokenizer.convert_ids_to_tokens(example[\"input_ids\"]))\n",
    "\n",
    "print(\"\\nLabels:\")\n",
    "print(example[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38f69882-c958-4196-ba4c-971e1b79120f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1390\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 348\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.1 Create train / eval split for Zero-Shot\n",
    "from datasets import DatasetDict\n",
    "\n",
    "split_dataset = tokenized_dataset.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "split_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4e8d558-9e77-4369-a4e1-ad5eab7411f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Load XLM-RoBERTa for Token Classification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"xlm-roberta-large\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bd2882d-9f3e-4652-942b-b94fccb60412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 FREEZE the encoder\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5618dbd9-3af0-4dd5-b2be-10c2b3fcd3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e570e991-48fe-406b-ac86-c1dc791e07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_zero_shot\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0894d655-fb1b-44d2-a647-3fc1481fdabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Using cached seqeval-1.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\areesa\\anaconda3\\envs\\urdu_glue_gpu\\lib\\site-packages (from seqeval) (2.2.6)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\areesa\\anaconda3\\envs\\urdu_glue_gpu\\lib\\site-packages (from seqeval) (1.7.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\areesa\\anaconda3\\envs\\urdu_glue_gpu\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\areesa\\anaconda3\\envs\\urdu_glue_gpu\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\areesa\\anaconda3\\envs\\urdu_glue_gpu\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a51bbc3b-8448-4d8a-88c0-d009608399b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred, lab in zip(predictions, labels):\n",
    "        filtered_pred = []\n",
    "        filtered_lab = []\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i != -100:\n",
    "                filtered_pred.append(id2label[p_i])\n",
    "                filtered_lab.append(id2label[l_i])\n",
    "        true_predictions.append(filtered_pred)\n",
    "        true_labels.append(filtered_lab)\n",
    "\n",
    "    # Entity-level F1 (NER standard)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "\n",
    "    # Token-level Accuracy\n",
    "    accuracy = accuracy_score(\n",
    "        [l for sent in true_labels for l in sent],\n",
    "        [p for sent in true_predictions for p in sent]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53e625da-c894-4d21-9377-c0d677c42496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\areesa\\AppData\\Local\\Temp\\ipykernel_34080\\851354905.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 6.6 Trainer (Zero-Shot)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  \n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fe412a4-c332-4601-8086-8375f1a04d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='880' max='880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [880/880 03:05, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.146254</td>\n",
       "      <td>0.007302</td>\n",
       "      <td>0.022994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.033601</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.022994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.155800</td>\n",
       "      <td>2.875426</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.022994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.155800</td>\n",
       "      <td>2.727278</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.022994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.857900</td>\n",
       "      <td>2.591779</td>\n",
       "      <td>0.008918</td>\n",
       "      <td>0.023165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.857900</td>\n",
       "      <td>2.467337</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>0.023420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.558900</td>\n",
       "      <td>2.353962</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>0.023846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.558900</td>\n",
       "      <td>2.251660</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.034236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.558900</td>\n",
       "      <td>2.160440</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.076307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.318600</td>\n",
       "      <td>2.079285</td>\n",
       "      <td>0.016183</td>\n",
       "      <td>0.158491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.318600</td>\n",
       "      <td>2.007854</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.256174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.132800</td>\n",
       "      <td>1.945645</td>\n",
       "      <td>0.023166</td>\n",
       "      <td>0.338528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.132800</td>\n",
       "      <td>1.891784</td>\n",
       "      <td>0.023910</td>\n",
       "      <td>0.400443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.000900</td>\n",
       "      <td>1.846029</td>\n",
       "      <td>0.022763</td>\n",
       "      <td>0.448561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.000900</td>\n",
       "      <td>1.807828</td>\n",
       "      <td>0.022784</td>\n",
       "      <td>0.481860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.900900</td>\n",
       "      <td>1.776980</td>\n",
       "      <td>0.023688</td>\n",
       "      <td>0.513541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.900900</td>\n",
       "      <td>1.753306</td>\n",
       "      <td>0.025690</td>\n",
       "      <td>0.533981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.900900</td>\n",
       "      <td>1.736171</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.550077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.847600</td>\n",
       "      <td>1.725789</td>\n",
       "      <td>0.023570</td>\n",
       "      <td>0.559615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.847600</td>\n",
       "      <td>1.721758</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.562851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=880, training_loss=2.29835336858576, metrics={'train_runtime': 185.7474, 'train_samples_per_second': 149.666, 'train_steps_per_second': 4.738, 'total_flos': 3935928502899612.0, 'train_loss': 2.29835336858576, 'epoch': 20.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e2289fe-f96a-426a-8a84-d941a1df864e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot NER Results (XLM-R Large):\n",
      "F1 Score     : 0.0257\n",
      "Accuracy     : 0.5340\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on the validation set\n",
    "zero_shot_results = trainer.evaluate()\n",
    "\n",
    "print(\"Zero-Shot NER Results (XLM-R Large):\")\n",
    "print(f\"F1 Score     : {zero_shot_results['eval_f1']:.4f}\")\n",
    "print(f\"Accuracy     : {zero_shot_results['eval_accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6bc1390-7941-4fa6-8438-512c8bc116b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity classes: ['DATE', 'DESIGNATION', 'LOCATION', 'NUMBER', 'ORGANIZATION', 'PERSON', 'TIME']\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Identify sentences containing each entity class\n",
    "from collections import defaultdict\n",
    "\n",
    "# entity classes (ignore O)\n",
    "entity_classes = sorted(\n",
    "    {label.replace(\"B-\", \"\").replace(\"I-\", \"\")\n",
    "     for label in label_list if label != \"O\"}\n",
    ")\n",
    "\n",
    "print(\"Entity classes:\", entity_classes)\n",
    "\n",
    "# Map entity -> sentence indices\n",
    "entity_to_sentences = defaultdict(list)\n",
    "\n",
    "for idx, sent in enumerate(bio_sentences):\n",
    "    labels = sent[\"ner_tags\"]\n",
    "    for ent in entity_classes:\n",
    "        if f\"B-{ent}\" in labels:\n",
    "            entity_to_sentences[ent].append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed48afc4-c2ef-4ce9-9325-41b8d623031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE: 16 sentences\n",
      "DESIGNATION: 16 sentences\n",
      "LOCATION: 16 sentences\n",
      "NUMBER: 16 sentences\n",
      "ORGANIZATION: 16 sentences\n",
      "PERSON: 16 sentences\n",
      "TIME: 16 sentences\n",
      "\n",
      "Total selected sentences: 75\n"
     ]
    }
   ],
   "source": [
    "# 7.2 Take first 16 usable sentences per class\n",
    "SHOT = 16\n",
    "\n",
    "selected_indices = set()\n",
    "\n",
    "for ent in entity_classes:\n",
    "    usable = entity_to_sentences[ent][:SHOT]\n",
    "    print(f\"{ent}: {len(usable)} sentences\")\n",
    "    selected_indices.update(usable)\n",
    "\n",
    "selected_indices = sorted(selected_indices)\n",
    "print(\"\\nTotal selected sentences:\", len(selected_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc35d4c7-ba90-4eca-b848-53198d7bede2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 75\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.3 Build the 16-shot dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "shot16_dataset = Dataset.from_list([\n",
    "    tokenized_dataset[i] for i in selected_indices\n",
    "])\n",
    "\n",
    "shot16_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c26c0838-a8f7-49e9-a900-e106dd526214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.4 Create a small validation split (for early stopping)\n",
    "shot16_split = shot16_dataset.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "shot16_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15fa2056-889d-4deb-9ce3-20838fa9baad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# STEP 7.5 — Load Model (UNFROZEN)\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model_16shot = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"xlm-roberta-large\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7265140c-285f-4439-8fa5-7015bdbcf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7.6 — Training Arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args_16 = TrainingArguments(\n",
    "    output_dir=\"./ner_16shot\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aeff9955-d939-4329-98cc-37aea2329a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\areesa\\AppData\\Local\\Temp\\ipykernel_34080\\533048788.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_16shot = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# STEP 7.7 — Trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer_16shot = Trainer(\n",
    "    model=model_16shot,\n",
    "    args=training_args_16,\n",
    "    train_dataset=shot16_split[\"train\"],\n",
    "    eval_dataset=shot16_split[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4404bd7a-6d06-4ebc-aa5b-47221a2aa258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 17:27, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.874883</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.026515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.874883</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.026515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.874883</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.026515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.656768</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.032197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.981714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.269806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.243556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.068496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.867547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.865116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.843674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.799242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.788390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.806818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.754085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.808712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.744907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.808712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.736239</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.810606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.724805</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.705108</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.820076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694708</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.827652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.679124</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.829545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=1.3729832649230957, metrics={'train_runtime': 1048.0889, 'train_samples_per_second': 1.145, 'train_steps_per_second': 0.038, 'total_flos': 165977412651000.0, 'train_loss': 1.3729832649230957, 'epoch': 20.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.8 — Train 16-Shot NER\n",
    "trainer_16shot.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8435312f-ae8b-4448-8722-6043d8f27182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16-Shot NER Results (XLM-R Large):\n",
      "F1 Score     : 0.2105\n",
      "Accuracy     : 0.8277\n"
     ]
    }
   ],
   "source": [
    "# 7.9 — Extract Final 16-Shot Results\n",
    "results_16shot = trainer_16shot.evaluate()\n",
    "\n",
    "print(\"16-Shot NER Results (XLM-R Large):\")\n",
    "print(f\"F1 Score     : {results_16shot['eval_f1']:.4f}\")\n",
    "print(f\"Accuracy     : {results_16shot['eval_accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36996478-d187-437e-91ca-b628b65a0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 FULL NER FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62794db0-5580-4206-9d82-702235d36bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1390\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 348\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.1 Create 80/20 split (FULL DATA)\n",
    "full_split = tokenized_dataset.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "full_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "243e054f-6212-43e7-bda2-70720dac82a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#8.2 Load fresh model (UNFROZEN)\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model_80 = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"xlm-roberta-large\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2add5a3-78f7-4358-a43b-4978e7ce595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Training Arguments \n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args_80 = TrainingArguments(\n",
    "    output_dir=\"./ner_80_20\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "919ca782-6892-405b-b8f7-7fc2590a3a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\areesa\\AppData\\Local\\Temp\\ipykernel_34080\\1676567414.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_80 = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 8.4 Trainer (80/20)\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer_80 = Trainer(\n",
    "    model=model_80,\n",
    "    args=training_args_80,\n",
    "    train_dataset=full_split[\"train\"],\n",
    "    eval_dataset=full_split[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78ca88a2-9194-4868-b38f-f0f51ac4e6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='880' max='880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [880/880 43:12, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.720599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.216297</td>\n",
       "      <td>0.638153</td>\n",
       "      <td>0.935360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.121300</td>\n",
       "      <td>0.142670</td>\n",
       "      <td>0.815910</td>\n",
       "      <td>0.960484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.121300</td>\n",
       "      <td>0.109028</td>\n",
       "      <td>0.851408</td>\n",
       "      <td>0.966275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.860814</td>\n",
       "      <td>0.968149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.108408</td>\n",
       "      <td>0.864846</td>\n",
       "      <td>0.968660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.128720</td>\n",
       "      <td>0.866856</td>\n",
       "      <td>0.967638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.106707</td>\n",
       "      <td>0.881237</td>\n",
       "      <td>0.971725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0.877180</td>\n",
       "      <td>0.969426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.133004</td>\n",
       "      <td>0.873530</td>\n",
       "      <td>0.970107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.138673</td>\n",
       "      <td>0.879742</td>\n",
       "      <td>0.969767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.150045</td>\n",
       "      <td>0.875409</td>\n",
       "      <td>0.968745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.148088</td>\n",
       "      <td>0.877255</td>\n",
       "      <td>0.969596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.878084</td>\n",
       "      <td>0.970192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.162873</td>\n",
       "      <td>0.881054</td>\n",
       "      <td>0.971640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.171670</td>\n",
       "      <td>0.872517</td>\n",
       "      <td>0.968404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.172061</td>\n",
       "      <td>0.875723</td>\n",
       "      <td>0.969170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.177090</td>\n",
       "      <td>0.884943</td>\n",
       "      <td>0.971811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.182142</td>\n",
       "      <td>0.878136</td>\n",
       "      <td>0.970107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.179770</td>\n",
       "      <td>0.882184</td>\n",
       "      <td>0.970022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=880, training_loss=0.15820768414573236, metrics={'train_runtime': 2593.159, 'train_samples_per_second': 10.721, 'train_steps_per_second': 0.339, 'total_flos': 3935928502899612.0, 'train_loss': 0.15820768414573236, 'epoch': 20.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_80.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa726015-3c88-4581-bc07-72706f774f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/20 NER Results (XLM-R Large):\n",
      "F1 Score     : 0.8849\n",
      "Accuracy     : 0.9718\n"
     ]
    }
   ],
   "source": [
    "results_80 = trainer_80.evaluate()\n",
    "\n",
    "print(\"80/20 NER Results (XLM-R Large):\")\n",
    "print(f\"F1 Score     : {results_80['eval_f1']:.4f}\")\n",
    "print(f\"Accuracy     : {results_80['eval_accuracy']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (urdu_glue_gpu)",
   "language": "python",
   "name": "urdu_glue_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
